# Modelos de regresión lineal

```{r, include=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(patchwork)
library(scales)
library(ISLR)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE,
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(),
                  axis.text = element_blank())
```

\section{Introducción}

El modelo de regresión lineal se utiliza para hacer predicciones sobre variables cuantitativas y, en este caso,
se emplea para construir modelos más complejos desde un punto de vista estadístico.

\section{Modelos de regresión lineal simple o en una dimensión}

El modelo de regresión lineal bajo una dimensión $\mathbb{R}$, se define como:
$$
y \approx \beta_0 + \beta_1
$$
donde $y$ es la variable respuesta o variable dependiente, es decir, la variable objetivo que se desea predecir;
mientras que $x$ es la variable predictora o variable inpependiente. Con ello, se espera predecir aproximadamente
una relación lineal entre las variables en términos de una recta.

Por ejemplo, se tiene un conjunto de datos correspondiente a las ventas y el presupuesto invertido en publicidad
del producto, ya sea en periódico, televisión y radio (como se muestra en el siguiente gráfico.

```{r}

datos <- read.csv("../datos/Advertising.csv")

datos %>%
    pivot_longer(cols = TV:Newspaper, values_to = "Presupuesto") %>%
    ggplot(aes( x = Presupuesto, y = Sales)) +
        geom_point() +
        facet_wrap(~name, scales = "free_x")  +
        sin_lineas

```

Por lo que, si se quisiera realizar un modelo predictivo o estudiar la relación del efecto de hacer inversión
en ciertos canales de publicidad, entonces se trataría de definir cuál es el mejor canal de comunicación para
promover el producto; para ello es necesario hacer ciertas preguntas:

- ¿Existe algún tipo de relación entre el presupuesto que se asigna a publicidad y las ventas?
- ¿Qué tan fuerte es la asociación entre el presupuesto y las ventas?
- ¿Qué tipo de anuncios tienen un mayor impacto?
- ¿La asociación es fuerte o cuáles tienen un mayor impacto?

Así mismo, después de que se realiza la estimación, se debe verificar qué tan precisa es la estimación de
la relación que se encontró, además de comprobar qué tan precisa es la predicción sobre el nivel de ventas.
Y, si la relación que se da entre las variables es lineal, eso significa que existe cierta monotonía, cierto
comportamiento sobre el efecto de invertir dinero en publicidad para incrementar las unidades vendidas. De
manera que, ¿existe algún tipo de cinergia entre los medios? Es decir, ¿se obtiene un incremento mayor
en las ventas del producto, si se invierte la misma cantidad de dinero en publicidad, tanto en periódicos
como en televisión, que si únicamente se hubiera invertido la misma cantidad de dinero, pero de manera
independiente? Todas estás interrogantes se pueden reponder al estimar el modelo de regresión lineal, para
este caso, se define de la siguiente manera:

$$
\textrm{Ventas} \approx \beta_0 + \beta_1\textrm{TV}.
$$
Con dicho modelo se espera que aproximadamente las ventas estén relacionadas de manera lineal con hacer
publicidad en televisión. No se conocen los parámetros $\beta_0$ y $\beta_1$, el intercepto y la pendiente, respectivamente;
así que al estimarlo se recuperan dichos parámetros ($\hat{\beta_0}$, $\hat{\beta_1}$), de tal forma que una predicción que se efectúe
estará dada por:

$$
\hat{y} \approx \hat{\beta_0} + \hat{\beta_1}x.
$$
Para el modelo anterior se define el vector de parámetros de dos dimensiones que se denota como, $\theta =(\beta_0, \beta_1)^T \in \Bbb{R}^2$. Y para estimarlo (encontrar $\theta$), se minimiza la diferencia entre la estimación ($\hat{y}$) y el valor observado ($y$) de todas las observaciones que se tienen al cuadrado, es decir, $\sum_{i=1}^n(\hat{y_i}-y_i)^2$. Con lo que se espera que que la predicción sea lo más cercana a los datos observados. Así que,

$$
\hat{\theta} = \textrm{argmin}\underbrace{\sum_{i=1}^n(\hat{y_i}-y_i)^2}_{SR},
$$
donde $SR$ es la suma de los residuales al cuadrado ($\sum_{i=1}^n(\hat{y_i}-y_i)^2$), o sea,

$$
SR = \sum_{i=1}^n(\hat{y_i}-y_i)^2 = \sum_{i=1}^n r_i^2 = \sum_{i=1}^n (\beta_0+\beta_1x_i-y_i)^2.
$$
De manera que usando reglas de cálculo multivariado, derivando e igualdo a cero cada deriva parcial, se
obtiene lo siguiente:

$$
\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x}_n)(y_i - \bar{y}_n)}{(x_i - \bar{x}_n)^2},
$$
donde $\bar{x}_n=\frac{1}{n}\sum_{i=1}^n x_i$. Y,

$$
\hat{\beta_0} = \hat{y}_n - \hat{\beta_1}\bar{x}_n.
$$
\subsection{Estimación de coeficientes usando código}

Se estiman los coeficientes usando la función **lm** que relaciona la variable de salida, **Sales** (ventas), con la
cantidad de inversión en publicidad, en este caso, televisión. Al correr este modelo (como se observa en la
salida del código siguiente) se obtienen los estimadores, $\hat{\beta_0} = 7.03259$ y $\hat{\beta_1}=0.04759$. La interpretación de
estos coeficientes es que en general, **en promedio**, $\hat{\beta_1}$ representa representa las unidades adicionales que se venderan
más por cada unidad adicional invertida en publicidad (TV), pero si se decide no invertir nada en publicidad
en televisión, entonces las ventas **en promedio** serían de 7.03 unidades.


```{r}

modelo <- lm(Sales ~ TV, data = datos)
modelo

```
En la siguiente gráfica se puede ver la relación positiva lineal que tienen las ventas y la cantidad invertida en
publicidad en televisión. Así mismo, con base en cómo se encuentra definida la ecuación, se sabe que la línea roja va ha descansar en el centro y este centro es, $(\bar{x}, \bar{y})$. Por otra parte, si,

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x,
$$
entonces,

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1}\bar{x}_n + \hat{\beta_1}(x - \bar{x}_n),
$$
es decir,

$$
\hat{y} = \bar{y}_n + \hat{\beta_1}(x - \bar{x}_n).
$$
Por consiguiente, los coeficientes se pueden interpretar como, por cada incremento adicional que se tenga sobre el promedio de inversión en publicidad en televisión, $(x - \bar{x}_n)$, se tiene un incremento de $\hat{\beta_1}$ unidades. Mientras que, el modelo asume que si no estamos por encima de lo que se invierte en promedio, o sea, estamos justo en lo que se invierte en promedio en ventas, $(x - \bar{x}_n) = 0$. Así que, la salida debe ser las ventas en promedio que hemos observado.

```{r}

datos %>%
    ggplot(aes(TV, Sales)) +
        geom_point() +
        stat_smooth(method = 'lm', col = 'red') + sin_lineas +
        geom_hline(yintercept = mean(datos$Sales), linetype = 'dashed') +
        geom_vline(xintercept = mean(datos$TV), linetype = 'dashed')

```


```{r}

datos %>%
    summarise(Xbar = mean(TV), Ybar = mean(Sales))


```


\subsection{Precisión en estimaciones}

El modelo probabilístico que se ha utilizado es el siguiente:

$$
y = \underbrace{\beta_0 + \beta_1x}_{\textrm{Modelo linel}} + \underbrace{\varepsilon}_{\textrm{Término error}}.
$$
El término error puede representar un error observacional alrededor del modelo lineal, pero también puede representar toda la inalienabilidad que tiene este modelo para establecer una relación directa con los datos. Este término de error compensa las limitantes del modelo lineal. Por lo que, $\varepsilon$ es una variable aleatoria con $\Bbb{E}(\varepsilon) = 0$ y $\Bbb{V}(\varepsilon)=\sigma^2$. Y se desea que el nivel de varianza sea el mismo alrededor de la recta, en todo el dominio del problema $(x)$, es decir, $\Bbb{V}(\varepsilon)=\sigma^2 \neq \sigma^2(x)$. Por simplicidad, asumimos que,

$$
\varepsilon \sim N(0, \sigma^2),
$$
De manera que,

$$
ln(\beta) = log \ \mathcal{L}_n(\beta) = \frac{1}{n} \sum_{i=1}^n log \ \pi_{\varepsilon} (y_i - \hat{y_i}),
$$
donde $\pi_{\varepsilon}$ es la densidad de una $N(0, \sigma^2)$. Si se quisiera efectuar el modelo bajo distintas realizaciones de $(x_i, y_i)$, se podría caracterizar la varianza de los estimadores $(\Bbb{V}(\hat{\beta}))$ usando el error cuadrático ($SE = (\hat{\beta_0}) = \sqrt{\Bbb{V}(\beta_0)}$) y $SE$ se puede calcular para cada estimador de la siguiente manera:

$$
SE(\hat{\beta_0}) = \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}_n^2}{\sum_{i=1}^{n}(x_i - x_n)^2}\right),
$$
y,

$$
SE(\hat{\beta_1}) = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - x_n)^2}.
$$

También, otra forma de calcular lo anterior de manera aproximada, es decir, caracterizar la variación que existe en estas estimaciones cuando solo se tiene una muestra, es utilizar **bootstrap**.

Codigo obtenido de
[tidymodels](https://www.tidymodels.org/learn/statistics/bootstrap/). También
pueden consultar la sección correspondiente en libro [R for Data
Science](https://r4ds.had.co.nz/many-models.html).

```{r}

library(rsample)
set.seed(108727)

boots <- bootstraps(datos %>% dplyr::select(Sales, TV), times = 5000, apparent = TRUE)

```

```{r}

ajusta_modelo <- function(split) {
    lm(Sales ~ TV, analysis(split))
}

```

```{r, cache = TRUE}

boot_models <- boots %>%
    mutate(modelo = map(splits, ajusta_modelo),
           coefs  = map(modelo, tidy))

boot_coefs <- boot_models %>%
    unnest(coefs)

```

- Con **bootstrap** se puede calcular el promedio y la desviación estándar del modelo, así mismo intervalos de confianza centrados utilizando el supuesto de normalidad y/o intervalos de confianza basados en percentiles, como se muestra a continuación.

```{r}

boot_models %>%
    unnest(coefs) %>%
    group_by(term) %>%
    summarise(mean = mean(estimate), se = sd(estimate))

```


```{r}
t_intervals <- int_t(boot_models, coefs)
t_intervals

percentile_intervals <- int_pctl(boot_models, coefs)
percentile_intervals

```

- Con este método se están generando distintos conjuntos de entrenamiento y como se presenta en los gráficos siguientes, se están calculando las estimaciones para $\beta_0$ y $\beta_1$.

```{r}

ggplot(boot_coefs, aes(estimate)) +
  geom_histogram(bins = 30) +
  facet_wrap( ~ term, scales = "free") +
  geom_vline(aes(xintercept = .lower), data = percentile_intervals, col = "salmon") +
  geom_vline(aes(xintercept = .upper), data = percentile_intervals, col = "salmon") +
    sin_lineas

```

Al final si se tiene una colección de remuestras, se puede pensar que se tiene una media de precisión sobre los parámetros $\beta$ y al tener una dispersión sobre los parámetros $\beta$, se tiene una familia de posibles funciones lineales que satisfacen la relación lineal. Finalmente lo que se dea es etimar, con este método de remuestreo, la incertidumbre del modelo, desde un enfoque frecuentista, la dispersión que tienen los datos bajo distintas realizaciones, bajos disintos intervalos de confianza. Y esto se realiza para considerar pruebas de hipótesis:

$$
\textrm{H}_0: \textrm{No existe una relación lineal entre} x \textrm{y} y.
\textrm{H}_1: \textrm{Existe alguna relación lineal entre} x \textrm{y} y.
$$

Dicho de otra manera,

$$
\textrm{H}_0:\beta_1 = 0.
\textrm{H}_1:\beta_1 \neq 0.
$$

Y, las pruebas de hipótesis se resuelven al observar el valor-$p$, la distribución de muestreo y calcular probabilidades de tener un estadístico tan extremo como el que se observa.


```{r}

boot_aug <-
  boot_models %>%
  sample_n(200) %>%
  mutate(augmented = map(modelo, augment)) %>%
  unnest(augmented)

ggplot(boot_aug, aes(TV, Sales)) +
  geom_line(aes(y = .fitted, group = id), alpha = .2, col = "salmon") +
  geom_point()

```

```{r}

summary(modelo)

```


### Precisión de las predicciones {-}

```{r}

RSE <- sqrt(sum(residuals(modelo)**2)/198)
RSE
```


Error porcentual:

```{r}

RSE / mean(datos$Sales)

```


```{r}

summary(modelo)$r.squared

```


## Regresión lineal múltiple {-}

### Predictores categóricos {-}

Sea $y$ el saldo de una cuenta bancaria y $x$ una variable categórica de dos niveles, t.q.:

\begin{equation}
    x_i=
    \begin{cases}
      1,& \text{si la observación i es un estudiante}, \\
      0,& \text{si el observación i no es un estudiante}
    \end{cases}
\end{equation}

Nuestro modelo lineal sería igual a:

$$y=\beta_0+ \beta_1x + \epsilon,$$

dónde, al tener una variable categórica de dos niveles, el valor de nuestro regresor queda definido de la siguiente manera:

\begin{equation}
    y =
    \begin{cases}
      \beta_0+\beta_1, & \text{(estudiante)}, \\
      \beta_0, & \text{(no estudiante)}
    \end{cases}
\end{equation}

Ajustando el modelo, obtendríamos los valores de los siguientes coeficientes:

+ $\beta_0$: el balance promedio para personas no estudiantes;
+ $\beta_0 + \beta_1$: el balance promedio para personas estudiantes;
* $\beta_1$: la diferencia promedio entre los balances --el balance de una persona promedio que es estudiante y el balance promedio de una persona que **no** es estudiante.

**Nota:** Todas estas interpretaciones son en términos de promedios.

Veamos un primer ejemplo, con una base de datos que contiene información general de créditos. Definimos el siguiente modelo simple:

$$Balance = \beta_0+\beta_1Student + \epsilon.$$

```{r}

summary(lm(Balance ~ Student, Credit))

```

Nuestro modelo nos indica que, en promedio, un estudiante tiene un saldo mucho mayor en su tarjeta de crédito que una persona que no es estudiante. De hecho, *la diferencia promedio en saldos*, entre el balance de un estudiante y una persona que no es estudiante, es de 396.46 unidades.


Este tipo de análisis es común cuando queremos contrastar alguna característica particular de nuestro objeto de estudio contra lo que sucede con un grupo de control. Por ejemplo, en el caso anterior, el grupo de control fue definido como el conjunto de personas que no son estudiantes.

Si nos interesaría cambiar la dirección del contraste, entonces, podemos cambiar el orden de las variables categóricas con la función `fct_relevel`.

Para ejemplificar este reordenamiento en los niveles, a continuación, especificamos que nuestro grupo de control será el conjunto de personas que sí son estudiantes.

```{r}

summary(lm(Balance~Student, Credit%>% mutate(Student =fct_relevel(Student, "Yes"))))

```

Tras correr la regresión, vemos que el único cambio fue el signo de $\beta_1$; lo cuál tiene sentido porque nuestro grupo de control ahora es el conjunto de personas que sí son estudiantes.

Ahora, si nos interesaría dimensionar la significancia de la diferencia en los saldos promedios entre estos dos grupos ($\beta_1$), podríamos implementar una prueba de hipótesis de la siguiente manera:

```{r}

t.test(Balance ~ Student, data = Credit)

```

Con este test, estaríamos evaluando si el ser (o no) estudiante tiene algún efecto en los saldos en las tarjetas de crédito. Como tenemos un p-value muy pequeño, ~0, entonces concluimos que hay evidencia suficiente para rechazar la hipótesis nula de que $\beta_1=0$. Otra forma de interpretar este resultado es decir que hay evidencia, al menos marginalmente, a favor de que el ser estudiante tiene algún efecto sobre los saldos en las tarjetas de crédito. **Nota:** Recordar que cuando tenemos pocos datos, los supuestos distribucionales que hacemos son asintóticos.


También podemos incorporar predictores con más niveles, como por ejemplo, en el siguiente caso tenemos la variable categórica `Ethnicity`, misma que consta de tres niveles:


```{r}
Credit %>%
  group_by(Ethnicity) %>%
  tally()
```


```{r}

summary(lm(Balance ~ Ethnicity, Credit))

```

Al igual que en la especificación anterior, los coeficientes se interpretan como la diferencia (con respecto al grupo de control) en los balances promedios, dependiendo de la etnicidad de los sujetos.

En este caso, de manera similar al ejemplo anterior, podemos cambiar nuestro grupo de control. Por ejemplo, podemos definir que ahora nuestro grupo de control sea el conjunto de sujetos con etnicidad asiática. En ese caso, el intercepto $\beta_0$ tiene la interpretación de ser el balance promedio del grupo de control. El resto de las $\beta$'s hacen referencia a la diferencia en los saldos promedios con respecto al grupo de control: personas con etnicidad asiática.


```{r}

summary(lm(Balance ~ Ethnicity, Credit %>% mutate(Ethnicity = fct_relevel(Ethnicity, "Asian"))))

```

**¿Cómo definimos nuestro grupo de control?**

Es importante recordar que estamos pensando en el uso de las regresiones como una herramienta para hacer comparaciones: los efectos de ciertos atributos sobre los individuos promedios que estamos estudiando, respecto a un grupo de control. Así, la definición de nuestro grupo de control estará motivada por el tipo de estudio que estemos realizando.

**Modelos sin intercepto**

Si quisierámos ver reflejado en nuestro modelo el **saldo promedio para cada grupo étnico**, un truco muy común es ajustar un modelo lineal sin incercepto, como se muestra a continuación:
$$Balance = \beta_1Asian + \beta_2 African American+ \beta_3Caucasian + \epsilon.$$

```{r}

summary(lm(Balance ~ Ethnicity-1, Credit %>% mutate(Ethnicity = fct_relevel(Ethnicity, "Asian"))))

```


Cuando quito el intercepto ($\beta_0$) en mi modelo:

+ Obtengo el saldo promedio para cada uno de los grupos: estos vienen definidos por cada uno de los coeficientes existentes en el modelo.

+ No hay problemas multicolinealidad.

+ Esta decisión dependerá de nuestro interés y objeto de estudio.



```{r}

summary(lm(Balance ~ Income + Student, Credit))

```

### Interacciones {-}

Otra práctica común es incluir interacciones (producto entre dos o más variables)  como términos explicativos en los modelos. Este tipo de prácticas resultan relevantes cuando es de nuestro interés identificar la contribución conjunta de dos variables sobre nuestra $y$.


En la siguiente regresión, por ejemplo, nos interesa conocer el efecto conjunto de una dos variables continuas --gastos en publicidad en `TV` y en `Radio`-- sobre las ventas.

$$Sales = \beta_0 + \beta_1Tv+\beta_2Radio+ \beta_3 Radio*Tv+ \epsilon.$$

```{r}

summary(lm(Sales ~ TV + Radio + TV*Radio, datos))

```

Nuestro término de interacción también podría darse entre una variable categórica y una variable continua.

Pensemos en el siguiente modelo:

$$y =\beta_0 +\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+\epsilon,$$

dónde, $x_i$ es una variable numérica y $x_2$ es una variable categórica de dos niveles.


\begin{equation}
    y=
    \begin{cases}
      \beta_0+\beta_1x_1 + \epsilon,& \text{cuando } x_2=0,\\
      (\beta_0+\beta_2) +(\beta_1+\beta_3)+\epsilon, & \text{cuando } x_2=1.
    \end{cases}
\end{equation}

Esto es como si ajustáramos dos rectas o modelos lineales para cada grupo:

\begin{equation}
    y=
    \begin{cases}
      \beta_0+\beta_1x_1+\epsilon,& \text{si } x_2=0,\\
      \hat{\beta_0}+ +\hat{\beta_1}x_1 +\epsilon, & \text{si } x_2=1.
    \end{cases}
\end{equation}

Ejemplifiquemos este último caso empleando la base de datos `Credit`. En este conjunto de datos, cada observación está caracterizada en términos de su ingreso, el saldo que tiene en su tarjeta de crédito y su estatus como estudiante o no. Si incluimos la interacción `Income*Student`, nuestra especificación queda de la siguiente manera:

$$Balance = \beta_0 +\beta_1Income+ \beta_2Student + \beta_3Income*Student + \epsilon.$$


```{r}

summary(lm(Balance ~ Income + Student + Income * Student, Credit))

```

Con esta última especificación, notamos que:

+ Es equivalente a ajustar una regresión distinta a cada grupo.
+ El grupo con pocas observaciones (el grupo de los estudiantes) tiene mayor incertidumbre en los coeficientes, pues al tener pocos datos no tiene tanta evidencia.
+ Si la interacción sale significativa para el modelo (es decir, tiene un valor-p cercano a cero), es importante mantener los atributos base en la regresión, aún cuando aparentemente no sean significativos para el modelo. Esto porque puede ser que la interacción está altamente correlacionada con las otras regresoras y, en ese sentido, al ser significativa en el modelo, "absorbe" parte del efecto de las regresoras base o individuales.


```{r, fig.asp = .4}

g1 <- Credit %>%
  ggplot(aes(Income, Balance)) +
    geom_point() + sin_lineas

g2 <- Credit %>%
  ggplot(aes(Income, Balance, group = Student, color = Student)) +
    geom_point() + sin_leyenda + sin_lineas

g3 <- Credit %>%
  ggplot(aes(Income, Balance, group = Student, color = Student)) +
    geom_point(alpha = .3) +
    stat_smooth(method = "lm")  + sin_lineas

g1 + g2 + g3

```

### ¿Interpretación de coeficientes? {-}


Tras ajustar este tipo de modelos, surge de manera natural la siguiente pregunta: ¿cómo interpretamos los coeficientes asociados a una interacción? Para entender mejor este tema, utilizaremos una base de datos que contiene información relativa a los
resultados en pruebas de IQ de niños, basados en ciertas características de la madre.

Comencemos con un modelo muy sencillo: una predicción del IQ del niño en terminos del puntaje de IQ de la madre.


$$kid\_score = \beta_0+\beta_1+ \epsilon.$$

```{r, message = FALSE}
kidiq <- read_csv("../datos/kidiq.csv")
summary(lm(kid_score ~ mom_iq, kidiq))
```

Una interpretación ingenua de este modelo, sería:

+ independientemente del IQ de la madre, todos lo niños tienen al menos un IQ de
25.79;
+ en promedio, cada incremento en una unidad del IQ de la madre, está asociado con un incremento de 0.60997 unidades adicionales en el puntaje de IQ del niño.

¿Cuál es el problema con esta interpretación? Sin conocimiento de dominio, esta interpretación, llevada al extremo, nos estaría indicando que si una madre tiene un IQ igual a cero, entonces el niño tiene un IQ igual a 25.799. ¿Qué hay de raro con esto? Por un lado, está raro que una mamá tenga un IQ de cero; por otro lado, está raro decir que un niño tiene un IQ de 25.799 (cuando el IQ promedio oscila alrededor de 80 puntos).

Extendamos nuestro modelo anterior con la inclusión de una variable indicadora `mom_hs`, que nos señala si la madre acabó o no high school, y una interacción entre `mom_hs`y `mom_iq`.

$$kid\_score =\beta_0+\beta_1mom\_iq+ \beta_2 mom\_hs+ \beta_3 mom\_iq*mom\_hs+\epsilon.$$

```{r}
summary(lm(kid_score ~ mom_iq + mom_hs + mom_iq * mom_hs, kidiq))
```

Esta nueva especificación nos indica que, en promedio, cuando una mamá no cuenta con preparatoria y tiene un IQ=0, el IQ del niño promedio es -11.48 puntos. Otra vez, ¡esta interpretación está un poco ruidosa!

```{r, message = FALSE}
g1 <- ggplot(kidiq, aes(mom_iq, kid_score)) +
  geom_point(alpha = .3) +
  stat_smooth(method = lm, color ='salmon') + sin_lineas

g2 <- ggplot(kidiq, aes(mom_iq, kid_score, group = mom_hs, color = factor(mom_hs))) +
  geom_point(alpha = .3) +
  stat_smooth(method = lm) + sin_lineas

g1 + g2
```

Graficando los dos modelos anteriores, vemos que, a pesar de nuestras interpretaciones ingenuas, ambos modelos se ajustan bastante bien. De hecho, vemos que el IQ de las madres comienza en 80, lo cuál ya suena un poco más razonable.

**Centrando nuestros regresores**

Una forma fácil de interpretar un modelo lineal es especificando como nuestro punto de comparación el promedio: el promedio de nuestros atributos predictivos. Para ello, debemos proceder a centrar nuestras variables regresoras.

Regresemos al modelo dónde el único predictor era el IQ de la madre y comparemos los resultados con su versión centrada.


```{r}

kidiq.centered <- kidiq %>%
  mutate(mom_iq = mom_iq - mean(mom_iq))

summary(lm(kid_score~mom_iq, kidiq))

summary(lm(kid_score ~ mom_iq,  kidiq.centered))

```

Notamos que al centrar el IQ de la madre, con respecto al puntaje promedio, los coeficientes del modelo son más interpretables:

+ $\beta_0$: en promedio, para una mamá con un IQ promedio, el puntaje del niño va a estar en 86.79 puntos.
+ $\beta_1$: al igual que en el modelo original (sin centrar `mom_iq`), sugiere que, en promedio, un aumento en una unidad en el IQ de la madre está asociado con un incremento en 0.60997 unidades en el IQ del niño.

Ahora, retomemos nuestra especificación con interacciones y comparémosla contra un modelo centrado.

```{r}
summary(lm(kid_score ~ mom_iq + mom_hs + mom_iq * mom_hs, kidiq))

summary(lm(kid_score ~ mom_iq + mom_hs + mom_iq * mom_hs, kidiq.centered))
```

La interpretación del intercepto tiene más sentido: una mamá, con un IQ promedio y sin tener el high school concluido, refleja un niño con IQ igual a 85.4069. Además, de acuerdo con el modelo, una mamá promedio, que sí terminó la secundaria, refleja, en promedio, un incremento de 2.84 unidades en el IQ del niño. Finalmente, vemos que la interacción `mom_iq*mom_hs` es significativa para el modelo, aunque  el término individual no lo es.

Comparando estos dos ejemplos, en sus versiones centradas y no centradas, concluimos que:

+ El efecto marginal es siempre el mismo, sin importar si centramos o no nuestras regresoras;

+ Lo que cambia es el punto de comparación.


También podría ser de nuestro interés fijar un punto de comparación distinto al promedio. Por ejemplo, podríamos estar interesados en fijar nuestro punto de comparación en una mamá super dotada (con un IQ=130).



```{r}
kidiq.centered <- kidiq %>%
  mutate(mom_iq = mom_iq - 130)

summary(lm(kid_score ~ mom_iq,  kidiq.centered))

```

El efecto promedio por cada incremento en una unidad en el IQ de la madre sigue siendo 0.60997 puntos en el IQ del niño; lo que cambia es el punto de comparación: una mamá super dotada. Así, nuestra interpretación de $\beta_1$ cambia ligeramente: en promedio, una mamá super dotada (con 130 puntos de IQ), tiene un hijo con un IQ de 105.

**Motivación para centrar nuestras variables**

En nuestros modelos, podemos centrar nuestros:

+ regresores;
+ variables respuestas;
+ o, ambas.

En general, la motivación detras de estos procedimientos es distinta: mientras el centrar los regresores está motivado por cuestiones de interpretabilidad, el centrar la variable de respuesta está asociado a cuestiones de facilidad en el ajuste del modelo.


**Estandarización de las variables**

Si en lugar de centrar nuestros regresores los estandarizamos, entonces tenemos todas las variables en las mismas unidades. Esto facilita la comparabilidad entre los distintos coeficientes.  *Nota: *  Por ejemplo, cuando lleguemos a la versión bayesiana de estos modelos, encontraremos que es recomendable estandarizar nuestras variables porque esto nos permite proponer una única distribución previa para todos los coeficientes.


### Relaciones no lineales {-}

Una de las grandes ventajas de la técnica de ajuste de las regresiones es que nos da toda la maquinaria necesaria para incorporar asociaciones más complicadas; por ejemplo, podemos incorporar asociaciones no lineales.

En este caso, tenemos el consumo de un automóvil en función de la potencia de un motor (`horsepower`). Al graficar esta relación, vemos que esta no es lineal.


```{r}
g0 <- Auto %>%
  ggplot(aes(horsepower, mpg)) +
  geom_point(alpha = .3) +
  sin_lineas

g1 <- Auto %>%
  ggplot(aes(horsepower, mpg)) +
  geom_point(alpha = .3) +
  stat_smooth(method = lm, formula = y ~ x, color = 'salmon') + sin_lineas

g0+g1
```


Una alternativa, podría ser la inclusión de términos polinomiales, como por ejemplo, incluir el término cuadrático de `horsepower`:
$$mpg = \beta_0+\beta_1*\text{horsepower}+ \beta_2 *\text{horsepower}^2+\epsilon.$$

```{r}

summary(lm(mpg ~ poly(horsepower, 2, raw = TRUE), Auto))

```

Con esta nueva especificación, observamos que nuestro modelo se ajusta bastante bien a los datos.


```{r}

g2 <- Auto %>%
  ggplot(aes(horsepower, mpg)) +
  geom_point(alpha = .3) +
  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE), color = 'salmon') +
  sin_lineas

g2

```

Con esta nueva especificación, observamos que hay in trade-off entre el ajuste de nuestro modelo y la interpretabilidad del mismo:

+ Nuestro modelo se ajusta bastante bien a los datos.
+ Aquí ya no podemos interpretar el coeficiente lineal como lo habíamos hecho en nuestras especificaciones previas: es decir, el $\beta_1$ asociado al término individual `horsepower` ya no puede ser interpretado como lo que pasaría si todo lo demás se mantiene constante o en cero (porque también tenemos el $\beta_2$ asociado a `horsepower`$^2$).

## Extensiones del modelo lineal {-}


Incluso podemos hacer más extensiones del modelo. Aquí, en esta figura, claramente una línea no ajusta el modelo.


```{r}
x <- 7*runif(50)
y <- cos(x) + 0.15 * rnorm(length(x))

newx <- tibble(x = matrix(seq(0,12,.1)))
dt <- tibble(x = x, y = y)

g1 <- ggplot(dt, aes(x,y)) + geom_point(size=2)
g1
```

De hecho, debido a la distribución de nuestros datos, al ajustarle un modelo del siguiente tipo $y=\beta_0 +\beta_1x+\epsilon$, confirmamos que una regresión lineal simple no es la mejor alternativa.

```{r}
m1 <- lm(y ~ x, data = dt)

newx <- data.frame(x = matrix(seq(0,7,.01)))
dnew <- tibble(x = newx$x, y = predict(m1, newx))
pred <- geom_line(data = dnew, aes(x = x, y = y), colour = 'salmon')

g1 + geom_smooth(method = lm, colour = 'salmon')
```

A continuación, experaremos algunas alternativas.

**Predictores por zonas**

Quizá, una mejor aproximación puede ser mediante la inclusión de predictores por zonas: haciendo predicciones por grupos, dependiendo de los valores que tome la variable explicativa en distintos intervalos. En el gráfico siguiente, se ajustó una predicción constante en cada uno de los intervalos.


```{r, echo = F}
eps1 <- 2
eps2 <- 4
dt$reg <- cut(dt$x, breaks = c(min(dt$x),eps1,eps2,max(dt$x)), include.lowest = T)
m4 <- lm(y~reg, data = dt)

newx$reg <- cut(newx$x, breaks = c(min(dt$x),eps1,eps2,max(dt$x)), include.lowest = T)
dnew$reg <- newx$reg
dnew$y1 <- predict(m4, newx)
dnew$s2 <- predict(m4, newx, se.fit = T)$se.fit

pred2 <- geom_line(data = dnew, aes(x = x, y = y1, group = reg), colour = 'salmon')

e1 <-  geom_vline(data = dnew, xintercept = eps1, lty = 2)
e2 <-  geom_vline(data = dnew, xintercept = eps2, lty = 2)
rib2 <- geom_ribbon(data = dnew, aes(x = x, ymin= y1-2*s2, ymax = y1+2*s2, group = reg),
alpha = 0.3)

print(g1 + pred2 + e1 + e2 + rib2 + ggtitle("Predicción lineal por regiones"))
```

También, podríamos incluir predictores que no sean constantes, sino más bien líneas que se ajusten de mejor manera a los datos.
```{r}
eps1 <- 2
eps2 <- 4
dt$reg <- cut(dt$x, breaks = c(min(dt$x),eps1,eps2,max(dt$x)), include.lowest = T)
m4 <- lm(y~reg * x, data = dt)

newx$reg <- cut(newx$x, breaks = c(min(dt$x),eps1,eps2,max(dt$x)), include.lowest = T)
dnew$reg <- newx$reg
dnew$y1 <- predict(m4, newx)
dnew$s2 <- predict(m4, newx, se.fit = T)$se.fit

pred2 <- geom_line(data = dnew, aes(x = x, y = y1, group = reg), colour = 'salmon')

e1 <-  geom_vline(data = dnew, xintercept = eps1, lty = 2)
e2 <-  geom_vline(data = dnew, xintercept = eps2, lty = 2)
rib2 <- geom_ribbon(data = dnew, aes(x = x, ymin= y1-2*s2, ymax = y1+2*s2, group = reg),
alpha = 0.3)

print(g1 + pred2 + e1 + e2 + rib2 + ggtitle("Predicción lineal por regiones"))
```


**Splines**

En lugar de la aproximación por zonas, podríamos incluir splines (funciones que se intersectan en ciertos nodos) en nuestros modelos.

```{r, echo = F}
library(splines)
m4 <- lm( y~bs(x, degree = 1, 2), data = dt)

dnew$y1 <- predict(m4, newx)
dnew$lower <- as_tibble(predict(m4, newx, interval = 'confidence'))$lwr
dnew$upper <- as_tibble(predict(m4, newx, interval = 'confidence'))$upr

pred2 <- geom_line(data = dnew, aes(x = x, y = y1), colour = 'salmon')
rib2 <- geom_ribbon(data = dnew, aes(x = x, ymin= lower, ymax = upper),
alpha = 0.3)

print(g1 + pred2 + rib2 + ggtitle("Regresión con splines"))
```


```{r, echo = F}
m4 <- lm( y~bs(x, degree = 1, 3), data = dt)

dnew$y1 <- predict(m4, newx)
dnew$lower <- as_tibble(predict(m4, newx, interval = 'confidence'))$lwr
dnew$upper <- as_tibble(predict(m4, newx, interval = 'confidence'))$upr

pred2 <- geom_line(data = dnew, aes(x = x, y = y1), colour = 'salmon')
rib2 <- geom_ribbon(data = dnew, aes(x = x, ymin= lower, ymax = upper),
alpha = 0.3)


print(g1 + pred2 + rib2 + ggtitle("Regresión con splines"))
```



## Conclusiones {-}

+ Las regresiones, por simple que parezcan, nos proporcionan una maquinaria muy potente para ajustar distintos modelos, sea mediante la inclusión de interacciones, polinomios, ajustes por regiones o splines.

+ Puede existir un trade-off implícito entre el ajuste de un modelo y su interpretabilidad.

+ Contrario a lo que se cree, los problemas de interpretabilidad que normalmente se le atribuyen a modelos más complejos como las redes neuronales están presentes en estos modelos más "sencillos".
