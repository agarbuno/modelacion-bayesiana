#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Bandits~
#+STARTUP: showall
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/11-bandits.pdf
:END:
#+PROPERTY: header-args:R :session bayes-bandits :exports both :results output org :tangle ../rscripts/11-bandits.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex reveal


#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022 | Bandits.\\
*Objetivo*: Con este tema nos encontramos en la parte final del curso. Veremos
 aplicaciones del estado del arte y temas de inferencia aproximada.  En
 particular en esta sección estudiaremos pruebas A/B en el contexto
 Bayesiano. \\
*Lectura recomendada*: El artículo citep:Scott2010. El artículo
 citep:Gronau2020. También encontrarán este [[https://www.dynamicyield.com/lesson/bayesian-testing/][mini-curso]] relevante en la discusión
 de este tema. También el libro citep:Lattimore2020 provee de un tratamiento
 general de este tema.
#+END_NOTES


#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

#+begin_src R :exports none :results none
  ## Librerias para modelacion bayesiana
  library(cmdstanr)
  library(posterior)
  library(bayesplot)
#+end_src

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#disclaimer][Disclaimer]]
- [[#introducción][Introducción]]
- [[#pruebas-ab-estáticas][Pruebas A/B estáticas]]
  - [[#pruebas-ab-bernoulli][Pruebas A/B Bernoulli]]
  - [[#diseño-de-experimentos][Diseño de experimentos]]
- [[#tragamonedas-con--brazos-múltiples][Tragamonedas con  brazos múltiples]]
  - [[#exploración-y-explotación][Exploración y explotación]]
  - [[#diseños-secuenciales][Diseños secuenciales]]
  - [[#pérdidas][Pérdidas]]
- [[#tragamonedas-bernoulli][Tragamonedas Bernoulli]]
- [[#políticas][Políticas]]
  - [[#tipos-de-políticas][Tipos de políticas]]
    - [[#política-round-robin][Política Round Robin:]]
    - [[#política-uniforme][Política uniforme:]]
    - [[#política-toma-y-daca][Política toma y daca:]]
  - [[#política-bayesiana][Política Bayesiana]]
- [[#tragamonedas-bernoulli-en-stan][Tragamonedas Bernoulli en Stan]]
  - [[#estadísticas-suficientes][Estadísticas suficientes]]
- [[#decisiones-decisiones-][Decisiones, decisiones, ...]]
- [[#tragamonedas-contextuales][Tragamonedas contextuales]]
:END:


* /Disclaimer/                                                          :reveal:
:PROPERTIES:
:reveal_background: #00468b
:END:
Con este tema empezamos la tercera parte del curso. La agenda para esta última parte estará constituida por:
1. Pruebas A/B. 
2. Teoría de valor latente. 
3. Inferencia aproximada. 
4. Modelado de tópicos.

* Introducción

En esta sección consideraremos el problema de ~ofrecer versiones distintas~ de un
producto y buscar cuál es la versión que ~mejora ciertas métricas de
interés~. Para esto se recolectan datos al respecto y después se hace inferencia
estadística. En particular, nos concentraremos en experimentación secuencial que
permite al diseñador decidir qué alternativa se presenta en cada iteración.

El objetivo con este mecanismo es diseñar la política que minimice la cantidad
de datos necesarios para poder hacer la inferencia.

Exploraremos la noción de los traga-monedas con brazos múltiples (/multi-armed
bandits/). Cada alternativa otorga una recompensa estocástica y el objetivo es
minimizar el tiempo necesario para explorar las alternativas para encontrar la
mejor.

El problema de los traga-monedas es un problema clásico en la literatura de
aprendizaje por refuerzo con un problema de decisión secuencial y tiene
aplicaciones en en toma de decisiones para distintos ámbitos como diseño de
páginas web, sistemas de recomendación personalizados, etc.

En particular nos concentraremos en explorar la política de muestreo Thompson,
donde cada brazo del traga-monedas se escoge con probabilidad proporcional a la
probabilidad posterior de obtener la máxima recompensa.

La agenda será:
1. Experimentación Bernoulli.
2. Experimentación secuencial.
3. Experimentación contextual. 


* Pruebas A/B estáticas

Supongamos que estamos hambrientos y nos encontramos en un pueblito con 3
restaurantes. El primero ha recibido 2/2 reseñas positivas; el segundo, 9/10 y
el tercero, 32/40. ¿Cuál escogerías?

Supongamos que administramos un sitio /web/ y tenemos que escoger que /Ads/ tenemos
que poner en la página. Cada /click/ nos da dinero de los
patrocinadores. Supongamos que tenemos tres opciones donde los /Ads/ han recibido
2/2 clicks, 9/10 y 32/40 clicks. ¿Cuál nos conviene?

En ambos problemas el conjunto de datos es el mismo. La pregunta es la misma,
¿qué opción tiene mayor probabilidad de éxito?

En general, los resultados no tienen que ser binarios. Estos pueden ser conteos,
continuos o multivariados. Pero la pregunta es la misma, ¿cuál es la mejor
opción?

\newpage

** Pruebas A/B Bernoulli

Mantengamos por el momento el escenario binario con éxito o fracaso. Asumiremos
que tenemos $K$ objetos para comparar. Para cada $k \in K$, hemos realizado
$N_k$ pruebas de las cuales $y_k$ han sido éxitos.

El modelo tiene parámetros $\theta_k \in (0,1)$ para cada una de las
posibilidades, que representa las probabilidades de éxito. Nuestro objetivo será
definir cuál es la mejor opción. Esto es, qué opción tiene la mejor posibilidad
de éxito.

De momento asumiremos previas
\begin{align}
\theta_k \sim \mathsf{Uniforme}(0,1)\,.
\end{align}
El modelo de verosimilitud es
\begin{align}
y_k \sim \mathsf{Binomial}(N_k, \theta_k)\,.
\end{align}

Por último, queremos saber cuál es la mejor opción. Esto lo podemos escribir como
\begin{align*}
\mathbb{P}[ \text{ la mejor opción es  }k | y] &= \mathbb{E} [I[\theta_k \geq \max \theta] | y ]\\
&= \int I[\theta_k \geq \max \theta] \, \pi(\theta| y) \, \text{d}\theta\\
&= \frac{1}{M} \sum_{m = 1}^{M} I[\theta_k^{(m)} \geq \max \theta^{(m)}] \,,
\end{align*}
donde $\theta^{(m)} \sim \pi(\theta|y)$ para $m = 1, \ldots, M$.

El código del modelo queda como sigue:
#+begin_src  stan :tangle ../modelos/bandits/tragamonedas-ab.stan
  data {
    int<lower=1> K;
    array[K] int<lower=0> N;
    array[K] int<lower=0> y;
  }
  parameters {
    vector<lower=0, upper=1>[K] theta;
  }
  model {
    y ~ binomial(N, theta);
  }
  generated quantities {
    array[K] int<lower=0, upper=1> mejor_ix;
    {
      real max_prob = max(theta);
      for (k in 1:K) {
        mejor_ix[k] = theta[k] >= max_prob;
      }
    }
  }
#+end_src

#+begin_src R :exports none :results none
  modelos_files <- "modelos/compilados/bandits"
  ruta <- file.path("modelos/bandits/tragamonedas-ab.stan")
  modelo <- cmdstan_model(ruta, dir = modelos_files)
#+end_src

#+begin_src R :exports code :results none
  data.list <- list(K = 3, y = c(2, 9, 32), N = c(2, 10, 40))
  posterior <- modelo$sample(data = data.list, refresh = 1000)
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/bandit-bernoulli.jpeg :exports results :results output graphics file
g1 <- bayesplot::mcmc_intervals(posterior$draws(), regex_pars = "theta") +
  sin_lineas
g2 <- bayesplot::mcmc_hist(posterior$draws(), regex_pars = "theta") +
  sin_lineas + xlim(0,1)
g2/ g1 + patchwork::plot_layout(heights = c(3, 2))
#+end_src
#+caption: Resúmenes gráficos de la distribución posterior con los datos de los restaurantes. 
#+RESULTS:
[[file:../images/bandit-bernoulli.jpeg]]

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/bandit-bernoulli-ganar.jpeg :exports results :results output graphics file
  posterior$draws(variables = "mejor_ix", format = "df") |>
    as_tibble() |>
    pivot_longer(cols = 1:3) |>
    group_by(name) |>
    summarise(gana = sum(value)) |>
    ggplot(aes(x = 1:3, y = gana)) +
    geom_col() +
    xlab("Opción") + ylab("Conteos") + sin_lineas
#+end_src
#+caption: Registro de opciones ganadoras bajo la distribución posterior. 
#+ATTR_LATEX: :width .5\linewidth
#+RESULTS:
[[file:../images/bandit-bernoulli-ganar.jpeg]]

** Diseño de experimentos

Necesitamos datos para poder determinar cuál es la mejor opción. Para esto se
debe de diseñar un experimento para poder determinar el tamaño de muestra
necesario para determinar diferencias significativas en las alternativas
posibles.

El diseño asume que cada opción es ~intercambiable~ y por lo tanto se trata sobre
establecer cuántas veces se tienen que probar cada opción.

#+BEGIN_NOTES
Intercambiabilidad en las opciones no quiere decir que éstas sean idénticas. Lo que
quiere decir es que /a priori/ las opciones no son identificables.
#+END_NOTES

* Tragamonedas con  brazos múltiples 

En este escenario tomamos una colección de opciones. Suponemos que cada opción
tiene recompensas $\mathsf{iid}$. Esto implica que cada opción siempre tendrá la
misma probabilidad para las recompensas, independiente del número de veces que
se juegue cada opción. Cada vez que se utiliza una de las opciones tenemos un
evento independiente.

** Exploración y explotación

En este contexto hablamos que tenemos que /explorar/ la distribución de
recompensas de cada una de las opciones y que tendremos que /explotar/ nuestro
conocimiento sobre la opción que genera mejores retornos.

Llamamos ~política~ a la forma en que exploramos las posibilidades. Nota que las elecciones
no tienen que ser deterministas.

** Diseños secuenciales

Podemos considerar una política que vaya cambiando la forma en que se van
escogiendo las opciones. Esto es, ajustar la forma en que escogemos las opciones
considerando los resultados previos que hemos observado.

** Pérdidas

Las políticas usualmente se comparan considerando la pérdida
esperada. Es decir, el valor esperado de la diferencia de las recompensas entre
escoger siempre la mejor opción contra la que escogimos nosotros.

* Tragamonedas Bernoulli

Consideremos que hay $K$ brazos en el tragamonedas y consideremos que tenemos
$N$ iteraciones del proceso. En este caso, consideramos $n \in \{1, \ldots, N\}$
donde hemos escogido el brazo $z_n \in \{1, \ldots, K\}$ y también hemos
recibido una recompensa $y_n \in \mathbb{R}$.

El supuesto mas fuerte que hacemos es que cada opción tiene la misma
distribución de recompensas. Independiente del número de veces que se ha
utilizado o de la historia que hemos observado.

Asumimos que las recompensas tienen distribución
\begin{align}
y_n \sim \mathsf{Bernoulli}(\theta_{[z_n]})\,.
\end{align}

* Políticas

Un tomador de decisiones está definido en términos de la estrategia que seguirá
para escoger las opciones basado en lo que ha observado en sus decisiones pasadas.
Para ser efectivo, se tendrá que balancear entre explorar y explotar las opciones.
Matemáticamente consideramos políticas estocásticas por medio de distribuciones
\begin{align}
\pi(z_{n+1} | y_{1:n}, z_{1:n})\,.
\end{align}

** Tipos de políticas

1. Políticas Markovianas, $\pi(z_{n+1} | y_{n}, z_{n})$.
2. Políticas sin memoria, $\pi(z_{n+1})$.
3. Política determinista, $z_{n+1} = f(y_{1:n}, z_{1:n})$.

*** Política /Round Robin/: 

Tomar la política como decisiones en secuencia
\begin{align}
z = 1, 2, \ldots, K, 1, 2, \ldots, K, 1, 2, \ldots, K, \ldots, 
\end{align}
preserva la idea de que cada opción se tomará de manera uniforme con la misma
proporción.

*** Política uniforme:

Se tomará cada opción con una probabilidad equiprobable
\begin{align}
\pi(z_{n+1} | y_{1:n}, z_{1:n}) = \mathsf{Categorical}\left( \frac1K, \ldots, \frac1K \right)\,.
\end{align}

*** Política toma y daca:

Se escoge una opción hasta que deja de dar recompensas, después, se cambia a la siguiente opción. Se empieza con la opción $z_n = 1$ y después se escogen las opciones de acuerdo a
\begin{align}
z_{n+1} = \begin{cases}
z_n &\text{ si } \\
z_n + 1 &\text{ si } y_n = 0 \text{ y } z_n < K \\
1 &\text{ si } y_n = 0 \text{ y } z_n = K \\
\end{cases}. 
\end{align}

** Política Bayesiana

citet:Thompson1933 introdujo una política que incorpora la historia de las
recompensas. Cada opción se escoge de acuerdo a la probabilidad de ser la mejor
hasta el momento. Dados los parámetros $\theta = (\theta_1, \ldots, \theta_K)$,
se considera que la opción $k$ es la mejor si $\theta_k = \max \theta$.

Las opciones se escogen de acuerdo
\begin{align}
z_n \sim \mathsf{Categorical}(\phi_n)\,,
\end{align}
donde $\sum \phi_{n,k} = 1$.

De acuerdo a los supuesto de recompensas Bernoulli y el supuesto de
intercambiabilidad escogemos una previa
\begin{align}
\theta_k \sim \mathsf{Beta}(\alpha, \alpha)\,.
\end{align}

Dado el modelo Bayesiano podemos escribir 
\begin{align*}
\phi_{k,n} &= \mathbb{P}[ \theta_k = \max \theta | y_{1:n}, z_{1:n}] \\
&= \mathbb{E} [I[\theta_k \geq \max \theta] | y_{1:n}, z_{1:n} ]\\
&= \int I[\theta_k \geq \max \theta] \, \pi(\theta| y_{1:n}, z_{1:n}) \, \text{d}\theta\\
&= \frac{1}{M} \sum_{m = 1}^{M} I[\theta_k^{(m)} \geq \max \theta^{(m)}] \,,
\end{align*}
donde $\theta^{(m)} \sim \pi(\theta|y_{1:n}, z_{1:n})$ para $m = 1, \ldots, M$.

* Tragamonedas Bernoulli en ~Stan~

El modelo lo implementamos como sigue

#+begin_src stan :tangle ../modelos/bandits/tragamonedas-bernoulli.stan
  data {
    int<lower=1> K; 
    int<lower=0> N; 
    array[N] int<lower=1, upper=K> z; 
    array[N] int<lower=0, upper=1> y; 
  }
  parameters {
    vector<lower=0, upper=1>[K] theta;
  }
  model {
    theta ~ beta(1, 1); 
    y ~ bernoulli(theta[z]); 
  }
  generated quantities {
    simplex[K] mejor_ix; 
    {
      real mejor_prob = max(theta);
      for (k in 1 : K) {
        mejor_ix[k] = theta[k] >= mejor_prob;
      }
      mejor_ix /= sum(mejor_ix); 
    }
  }
#+end_src

#+begin_src R :exports none :results none
  ruta <- file.path("modelos/bandits/tragamonedas-bernoulli.stan")
  modelo <- cmdstan_model(ruta, dir = modelos_files)
#+end_src

** Estadísticas suficientes

El código anterior puede ser lento pues los experimentos son Bernoulli. Se puede
hacer el código mas eficiente si agrupamos para tener experimentos Binomiales. El agrupado se puede hacer desde ~Stan~
#+begin_src stan
  transformed data {
    int<lower = 0> experimentos[K] = rep_array(0, K);
    int<lower = 0> exitos[K] = rep_array(0, K);
    for (n in 1:N) {
      experimentos[z[n]] += 1;
      exitos[z[n]] += y[n];
    }
  }
#+end_src

Y utilizaríamos un modelo
#+begin_src stan
  model {
    theta ~ beta(1, 1);
    exitos ~ binomial(experimentos, theta);
  }
#+end_src

Así que el código queda
#+begin_src stan :tangle ../modelos/bandits/tragamonedas-conjugado.stan
  data {
    int<lower=1> K; 
    int<lower=0> N; 
    array[N] int<lower=1, upper=K> z; 
    array[N] int<lower=0, upper=1> y; 
  }
  transformed data {
    array[K] int<lower = 0> experimentos = rep_array(0, K);
    array[K] int<lower = 0> exitos = rep_array(0, K);
    for (n in 1:N) {
      experimentos[z[n]] += 1;
      exitos[z[n]] += y[n];
    }
  }
  generated quantities {
    array[K] real<lower = 0, upper = 1> theta;
    for (k in 1:K)
      theta[k] = beta_rng(1 + exitos[k], 1 + experimentos[k] - exitos[k]);
  
    simplex[K] mejor_ix; 
    {
      real mejor_prob = max(theta);
      for (k in 1 : K) {
        mejor_ix[k] = theta[k] >= mejor_prob;
        }
        mejor_ix /= sum(mejor_ix); 
      }
    }
#+end_src

#+begin_src R :exports none :results none
  ruta <- file.path("modelos/bandits/tragamonedas-conjugado.stan")
  modelo <- cmdstan_model(ruta, dir = modelos_files)
#+end_src

Lo que va a cambiar con los ejemplos anteriores que hemos visto en el curso es
que haremos una actualización Bayesiana secuencial y necesitaremos hacer unos
pequeños cambios en la forma que interactuamos con el código.

#+BEGIN_NOTES
Hacer inferencia secuencial no es trivial y son sólo estos casos donde podemos
explotar ciertas propiedades de nuestros modelos. El área de /Asimilación de
datos/ (citep:Law2015,Reich2015) y los métodos secuenciales Monte Carlo como los
filtros de partículas (citep:DelMoral2006) son instancias donde se estudian y
proponen nuevos algoritmos con buenas propiedades teóricas.
#+END_NOTES

#+begin_src R :exports code :results none :eval never 
  ## Declaramos el problema
  K <- 2
  theta <- c(0.05, 0.04)
  N <- 5000

  ## Inicializamos
  p_best <- matrix(0, N, K)
  r_hat <- matrix(0, N, K)
  y <- array(0.0, 0)
  z <- array(0.0, 0)
  prefix <- function(y, n) array(y, dim = n - 1)

  ## Hacemos el aprendizaje secuencial
  for (n in 1:N) {
    data <- list(K = K, N = n - 1, y = prefix(y, n), z = prefix(z, n))
    posterior <- modelo$sample(data, fixed_param = TRUE,
                               chains = 1, iter_sampling = 1000, refresh = 0)
    p_best[n, ] <- posterior$summary(variables = "mejor_ix")$mean
    r_hat[n, ] <- posterior$summary(variables = "theta")$rhat
    z[n] <- sample(K, 1, replace = TRUE, p_best[n, ])
    y[n] <- rbinom(1, 1, theta[z[n]])
  }
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/bandits-policy.jpeg :exports results :results output graphics file :eval never
  g1 <- tibble(rhat = r_hat[,1]) |>
    ggplot(aes(rhat)) +
    geom_histogram() + sin_lineas


  g2 <- tibble(p1 = p_best[,1],
         Turno = 1:N) |>
    ggplot(aes(Turno, p1)) +
    geom_line() + sin_lineas +
    scale_x_log10() +
    ylab("P[θ1 > θ2]")


  g1 + g2
#+end_src
#+caption: Histogramas del diagnóstico $\hat R_n$ y trayectoria de la probabilidad posterior de que $\theta_1 > \theta_2$. 
#+RESULTS:
[[file:../images/bandits-policy.jpeg]]

La figura anterior nos muestra un resultado bastante poderoso. El aprendizaje
secuencial Bayesiano sobre la incertidumbre en las tasas de recompensas nos
puede ayudar a identificar con alta probabilidad la mejor opción. Un análisis de
potencia frecuentista nos diría que necesitamos hasta 10 veces más experimentos
para detectar la proporción correcta.

#+begin_src R :exports both :results org 
  power.prop.test(p1 = .05, p2 = .04, power = .95)
#+end_src

#+RESULTS:
#+begin_src org

     Two-sample comparison of proportions power calculation 

              n = 11166
             p1 = 0.05
             p2 = 0.04
      sig.level = 0.05
          power = 0.95
    alternative = two.sided

NOTE: n is number in *each* group
#+end_src



* Decisiones, decisiones, ...

En el marco de teoría de la decisión utilizaremos la opción maximice la utilidad
esperada. Esto es, nuestra política óptima será aquella que en cada turno $n$
escogerá
\begin{align}
k^*_n = \arg \max_{k = 1, \ldots, K} \mathbb{E} [Y_k | y_{1:n}, z_{1:n}]\,,
\end{align}
donde
\begin{align}
\mathbb{E}[Y_k | y_{1:n}, z_{1:n}]  = \int y_{k} \, \pi(y_k | y_{1:n}, z_{1:n}) \, \text{d}y_k\,.
\end{align}

* Tragamonedas contextuales

Se pueden utilizar modelos predictivos para obtener recompensas
contextuales. Esto se utiliza en sistemas de recomendación personalizados. Para
esto, utilizamos covariables que nos ayuden a modelar de mejor manera
\begin{align}
\mathbb{E}[Y_k|X_k]\,,
\end{align}
donde se pueden utilizar cualquier modelo de regresión generalizada, o modelos
basados en /splines/, o modelos BART (ver citep:Martin2021,Li2010b).

Alternativas --y una breve revisión de literatura-- también se pueden encontrar
en el artículo citep:Gronau2020. Por último, [[https://www.youtube.com/watch?v=kY-BCNHd_dM][la sesión de conferencia]] en
tragamonedas de brazos múltiples por parte del equipo de ciencia de datos de
Netflix es muy informativa sobre el tema. 

bibliographystyle:abbrvnat
bibliography:references.bib

