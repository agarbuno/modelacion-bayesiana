@book{bda,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC Press}
}

@book{bayes-regression,
  title={Regression and other stories},
  author={Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year={2020},
  publisher={Cambridge University Press}
}


@book{kruschke,
  author = {Kruschke, John},
  keywords = {diss imported inthesis mixedtrails},
  publisher = {Academic Press},
  title = {Doing Bayesian Data Analysis (Second Edition)},
  year = 2015
}

@book{sivia,
  title={Data analysis: a Bayesian tutorial},
  author={Sivia, Devinderjit and Skilling, John},
  year={2006},
  publisher={OUP Oxford}
}


@book{gelman-hill,
  added-at = {2011-08-15T12:47:13.000+0200},
  asin = {052168689X},
  author = {Gelman, Andrew and Hill, Jennifer},
  biburl = {https://www.bibsonomy.org/bibtex/201c2497e4ffea441a9835d0f05160dd7/vivion},
  description = {Amazon.com: Data Analysis Using Regression and Multilevel/Hierarchical Models (9780521686891): Andrew Gelman, Jennifer Hill: Books},
  dewey = {519.536},
  ean = {9780521686891},
  edition = 1,
  interhash = {3a8313c400b72653645c195a19c1eb02},
  intrahash = {01c2497e4ffea441a9835d0f05160dd7},
  isbn = {052168689X},
  keywords = {statistics},
  publisher = {Cambridge University Press},
  timestamp = {2011-08-15T12:47:13.000+0200},
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  url = {http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?s=books&ie=UTF8&qid=1313405184&sr=1-1},
  year = 2006
}

@book{liuMonte,
  title={Monte Carlo strategies in scientific computing},
  author={Liu, Jun S},
  year={2008},
  publisher={Springer Science \& Business Media}
}


@book{mcmcStability,
  title={Markov chains and stochastic stability},
  author={Meyn, Sean P and Tweedie, Richard L},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{stan,
  title={Stan: A probabilistic programming language},
  author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  journal={Journal of Statistical Software},
  volume={76},
  number={1},
  year={2017},
  publisher={Columbia University}
}


@article{inverse-problems,
  title={Inverse problems and data assimilation},
  author={Sanz-Alonso, Daniel and Stuart, Andrew M and Taeb, Armeen},
  journal={arXiv preprint arXiv:1810.06191},
  year={2018}
}

@article{metropolis,
  title={The monte carlo method},
  author={Metropolis, Nicholas and Ulam, Stanislaw},
  journal={Journal of the American statistical association},
  volume={44},
  number={247},
  pages={335--341},
  year={1949},
  publisher={Taylor \& Francis}
}


@article{hastings,
  title={Monte Carlo sampling methods using Markov chains and their applications},
  author={Hastings, W Keith},
  year={1970},
  publisher={Oxford University Press}
}


@book{monteHandbook,
  title={Handbook of markov chain monte carlo},
  author={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year={2011},
  publisher={CRC press}
}

@article{twalk,
  title={A general purpose sampling algorithm for continuous distributions (the t-walk)},
  author={Christen, J Andr{\'e}s and Fox, Colin},
  journal={Bayesian Analysis},
  volume={5},
  number={2},
  pages={263--281},
  year={2010},
  publisher={International Society for Bayesian Analysis}
}


@article{mala,
  title={{Optimal scaling of discrete approximations to Langevin diffusions}},
  author={Roberts, Gareth O and Rosenthal, Jeffrey S},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={60},
  number={1},
  pages={255--268},
  year={1998},
  publisher={Wiley Online Library}
}

@article{LangevinMonte,
  title={{Exponential convergence of Langevin distributions and their discrete approximations}},
  author={Roberts, Gareth O and Tweedie, Richard L},
  journal={Bernoulli},
  volume={2},
  number={4},
  pages={341--363},
  year={1996},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}


@article{manifold,
  title={Riemann manifold Langevin and hamiltonian monte carlo methods},
  author={Girolami, Mark and Calderhead, Ben},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={73},
  number={2},
  pages={123--214},
  year={2011},
  publisher={Wiley Online Library}
}

@article{nuts,
  title={The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={J. Mach. Learn. Res.},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}
@book{robertCasella,
  title={{Monte Carlo statistical methods}},
  author={Robert, Christian and Casella, George},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@incollection{nealHMC,
  title={{MCMC using Hamiltonian dynamics}},
  author={Neal, Radford M},
  editor={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  booktitle={{Handbook of Markov chain Monte Carlo}},
  year={2011},
  publisher={CRC Press}
}

@article{geyer,
  title={{Practical Markov chain Monte Carlo}},
  author={Geyer, Charles J},
  journal={Statistical Science},
  pages={473--483},
  year={1992},
  publisher={JSTOR}
}


@article{conceptual,
  title={{A conceptual introduction to Hamiltonian Monte Carlo}},
  author={Betancourt, Michael},
  journal={arXiv preprint arXiv:1701.02434},
  year={2017}
}

@article{vehtariRank,
  title={{Rank-normalization, folding, and localization: An improved Rhat for assessing convergence of MCMC}},
  author={Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  journal={Bayesian Analysis},
  year={2020},
  publisher={International Society for Bayesian Analysis}
}


@article{bfmi,
  title={{Diagnosing suboptimal cotangent disintegrations in Hamiltonian Monte Carlo}},
  author={Betancourt, Michael},
  journal={arXiv preprint arXiv:1604.00695},
  year={2016}
 }

@article{TaltsEtAl,
	Author = {Sean Talts and Michael Betancourt and Daniel Simpson and Aki Vehtari and Andrew Gelman},
	Month = {04},
	Title = {Validating Bayesian Inference Algorithms with Simulation-Based Calibration},
	Year = {2018}}

@article{HolmesGolf,
	author = {Holmes,Brian W. },
	title = {Putting: How a golf ball and hole interact},
	journal = {American Journal of Physics},
	volume = {59},
	number = {2},
	pages = {129-136},
	year = {1991},
	doi = {10.1119/1.16592}}

@article{BetancourtCase,
	Author = {Betancourt, Michael},
	Title = {Towards A Principled Bayesian Workflow},
	url = {https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html},
	Year = {2020}}

@article{GolfCase,
    author = {Gelman, Andrew},
    title = {Model building and expansion for golf putting},
    year = 2019,
    url ={https://mc-stan.org/users/documentation/case-studies/golf.html}
}

@article{GelmanNolan,
    author = {Gelman, Andrew and Nolan, Deborah},
    year = {2002},
    month = {01},
    pages = {93 - 95},
    title = {A Probability Model for Golf Putting},
    volume = {24},
    journal = {Teaching Statistics},
    doi = {10.1111/1467-9639.00097}
}

@article{PennerPutting,
    author = {Penner, Albert},
    year = {2002},
    month = {02},
    pages = {83-96},
    title = {The physics of putting},
    volume = {80},
    journal = {Canadian Journal of Physics},
    doi = {10.1139/p01-137}
}

@article{Gelman2020,
  title={Bayesian Workflow},
  author={Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  journal={arXiv preprint arXiv:2011.01808},
  year={2020}
}


@article{rubin,
  title={Estimation in parallel randomized experiments},
  author={Rubin, Donald B},
  journal={Journal of Educational Statistics},
  volume={6},
  number={4},
  pages={377--401},
  year={1981},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


@book{Hig,
	Address = {Philadelphia, PA},
	Author = {Higham, Nicholas J.},
	Date-Added = {2013-10-01 06:19:01 -0700},
	Date-Modified = {2013-10-01 06:19:01 -0700},
	Doi = {10.1137/1.9780898719550},
	Edition = {Second},
	Isbn = {0-89871-420-6},
	Mrclass = {00A20},
	Mrnumber = {1640787 (99g:00017)},
	Pages = {xvi+302},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM)},
	Title = {Handbook of writing for the mathematical sciences},
	Url = {http://dx.doi.org/10.1137/1.9780898719550},
	Year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/1.9780898719550}}

@article{Hal,
	Author = {Halmos, P. R.},
	Date-Added = {2013-10-01 06:18:55 -0700},
	Date-Modified = {2013-10-01 06:18:55 -0700},
	Fjournal = {L'Enseignement Math\'ematique. Revue Internationale. IIe S\'erie},
	Issn = {0013-8584},
	Journal = {Enseignement Math. (2)},
	Mrclass = {00.00},
	Mrnumber = {0277319 (43 \#3055)},
	Mrreviewer = {G. Piranian},
	Pages = {123--152},
	Title = {How to write mathematics},
	Volume = {16},
	Year = {1970}}

@misc{mathscinet,
	Date-Added = {2013-10-01 06:18:44 -0700},
	Date-Modified = {2013-10-01 06:18:44 -0700},
	Howpublished = {Online.},
	Key = {MSC},
	Note = {\url{http://www.ams.org/mathscinet/}},
	Publisher = {Americal Mathematical Society},
	Title = {{M}ath{S}ci{N}et Mathematical Reviews},
	Url = {http://www.ams.org/mathscinet/},
	Year = {2013},
	Bdsk-Url-1 = {http://www.ams.org/mathscinet/}
    }

@article{scoringRules,
  title={Strictly Proper Scoring Rules, Prediction, and Estimation},
  author={Gneiting, Tilmann and Raftery, Adrian E.},
  journal={Journal of the American Statistical Association},
  volume={102},
  number={477},
  pages={359--78},
  year={2007},
  publisher=&{American Statistical Association}
}
@article{Gelman2021,
  title = {Holes in {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Yao, Yuling},
  year = {2021},
  month = {jan},
  journal = {Journal of Physics G: Nuclear and Particle Physics},
  volume = {48},
  number = {1},
  pages = {014002},
  issn = {0954-3899, 1361-6471},
  doi = {10.1088/1361-6471/abc3a5},
  abstract = {Every philosophy has holes, and it is the responsibility of proponents of a philosophy to point out these problems. Here are a few holes in Bayesian data analysis: (1) the usual rules of conditional probability fail in the quantum realm, (2) flat or weak priors lead to terrible inferences about things we care about, (3) subjective priors are incoherent, (4) Bayesian decision picks the wrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6) for Cantorian reasons we need to check our models, but this destroys the coherence of Bayesian inference.},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gelman2021 - Holes in Bayesian statistics.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gelman2021 - Holes in Bayesian Statistics2.pdf;/Users/agarbuno/Zotero/storage/SI5YGRFS/2002.html},
}

@book{Kruschke2014,
  title = {Doing {{Bayesian}} Data Analysis: {{A}} Tutorial with {{R}}, {{JAGS}}, and {{Stan}}},
  author = {Kruschke, John},
  year = {2014},
  publisher = {{Academic Press}},
}

@book{Dogucu2021,
  title = {Bayes {{Rules}}! {{An Introduction}} to {{Applied Bayesian Modeling}}},
  author = {Johnson, Alicia and Ott, Miles and Dogucu, Mine},
  year = {2021},
  abstract = {An introduction to applied Bayesian modeling.},
  file = {/Users/agarbuno/Zotero/storage/XLCW9T4W/index.html},
}

@book{Carlin2009a,
  title = {Bayesian Methods for Data Analysis},
  author = {Carlin, Bradley P. and Louis, Thomas A. and Carlin, Bradley P.},
  year = {2009},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  edition = {3rd ed},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  isbn = {978-1-58488-697-6},
  langid = {english},
  lccn = {QA279.5 .C36 2009},
  keywords = {Bayesian statistical decision theory},
  annotation = {OCLC: ocn227205756},
  file = {/Users/agarbuno/Zotero/storage/6UMZU43A/Carlin et al. - 2009 - Bayesian methods for data analysis.pdf},
}

@book{Reich2015,
  title = {Probabilistic Forecasting and {{Bayesian}} Data Assimilation},
  author = {Reich, Sebastian and Cotter, Colin},
  year = {2015},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-1-107-06939-8 978-1-107-66391-6},
  langid = {english},
  lccn = {QA279.5 .R45 2015},
  keywords = {Bayesian statistical decision theory,Probabilities,Uncertainty (Information theory)},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Reich2015 - Probabilistic forecasting and Bayesian data assimilation.pdf},
}

@article{Sanz-Alonso2019,
  title = {Inverse {{Problems}} and {{Data Assimilation}}},
  author = {{Sanz-Alonso}, Daniel and Stuart, Andrew M. and Taeb, Armeen},
  year = {2019},
  month = {jul},
  journal = {arXiv:1810.06191 [stat]},
  eprint = {1810.06191},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {These notes are designed with the aim of providing a clear and concise introduction to the subjects of Inverse Problems and Data Assimilation, and their inter-relations, together with citations to some relevant literature in this area. The first half of the notes is dedicated to studying the Bayesian framework for inverse problems. Techniques such as importance sampling and Markov Chain Monte Carlo (MCMC) methods are introduced; these methods have the desirable property that in the limit of an infinite number of samples they reproduce the full posterior distribution. Since it is often computationally intensive to implement these methods, especially in high dimensional problems, approximate techniques such as approximating the posterior by a Dirac or a Gaussian distribution are discussed. The second half of the notes cover data assimilation. This refers to a particular class of inverse problems in which the unknown parameter is the initial condition of a dynamical system, and in the stochastic dynamics case the subsequent states of the system, and the data comprises partial and noisy observations of that (possibly stochastic) dynamical system. We will also demonstrate that methods developed in data assimilation may be employed to study generic inverse problems, by introducing an artificial time to generate a sequence of probability measures interpolating from the prior to the posterior.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Sanz-Alonso2019 - Inverse Problems and Data Assimilation.pdf;/Users/agarbuno/Zotero/storage/XCRBSU2E/1810.html},
}
@book{Meyn1993,
  title = {Markov {{Chains}} and {{Stochastic Stability}}},
  author = {Meyn, Sean P. and Tweedie, Richard L.},
  year = {1993},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-3267-7},
  isbn = {978-1-4471-3269-1 978-1-4471-3267-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Meyn1993 - Markov Chains and Stochastic Stability.pdf},
}
@book{Wickham2019,
  title = {Advanced {{R}}, {{Second Edition}}},
  author = {Wickham, Hadley},
  year = {2019},
  month = {may},
  publisher = {{CRC Press}},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimisingyour code.By reading this book, you will learn: The difference between an object and its name, and why the distinction is important The important vector data structures, how they fit together, and how you can pull them apart using subsetting The fine details of functions and environments The condition system, which powers messages, warnings, and errors The powerful functional programming paradigm, which can replace many for loops The three most important OO systems: S3, S4, and R6 The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation Effective debugging techniques that you can deploy, regardless of how your code is run How to find and remove performance bottlenecks The second edition is a comprehensive update: New foundational chapters: "Names and values," "Control flow," and "Conditions" comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them Much deeper coverage of metaprogramming, including the new tidy evaluation framework use of new package like rlang (http://rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (http://purrr.tidyverse.org/) for functional programming Use of color in code chunks and figuresHadley Wickham is Chief Scientist at RStudio, an Adjunct Professor at Stanford University and the University of Auckland, and a member of the R Foundation. He is the lead developer of the tidyverse, a collection of R packages, including ggplot2 and dplyr, designed to support data science. He is also the author of R for Data Science (with Garrett Grolemund), R Packages, and ggplot2: Elegant Graphics for Data Analysis.},
  googlebooks = {JAOaDwAAQBAJ},
  isbn = {978-1-351-20129-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Reference / General},
}
@article{Roberts1998,
  author = {Roberts, Gareth O and Rosenthal, Jeffrey S},
  title = {Optimal Scaling of Discrete Approximations To Langevin Diffusions},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {60},
  number = {1},
  pages = {255--268},
  year = {1998},
  publisher = {Wiley Online Library},
}
@article{Girolami2011,
  title = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods: {{Riemann Manifold Langevin}} and {{Hamiltonian Monte Carlo Methods}}},
  shorttitle = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods},
  author = {Girolami, Mark and Calderhead, Ben},
  year = {2011},
  month = {mar},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {2},
  pages = {123--214},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2010.00765.x},
  abstract = {The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis\textendash Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain.The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from www.ucl.ac.uk/statistics/ research/rmhmc allows replication of all the results reported.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Girolami2011 - Riemann manifold Langevin and Hamiltonian Monte Carlo methods.pdf},
}
@article{Byrne2013,
  title = {Geodesic {{Monte Carlo}} on {{Embedded Manifolds}}: {{Geodesic Monte Carlo}}},
  shorttitle = {Geodesic {{Monte Carlo}} on {{Embedded Manifolds}}},
  author = {Byrne, Simon and Girolami, Mark},
  year = {2013},
  month = {dec},
  journal = {Scandinavian Journal of Statistics},
  volume = {40},
  number = {4},
  pages = {825--845},
  issn = {03036898},
  doi = {10.1111/sjos.12036},
  abstract = {Markov chain Monte Carlo methods explicitly defined on the manifold of probability distributions have recently been established. These methods are constructed from diffusions across the manifold and the solution of the equations describing geodesic flows in the Hamilton\textendash Jacobi representation. This paper takes the differential geometric basis of Markov chain Monte Carlo further by considering methods to simulate from probability distributions that themselves are defined on a manifold, with common examples being classes of distributions describing directional statistics. Proposal mechanisms are developed based on the geodesic flows over the manifolds of support for the distributions, and illustrative examples are provided for the hypersphere and Stiefel manifold of orthonormal matrices.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Byrne2013 - Geodesic Monte Carlo on Embedded Manifolds.pdf},
}
@book{Pavliotis2014,
  title = {Stochastic {{Processes}} and {{Applications}}: {{Diffusion Processes}}, the {{Fokker-Planck}} and {{Langevin Equations}}},
  shorttitle = {Stochastic {{Processes}} and {{Applications}}},
  author = {Pavliotis, Grigorios A.},
  year = {2014},
  series = {Texts in {{Applied Mathematics}}},
  volume = {60},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-1323-7},
  isbn = {978-1-4939-1322-0 978-1-4939-1323-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Pavliotis2014 - Stochastic Processes and Applications.pdf},
}
@article{Jorgensen1983a,
  title = {Comparison of Simple Potential Functions for Simulating Liquid Water},
  author = {Jorgensen, William L. and Chandrasekhar, Jayaraman and Madura, Jeffry D. and Impey, Roger W. and Klein, Michael L.},
  year = {1983},
  month = {jul},
  journal = {The Journal of Chemical Physics},
  volume = {79},
  number = {2},
  pages = {926--935},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.445869},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/HZ32TPAQ/Jorgensen et al. - 1983 - Comparison of simple potential functions for simul.pdf},
}
@article{Neal2011,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  author = {Neal, Radford M.},
  year = {2011},
  month = {may},
  journal = {arXiv:1206.1901 [physics, stat]},
  eprint = {1206.1901},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  doi = {10.1201/b10905},
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Neal2011 - MCMC using Hamiltonian dynamics.pdf},
}
@article{Betancourt2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = {jul},
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Betancourt2018 - A Conceptual Introduction to Hamiltonian Monte Carlo.pdf},
}
@article{Duane1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Duane, Simon and Kennedy, Anthony D and Pendleton, Brian J and Roweth, Duncan},
  year = {1987},
  journal = {Physics letters B},
  volume = {195},
  number = {2},
  pages = {216--222},
}
@book{McElreath2020,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical Rethinking},
  author = {McElreath, Richard},
  year = {2020},
  series = {{{CRC}} Texts in Statistical Science},
  edition = {Second},
  publisher = {{Taylor and Francis, CRC Press}},
  address = {{Boca Raton}},
  abstract = {"Statistical Rethinking: A Bayesian Course with Examples in R and Stan, Second Edition builds knowledge/confidence in statistical modeling. Pushes readers to perform step-by-step calculations (usually automated.) Unique, computational approach ensures readers understand details to make reasonable choices and interpretations in their modeling work"--},
  isbn = {978-0-367-13991-9},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/McElreath2020 - Statistical rethinking.pdf},
}
@article{Carpenter2017,
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  title = {Stan: a Probabilistic Programming Language},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {nil},
  year = {2017},
  doi = {10.18637/jss.v076.i01},
  url = {https://doi.org/10.18637/jss.v076.i01},
  DATE_ADDED = {Tue Jun 29 15:03:33 2021},
}
@article{Hoffman2011,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2011},
  month = {nov},
  journal = {arXiv:1111.4246 [cs, stat]},
  eprint = {1111.4246},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Hoffman2011 - The No-U-Turn Sampler.pdf;/Users/agarbuno/Zotero/storage/CWMVQ7J4/1111.html},
}
@book{Liu2004,
  title = {Monte {{Carlo Strategies}} in {{Scientific Computing}}},
  author = {Liu, Jun S.},
  year = {2004},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-76371-2},
  isbn = {978-0-387-76369-9 978-0-387-76371-2},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Liu2004 - Monte Carlo Strategies in Scientific Computing.pdf},
}
@book{Brooks2011,
  title = {Handbook of Markov Chain Monte Carlo},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year = {2011},
  publisher = {{CRC press}},
}
@inproceedings{Hornik2003,
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  booktitle = {Proceedings of {{DSC}}},
  author = {Hornik, Kurt and Leisch, Friedrich and Zeileis, Achim},
  year = {2003},
  volume = {2},
  pages = {1--1},
}
@article{Salvatier2016,
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  author = {Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher},
  year = {2016},
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e55},
}
@article{Christen2010,
  title = {A General Purpose Sampling Algorithm for Continuous Distributions (the t-Walk)},
  author = {Christen, J. Andr{\'e}s and Fox, Colin},
  year = {2010},
  month = {jun},
  journal = {Bayesian Analysis},
  volume = {5},
  number = {2},
  pages = {263--281},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/10-BA603},
  abstract = {We develop a new general purpose MCMC sampler for arbitrary continuous distributions that requires no tuning. We call this MCMC the t-walk. The t-walk maintains two independent points in the sample space, and all moves are based on proposals that are then accepted with a standard Metropolis-Hastings acceptance probability on the product space. Hence the t-walk is provably convergent under the usual mild requirements. We restrict proposal distributions, or `moves', to those that produce an algorithm that is invariant to scale, and approximately invariant to affine transformations of the state space. Hence scaling of proposals, and effectively also coordinate transformations, that might be used to increase efficiency of the sampler, are not needed since the t-walk's operation is identical on any scaled version of the target distribution. Four moves are given that result in an effective sampling algorithm. We use the simple device of updating only a random subset of coordinates at each step to allow application of the t-walk to high-dimensional problems. In a series of test problems across dimensions we find that the t-walk is only a small factor less efficient than optimally tuned algorithms, but significantly outperforms general random-walk M-H samplers that are not tuned for specific problems. Further, the t-walk remains effective for target distributions for which no optimal affine transformation exists such as those where correlation structure is very different in differing regions of state space. Several examples are presented showing good mixing and convergence characteristics, varying in dimensions from 1 to 200 and with radically different scale and correlation structure, using exactly the same sampler. The t-walk is available for R, Python, MatLab and C++ at http://www.cimat.mx/\textasciitilde jac/twalk/},
  keywords = {Bayesian inference,Database Expansion Item,MCMC,simulation,t-walk},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Christen2010 - A general purpose sampling algorithm for continuous distributions (the t-walk).pdf;/Users/agarbuno/Zotero/storage/UHLU7UHP/10-BA603.html},
}
@article{Foreman-Mackey2013,
  title = {Emcee: {{The MCMC Hammer}}},
  shorttitle = {Emcee},
  author = {{Foreman-Mackey}, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
  year = {2013},
  month = {mar},
  journal = {Publications of the Astronomical Society of the Pacific},
  volume = {125},
  number = {925},
  eprint = {1202.3665},
  eprinttype = {arxiv},
  pages = {306--312},
  issn = {00046280, 15383873},
  doi = {10.1086/670067},
  abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman \& Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to \$\textbackslash sim N\^2\$ for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Computational Physics,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Foreman-Mackey2013 - emcee.pdf;/Users/agarbuno/Zotero/storage/IG557KL7/1202.html},
}
@article{Garbuno-Inigo2019,
  title = {Interacting {{Langevin Diffusions}}: {{Gradient Structure And Ensemble Kalman Sampler}}},
  shorttitle = {Interacting {{Langevin Diffusions}}},
  author = {{Garbuno-Inigo}, Alfredo and Hoffmann, Franca and Li, Wuchen and Stuart, Andrew M.},
  year = {2019},
  month = {oct},
  journal = {arXiv:1903.08866 [math]},
  eprint = {1903.08866},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Solving inverse problems without the use of derivatives or adjoints of the forward model is highly desirable in many applications arising in science and engineering. In this paper we propose a new version of such a methodology, a framework for its analysis, and numerical evidence of the practicality of the method proposed. Our starting point is an ensemble of over-damped Langevin diffusions which interact through a single preconditioner computed as the empirical ensemble covariance. We demonstrate that the nonlinear Fokker-Planck equation arising from the mean-field limit of the associated stochastic differential equation (SDE) has a novel gradient flow structure, built on the Wasserstein metric and the covariance matrix of the noisy flow. Using this structure, we investigate large time properties of the Fokker-Planck equation, showing that its invariant measure coincides with that of a single Langevin diffusion, and demonstrating exponential convergence to the invariant measure in a number of settings. We introduce a new noisy variant on ensemble Kalman inversion (EKI) algorithms found from the original SDE by replacing exact gradients with ensemble differences; this defines the ensemble Kalman sampler (EKS). Numerical results are presented which demonstrate its efficacy as a derivative-free approximate sampler for the Bayesian posterior arising from inverse problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Garbuno-Inigo2019 - Interacting Langevin Diffusions.pdf},
}
@article{Rubin1981,
  title = {Estimation in {{Parallel Randomized Experiments}}},
  author = {Rubin, Donald B.},
  year = {1981},
  journal = {Journal of Educational Statistics},
  volume = {6},
  number = {4},
  pages = {377--401},
  publisher = {{[Sage Publications, Inc., American Educational Research Association, American Statistical Association]}},
  issn = {0362-9791},
  doi = {10.2307/1164617},
  abstract = {Many studies comparing new treatments to standard treatments consist of parallel randomized experiments. In the example considered here, randomized experiments were conducted in eight schools to determine the effectiveness of special coaching programs for the SAT. The purpose here is to illustrate Bayesian and empirical Bayesian techniques that can be used to help summarize the evidence in such data about differences among treatments, thereby obtaining improved estimates of the treatment effect in each experiment, including the one having the largest observed effect. Three main tools are illustrated: 1) graphical techniques for displaying sensitivity within an empirical Bayes framework, 2) simple simulation techniques for generating Bayesian posterior distributions of individual effects and the largest effect, and 3) methods for monitoring the adequacy of the Bayesian model specification by simulating the posterior predictive distribution in hypothetical replications of the same treatments in the same eight schools.},
}
@book{Gelman2014a,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year = {2014},
  volume = {2},
  publisher = {{CRC press Boca Raton, FL}}
}
@book{Tarantola2005,
  title = {Inverse {{Problem Theory}} and {{Methods}} for {{Model Parameter Estimation}}},
  author = {Tarantola, Albert},
  year = {2005},
  month = {jan},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898717921},
  isbn = {978-0-89871-572-9 978-0-89871-792-1},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Tarantola2005 - Inverse Problem Theory and Methods for Model Parameter Estimation.pdf},
}
@book{Kaipio2005,
  title = {Statistical and Computational Inverse Problems},
  author = {Kaipio, Jari and Somersalo, Erkki},
  year = {2005},
  series = {Applied Mathematical Sciences},
  number = {v. 160},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-22073-4},
  langid = {english},
  lccn = {QA1 QA377 .A647 vol. 160},
  keywords = {Inverse problems (Differential equations),Numerical solutions},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Kaipio2005 - Statistical and computational inverse problems.pdf},
}
@book{Hastie2009c,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Hastie2009 - The Elements of Statistical Learning.pdf},
}
@article{Piironen2017a,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  month = {jan},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  eprint = {1707.01694},
  eprinttype = {arxiv},
  issn = {1935-7524},
  doi = {10.1214/17-EJS1337SI},
  abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Piironen2017 - Sparsity information and regularization in the horseshoe and other shrinkage.pdf},
}
@article{Fourier1878,
  title={The analytical theory of heat},
  author={Baron Fourier, Jean Baptiste Joseph},
  year={1878},
  publisher={The University Press}
}
@article{Vehtari2021a,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R\textasciicircum}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = {jun},
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  issn = {1936-0975},
  doi = {10.1214/20-BA1221},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Vehtari2021 - Rank-Normalization, Folding, and Localization.pdf},
}
@article{Geyer2002,
  title = {Introduction to {{Markov Chain Monte Carlo}}},
  author = {Geyer, Charles J},
  year = {2002},
  journal = {Handbook of Markov Chain Monte Carlo},
  number = {1990},
  pages = {3--48},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory. The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology. The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
}
@book{Cressie2015,
  title = {Statistics for Spatio-Temporal Data},
  author = {Cressie, Noel and Wikle, Christopher K},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
}
@book{Martin2021,
  title = {Bayesian {{Modeling}} and {{Computation}} in {{Python}}},
  author = {Martin, Osvaldo A. and Kumar, Ravin and Lao, Junpeng},
  year = {2021},
  month = {nov},
  edition = {First},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  doi = {10.1201/9781003019169},
  isbn = {978-1-00-301916-9},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Martin2021 - Bayesian Modeling and Computation in Python.pdf},
}
@article{Gelman2002a,
  title = {A {{Probability Model}} for {{Golf Putting}}},
  author = {Gelman, Andrew and Nolan, Deborah},
  year = {2002},
  journal = {Teaching Statistics},
  volume = {24},
  number = {3},
  pages = {93--95},
  issn = {1467-9639},
  doi = {10.1111/1467-9639.00097},
  abstract = {We derive a model, using trigonometry and the Normal distribution, for the probability that a golf putt is successful. We describe a class activity in which we lead the students through the steps of examining the data, considering possible models, constructing a probability model and checking the fit. The model is,of necessity, oversimplified, a point which the class discusses at the end of the demonstration.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9639.00097},
  file = {/Users/agarbuno/Zotero/storage/APZXX7WS/Gelman and Nolan - 2002 - A Probability Model for Golf Putting.pdf;/Users/agarbuno/Zotero/storage/S8VUMYZY/1467-9639.html},
}
@misc{Gelman2019,
  title = {Model Building and Expansion for Golf Putting},
  author = {Gelman, Andrew},
  year = {2019},
  howpublished = {https://mc-stan.org/users/documentation/case-studies/golf.html},
  file = {/Users/agarbuno/Zotero/storage/PMMLHWB3/golf.html},
}
@article{Penner2002,
  title = {The Physics of Putting},
  author = {Penner, A R},
  year = {2002},
  month = {feb},
  journal = {Canadian Journal of Physics},
  volume = {80},
  number = {2},
  pages = {83--96},
  issn = {0008-4204, 1208-6045},
  doi = {10.1139/p01-137},
  abstract = {The motion of a rolling golf ball on a sloped golf green is modeled. The resulting calculated path of a golf ball is then used, along with a model of the capture of the golf ball by the hole, to determine the resulting launch conditions required for a successful putt. Estimates of the probability of making certain putts are also presented.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Penner2002 - The physics of putting.pdf},
}
@article{Mikkola2021,
  title = {Prior Knowledge Elicitation: {{The}} Past, Present, and Future},
  shorttitle = {Prior Knowledge Elicitation},
  author = {Mikkola, Petrus and Martin, Osvaldo A. and Chandramouli, Suyog and Hartmann, Marcelo and Pla, Oriol Abril and Thomas, Owen and Pesonen, Henri and Corander, Jukka and Vehtari, Aki and Kaski, Samuel and B{\"u}rkner, Paul-Christian and Klami, Arto},
  year = {2021},
  month = {dec},
  journal = {arXiv:2112.01380 [stat]},
  eprint = {2112.01380},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem, in principle. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyze the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Mikkola2021 - Prior knowledge elicitation.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Mikkola2021 - Prior knowledge elicitation2.pdf;/Users/agarbuno/Zotero/storage/9DFNSFKT/2112.html;/Users/agarbuno/Zotero/storage/BVJCYV5Y/2112.html},
}
