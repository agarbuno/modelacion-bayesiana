@book{bda,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC Press}
}

@book{bayes-regression,
  title={Regression and other stories},
  author={Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year={2020},
  publisher={Cambridge University Press}
}


@book{kruschke,
  author = {Kruschke, John},
  keywords = {diss imported inthesis mixedtrails},
  publisher = {Academic Press},
  title = {Doing Bayesian Data Analysis (Second Edition)},
  year = 2015
}

@book{sivia,
  title={Data analysis: a Bayesian tutorial},
  author={Sivia, Devinderjit and Skilling, John},
  year={2006},
  publisher={OUP Oxford}
}


@book{gelman-hill,
  added-at = {2011-08-15T12:47:13.000+0200},
  asin = {052168689X},
  author = {Gelman, Andrew and Hill, Jennifer},
  biburl = {https://www.bibsonomy.org/bibtex/201c2497e4ffea441a9835d0f05160dd7/vivion},
  description = {Amazon.com: Data Analysis Using Regression and Multilevel/Hierarchical Models (9780521686891): Andrew Gelman, Jennifer Hill: Books},
  dewey = {519.536},
  ean = {9780521686891},
  edition = 1,
  interhash = {3a8313c400b72653645c195a19c1eb02},
  intrahash = {01c2497e4ffea441a9835d0f05160dd7},
  isbn = {052168689X},
  keywords = {statistics},
  publisher = {Cambridge University Press},
  timestamp = {2011-08-15T12:47:13.000+0200},
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  url = {http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?s=books&ie=UTF8&qid=1313405184&sr=1-1},
  year = 2006
}

@book{liuMonte,
  title={Monte Carlo strategies in scientific computing},
  author={Liu, Jun S},
  year={2008},
  publisher={Springer Science \& Business Media}
}


@book{mcmcStability,
  title={Markov chains and stochastic stability},
  author={Meyn, Sean P and Tweedie, Richard L},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{stan,
  title={Stan: A probabilistic programming language},
  author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  journal={Journal of Statistical Software},
  volume={76},
  number={1},
  year={2017},
  publisher={Columbia University}
}

@article{metropolis,
  title={The monte carlo method},
  author={Metropolis, Nicholas and Ulam, Stanislaw},
  journal={Journal of the American statistical association},
  volume={44},
  number={247},
  pages={335--341},
  year={1949},
  publisher={Taylor \& Francis}
}


@article{hastings,
  title={Monte Carlo sampling methods using Markov chains and their applications},
  author={Hastings, W Keith},
  year={1970},
  publisher={Oxford University Press}
}


@book{monteHandbook,
  title={Handbook of markov chain monte carlo},
  author={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year={2011},
  publisher={CRC press}
}

@article{twalk,
  title={A general purpose sampling algorithm for continuous distributions (the t-walk)},
  author={Christen, J Andr{\'e}s and Fox, Colin},
  journal={Bayesian Analysis},
  volume={5},
  number={2},
  pages={263--281},
  year={2010},
  publisher={International Society for Bayesian Analysis}
}


@article{mala,
  title={{Optimal scaling of discrete approximations to Langevin diffusions}},
  author={Roberts, Gareth O and Rosenthal, Jeffrey S},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={60},
  number={1},
  pages={255--268},
  year={1998},
  publisher={Wiley Online Library}
}

@article{LangevinMonte,
  title={{Exponential convergence of Langevin distributions and their discrete approximations}},
  author={Roberts, Gareth O and Tweedie, Richard L},
  journal={Bernoulli},
  volume={2},
  number={4},
  pages={341--363},
  year={1996},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}


@article{manifold,
  title={Riemann manifold Langevin and hamiltonian monte carlo methods},
  author={Girolami, Mark and Calderhead, Ben},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={73},
  number={2},
  pages={123--214},
  year={2011},
  publisher={Wiley Online Library}
}

@article{nuts,
  title={The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={J. Mach. Learn. Res.},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}
@book{robertCasella,
  title={{Monte Carlo statistical methods}},
  author={Robert, Christian and Casella, George},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@incollection{nealHMC,
  title={{MCMC using Hamiltonian dynamics}},
  author={Neal, Radford M},
  editor={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  booktitle={{Handbook of Markov chain Monte Carlo}},
  year={2011},
  publisher={CRC Press}
}

@article{geyer,
  title={{Practical Markov chain Monte Carlo}},
  author={Geyer, Charles J},
  journal={Statistical Science},
  pages={473--483},
  year={1992},
  publisher={JSTOR}
}


@article{conceptual,
  title={{A conceptual introduction to Hamiltonian Monte Carlo}},
  author={Betancourt, Michael},
  journal={arXiv preprint arXiv:1701.02434},
  year={2017}
}

@article{vehtariRank,
  title={{Rank-normalization, folding, and localization: An improved Rhat for assessing convergence of MCMC}},
  author={Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  journal={Bayesian Analysis},
  year={2020},
  publisher={International Society for Bayesian Analysis}
}


@article{bfmi,
  title={{Diagnosing suboptimal cotangent disintegrations in Hamiltonian Monte Carlo}},
  author={Betancourt, Michael},
  journal={arXiv preprint arXiv:1604.00695},
  year={2016}
 }

@article{TaltsEtAl,
	Author = {Sean Talts and Michael Betancourt and Daniel Simpson and Aki Vehtari and Andrew Gelman},
	Month = {04},
	Title = {Validating Bayesian Inference Algorithms with Simulation-Based Calibration},
	Year = {2018}}

@article{HolmesGolf,
	author = {Holmes,Brian W. },
	title = {Putting: How a golf ball and hole interact},
	journal = {American Journal of Physics},
	volume = {59},
	number = {2},
	pages = {129-136},
	year = {1991},
	doi = {10.1119/1.16592}}

@article{BetancourtCase,
	Author = {Betancourt, Michael},
	Title = {Towards A Principled Bayesian Workflow},
	url = {https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html},
	Year = {2020}}

@article{GolfCase,
    author = {Gelman, Andrew},
    title = {Model building and expansion for golf putting},
    year = 2019,
    url ={https://mc-stan.org/users/documentation/case-studies/golf.html}
}

@article{GelmanNolan,
    author = {Gelman, Andrew and Nolan, Deborah},
    year = {2002},
    month = {01},
    pages = {93 - 95},
    title = {A Probability Model for Golf Putting},
    volume = {24},
    journal = {Teaching Statistics},
    doi = {10.1111/1467-9639.00097}
}

@article{PennerPutting,
    author = {Penner, Albert},
    year = {2002},
    month = {02},
    pages = {83-96},
    title = {The physics of putting},
    volume = {80},
    journal = {Canadian Journal of Physics},
    doi = {10.1139/p01-137}
}

@article{Gelman2020,
  title={Bayesian Workflow},
  author={Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  journal={arXiv preprint arXiv:2011.01808},
  year={2020}
}


@article{rubin,
  title={Estimation in parallel randomized experiments},
  author={Rubin, Donald B},
  journal={Journal of Educational Statistics},
  volume={6},
  number={4},
  pages={377--401},
  year={1981},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


@book{Hig,
	Address = {Philadelphia, PA},
	Author = {Higham, Nicholas J.},
	Date-Added = {2013-10-01 06:19:01 -0700},
	Date-Modified = {2013-10-01 06:19:01 -0700},
	Doi = {10.1137/1.9780898719550},
	Edition = {Second},
	Isbn = {0-89871-420-6},
	Mrclass = {00A20},
	Mrnumber = {1640787 (99g:00017)},
	Pages = {xvi+302},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM)},
	Title = {Handbook of writing for the mathematical sciences},
	Url = {http://dx.doi.org/10.1137/1.9780898719550},
	Year = {1998},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/1.9780898719550}}

@article{Hal,
	Author = {Halmos, P. R.},
	Date-Added = {2013-10-01 06:18:55 -0700},
	Date-Modified = {2013-10-01 06:18:55 -0700},
	Fjournal = {L'Enseignement Math\'ematique. Revue Internationale. IIe S\'erie},
	Issn = {0013-8584},
	Journal = {Enseignement Math. (2)},
	Mrclass = {00.00},
	Mrnumber = {0277319 (43 \#3055)},
	Mrreviewer = {G. Piranian},
	Pages = {123--152},
	Title = {How to write mathematics},
	Volume = {16},
	Year = {1970}}

@misc{mathscinet,
	Date-Added = {2013-10-01 06:18:44 -0700},
	Date-Modified = {2013-10-01 06:18:44 -0700},
	Howpublished = {Online.},
	Key = {MSC},
	Note = {\url{http://www.ams.org/mathscinet/}},
	Publisher = {Americal Mathematical Society},
	Title = {{M}ath{S}ci{N}et Mathematical Reviews},
	Url = {http://www.ams.org/mathscinet/},
	Year = {2013},
	Bdsk-Url-1 = {http://www.ams.org/mathscinet/}
    }

@article{scoringRules,
  title={Strictly Proper Scoring Rules, Prediction, and Estimation},
  author={Gneiting, Tilmann and Raftery, Adrian E.},
  journal={Journal of the American Statistical Association},
  volume={102},
  number={477},
  pages={359--78},
  year={2007},
  publisher=&{American Statistical Association}
}
@article{Gelman2021,
  title = {Holes in {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Yao, Yuling},
  year = {2021},
  month = {jan},
  journal = {Journal of Physics G: Nuclear and Particle Physics},
  volume = {48},
  number = {1},
  pages = {014002},
  issn = {0954-3899, 1361-6471},
  doi = {10.1088/1361-6471/abc3a5},
  abstract = {Every philosophy has holes, and it is the responsibility of proponents of a philosophy to point out these problems. Here are a few holes in Bayesian data analysis: (1) the usual rules of conditional probability fail in the quantum realm, (2) flat or weak priors lead to terrible inferences about things we care about, (3) subjective priors are incoherent, (4) Bayesian decision picks the wrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6) for Cantorian reasons we need to check our models, but this destroys the coherence of Bayesian inference.},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gelman2021 - Holes in Bayesian statistics.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gelman2021 - Holes in Bayesian Statistics2.pdf;/Users/agarbuno/Zotero/storage/SI5YGRFS/2002.html},
}

@book{Kruschke2014,
  title = {Doing {{Bayesian}} Data Analysis: {{A}} Tutorial with {{R}}, {{JAGS}}, and {{Stan}}},
  author = {Kruschke, John},
  year = {2014},
  publisher = {{Academic Press}},
}

@book{Dogucu2021,
  title = {Bayes {{Rules}}! {{An Introduction}} to {{Applied Bayesian Modeling}}},
  author = {Johnson, Alicia and Ott, Miles and Dogucu, Mine},
  year = {2021},
  abstract = {An introduction to applied Bayesian modeling.},
  file = {/Users/agarbuno/Zotero/storage/XLCW9T4W/index.html},
}

@book{Carlin2009a,
  title = {Bayesian Methods for Data Analysis},
  author = {Carlin, Bradley P. and Louis, Thomas A. and Carlin, Bradley P.},
  year = {2009},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  edition = {3rd ed},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  isbn = {978-1-58488-697-6},
  langid = {english},
  lccn = {QA279.5 .C36 2009},
  keywords = {Bayesian statistical decision theory},
  annotation = {OCLC: ocn227205756},
  file = {/Users/agarbuno/Zotero/storage/6UMZU43A/Carlin et al. - 2009 - Bayesian methods for data analysis.pdf},
}

@book{Reich2015,
  title = {Probabilistic Forecasting and {{Bayesian}} Data Assimilation},
  author = {Reich, Sebastian and Cotter, Colin},
  year = {2015},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-1-107-06939-8 978-1-107-66391-6},
  langid = {english},
  lccn = {QA279.5 .R45 2015},
  keywords = {Bayesian statistical decision theory,Probabilities,Uncertainty (Information theory)},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Reich2015 - Probabilistic forecasting and Bayesian data assimilation.pdf},
}

@article{Sanz-Alonso2019,
  title = {Inverse {{Problems}} and {{Data Assimilation}}},
  author = {{Sanz-Alonso}, Daniel and Stuart, Andrew M. and Taeb, Armeen},
  year = {2019},
  month = {jul},
  journal = {arXiv:1810.06191 [stat]},
  eprint = {1810.06191},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {These notes are designed with the aim of providing a clear and concise introduction to the subjects of Inverse Problems and Data Assimilation, and their inter-relations, together with citations to some relevant literature in this area. The first half of the notes is dedicated to studying the Bayesian framework for inverse problems. Techniques such as importance sampling and Markov Chain Monte Carlo (MCMC) methods are introduced; these methods have the desirable property that in the limit of an infinite number of samples they reproduce the full posterior distribution. Since it is often computationally intensive to implement these methods, especially in high dimensional problems, approximate techniques such as approximating the posterior by a Dirac or a Gaussian distribution are discussed. The second half of the notes cover data assimilation. This refers to a particular class of inverse problems in which the unknown parameter is the initial condition of a dynamical system, and in the stochastic dynamics case the subsequent states of the system, and the data comprises partial and noisy observations of that (possibly stochastic) dynamical system. We will also demonstrate that methods developed in data assimilation may be employed to study generic inverse problems, by introducing an artificial time to generate a sequence of probability measures interpolating from the prior to the posterior.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Sanz-Alonso2019 - Inverse Problems and Data Assimilation.pdf;/Users/agarbuno/Zotero/storage/XCRBSU2E/1810.html},
}
@book{Meyn1993,
  title = {Markov {{Chains}} and {{Stochastic Stability}}},
  author = {Meyn, Sean P. and Tweedie, Richard L.},
  year = {1993},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-3267-7},
  isbn = {978-1-4471-3269-1 978-1-4471-3267-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Meyn1993 - Markov Chains and Stochastic Stability.pdf},
}
@book{Wickham2019,
  title = {Advanced {{R}}, {{Second Edition}}},
  author = {Wickham, Hadley},
  year = {2019},
  month = {may},
  publisher = {{CRC Press}},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimisingyour code.By reading this book, you will learn: The difference between an object and its name, and why the distinction is important The important vector data structures, how they fit together, and how you can pull them apart using subsetting The fine details of functions and environments The condition system, which powers messages, warnings, and errors The powerful functional programming paradigm, which can replace many for loops The three most important OO systems: S3, S4, and R6 The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation Effective debugging techniques that you can deploy, regardless of how your code is run How to find and remove performance bottlenecks The second edition is a comprehensive update: New foundational chapters: "Names and values," "Control flow," and "Conditions" comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them Much deeper coverage of metaprogramming, including the new tidy evaluation framework use of new package like rlang (http://rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (http://purrr.tidyverse.org/) for functional programming Use of color in code chunks and figuresHadley Wickham is Chief Scientist at RStudio, an Adjunct Professor at Stanford University and the University of Auckland, and a member of the R Foundation. He is the lead developer of the tidyverse, a collection of R packages, including ggplot2 and dplyr, designed to support data science. He is also the author of R for Data Science (with Garrett Grolemund), R Packages, and ggplot2: Elegant Graphics for Data Analysis.},
  googlebooks = {JAOaDwAAQBAJ},
  isbn = {978-1-351-20129-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Reference / General},
}
@article{Roberts1998,
  author = {Roberts, Gareth O and Rosenthal, Jeffrey S},
  title = {Optimal Scaling of Discrete Approximations To Langevin Diffusions},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {60},
  number = {1},
  pages = {255--268},
  year = {1998},
  publisher = {Wiley Online Library},
}
@article{Girolami2011,
  title = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods: {{Riemann Manifold Langevin}} and {{Hamiltonian Monte Carlo Methods}}},
  shorttitle = {Riemann Manifold {{Langevin}} and {{Hamiltonian Monte Carlo}} Methods},
  author = {Girolami, Mark and Calderhead, Ben},
  year = {2011},
  month = {mar},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {2},
  pages = {123--214},
  issn = {13697412},
  doi = {10.1111/j.1467-9868.2010.00765.x},
  abstract = {The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis\textendash Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain.The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from www.ucl.ac.uk/statistics/ research/rmhmc allows replication of all the results reported.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Girolami2011 - Riemann manifold Langevin and Hamiltonian Monte Carlo methods.pdf},
}
@article{Byrne2013,
  title = {Geodesic {{Monte Carlo}} on {{Embedded Manifolds}}: {{Geodesic Monte Carlo}}},
  shorttitle = {Geodesic {{Monte Carlo}} on {{Embedded Manifolds}}},
  author = {Byrne, Simon and Girolami, Mark},
  year = {2013},
  month = {dec},
  journal = {Scandinavian Journal of Statistics},
  volume = {40},
  number = {4},
  pages = {825--845},
  issn = {03036898},
  doi = {10.1111/sjos.12036},
  abstract = {Markov chain Monte Carlo methods explicitly defined on the manifold of probability distributions have recently been established. These methods are constructed from diffusions across the manifold and the solution of the equations describing geodesic flows in the Hamilton\textendash Jacobi representation. This paper takes the differential geometric basis of Markov chain Monte Carlo further by considering methods to simulate from probability distributions that themselves are defined on a manifold, with common examples being classes of distributions describing directional statistics. Proposal mechanisms are developed based on the geodesic flows over the manifolds of support for the distributions, and illustrative examples are provided for the hypersphere and Stiefel manifold of orthonormal matrices.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Byrne2013 - Geodesic Monte Carlo on Embedded Manifolds.pdf},
}
@book{Pavliotis2014,
  title = {Stochastic {{Processes}} and {{Applications}}: {{Diffusion Processes}}, the {{Fokker-Planck}} and {{Langevin Equations}}},
  shorttitle = {Stochastic {{Processes}} and {{Applications}}},
  author = {Pavliotis, Grigorios A.},
  year = {2014},
  series = {Texts in {{Applied Mathematics}}},
  volume = {60},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-1323-7},
  isbn = {978-1-4939-1322-0 978-1-4939-1323-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Pavliotis2014 - Stochastic Processes and Applications.pdf},
}
@article{Jorgensen1983a,
  title = {Comparison of Simple Potential Functions for Simulating Liquid Water},
  author = {Jorgensen, William L. and Chandrasekhar, Jayaraman and Madura, Jeffry D. and Impey, Roger W. and Klein, Michael L.},
  year = {1983},
  month = {jul},
  journal = {The Journal of Chemical Physics},
  volume = {79},
  number = {2},
  pages = {926--935},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.445869},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/HZ32TPAQ/Jorgensen et al. - 1983 - Comparison of simple potential functions for simul.pdf},
}
@article{Neal2011,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  author = {Neal, Radford M.},
  year = {2011},
  month = {may},
  journal = {arXiv:1206.1901 [physics, stat]},
  eprint = {1206.1901},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  doi = {10.1201/b10905},
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Neal2011 - MCMC using Hamiltonian dynamics.pdf},
}
@article{Betancourt2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = {jul},
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Betancourt2018 - A Conceptual Introduction to Hamiltonian Monte Carlo.pdf},
}
@article{Duane1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Duane, Simon and Kennedy, Anthony D and Pendleton, Brian J and Roweth, Duncan},
  year = {1987},
  journal = {Physics letters B},
  volume = {195},
  number = {2},
  pages = {216--222},
}
@book{McElreath2020,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical Rethinking},
  author = {McElreath, Richard},
  year = {2020},
  series = {{{CRC}} Texts in Statistical Science},
  edition = {{Second}},
  publisher = {{Taylor and Francis, CRC Press}},
  address = {{Boca Raton}},
  abstract = {"Statistical Rethinking: A Bayesian Course with Examples in R and Stan, Second Edition builds knowledge/confidence in statistical modeling. Pushes readers to perform step-by-step calculations (usually automated.) Unique, computational approach ensures readers understand details to make reasonable choices and interpretations in their modeling work"--},
  isbn = {978-0-367-13991-9},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/McElreath2020 - Statistical rethinking.pdf},
}
@article{Carpenter2017,
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  title = {Stan: a Probabilistic Programming Language},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {nil},
  year = {2017},
  doi = {10.18637/jss.v076.i01},
  url = {https://doi.org/10.18637/jss.v076.i01},
  DATE_ADDED = {Tue Jun 29 15:03:33 2021},
}
@article{Hoffman2011,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2011},
  month = {nov},
  journal = {arXiv:1111.4246 [cs, stat]},
  eprint = {1111.4246},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Hoffman2011 - The No-U-Turn Sampler.pdf;/Users/agarbuno/Zotero/storage/CWMVQ7J4/1111.html},
}
@book{Liu2004,
  title = {Monte {{Carlo Strategies}} in {{Scientific Computing}}},
  author = {Liu, Jun S.},
  year = {2004},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-76371-2},
  isbn = {978-0-387-76369-9 978-0-387-76371-2},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Liu2004 - Monte Carlo Strategies in Scientific Computing.pdf},
}
@book{Brooks2011,
  title = {Handbook of Markov Chain Monte Carlo},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year = {2011},
  publisher = {{CRC press}},
}
@inproceedings{Hornik2003,
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  booktitle = {Proceedings of {{DSC}}},
  author = {Hornik, Kurt and Leisch, Friedrich and Zeileis, Achim},
  year = {2003},
  volume = {2},
  pages = {1--1},
}
@article{Salvatier2016,
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  author = {Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher},
  year = {2016},
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e55},
}
@article{Christen2010,
  title = {A General Purpose Sampling Algorithm for Continuous Distributions (the t-Walk)},
  author = {Christen, J. Andr{\'e}s and Fox, Colin},
  year = {2010},
  month = {jun},
  journal = {Bayesian Analysis},
  volume = {5},
  number = {2},
  pages = {263--281},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/10-BA603},
  abstract = {We develop a new general purpose MCMC sampler for arbitrary continuous distributions that requires no tuning. We call this MCMC the t-walk. The t-walk maintains two independent points in the sample space, and all moves are based on proposals that are then accepted with a standard Metropolis-Hastings acceptance probability on the product space. Hence the t-walk is provably convergent under the usual mild requirements. We restrict proposal distributions, or `moves', to those that produce an algorithm that is invariant to scale, and approximately invariant to affine transformations of the state space. Hence scaling of proposals, and effectively also coordinate transformations, that might be used to increase efficiency of the sampler, are not needed since the t-walk's operation is identical on any scaled version of the target distribution. Four moves are given that result in an effective sampling algorithm. We use the simple device of updating only a random subset of coordinates at each step to allow application of the t-walk to high-dimensional problems. In a series of test problems across dimensions we find that the t-walk is only a small factor less efficient than optimally tuned algorithms, but significantly outperforms general random-walk M-H samplers that are not tuned for specific problems. Further, the t-walk remains effective for target distributions for which no optimal affine transformation exists such as those where correlation structure is very different in differing regions of state space. Several examples are presented showing good mixing and convergence characteristics, varying in dimensions from 1 to 200 and with radically different scale and correlation structure, using exactly the same sampler. The t-walk is available for R, Python, MatLab and C++ at http://www.cimat.mx/\textasciitilde jac/twalk/},
  keywords = {Bayesian inference,Database Expansion Item,MCMC,simulation,t-walk},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Christen2010 - A general purpose sampling algorithm for continuous distributions (the t-walk).pdf;/Users/agarbuno/Zotero/storage/UHLU7UHP/10-BA603.html},
}
@article{Foreman-Mackey2013,
  title = {Emcee: {{The MCMC Hammer}}},
  shorttitle = {Emcee},
  author = {{Foreman-Mackey}, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
  year = {2013},
  month = {mar},
  journal = {Publications of the Astronomical Society of the Pacific},
  volume = {125},
  number = {925},
  eprint = {1202.3665},
  eprinttype = {arxiv},
  pages = {306--312},
  issn = {00046280, 15383873},
  doi = {10.1086/670067},
  abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman \& Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to \$\textbackslash sim N\^2\$ for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Computational Physics,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Foreman-Mackey2013 - emcee.pdf;/Users/agarbuno/Zotero/storage/IG557KL7/1202.html},
}
@article{Garbuno-Inigo2019,
  title = {Interacting {{Langevin Diffusions}}: {{Gradient Structure And Ensemble Kalman Sampler}}},
  shorttitle = {Interacting {{Langevin Diffusions}}},
  author = {{Garbuno-Inigo}, Alfredo and Hoffmann, Franca and Li, Wuchen and Stuart, Andrew M.},
  year = {2019},
  month = {oct},
  journal = {arXiv:1903.08866 [math]},
  eprint = {1903.08866},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Solving inverse problems without the use of derivatives or adjoints of the forward model is highly desirable in many applications arising in science and engineering. In this paper we propose a new version of such a methodology, a framework for its analysis, and numerical evidence of the practicality of the method proposed. Our starting point is an ensemble of over-damped Langevin diffusions which interact through a single preconditioner computed as the empirical ensemble covariance. We demonstrate that the nonlinear Fokker-Planck equation arising from the mean-field limit of the associated stochastic differential equation (SDE) has a novel gradient flow structure, built on the Wasserstein metric and the covariance matrix of the noisy flow. Using this structure, we investigate large time properties of the Fokker-Planck equation, showing that its invariant measure coincides with that of a single Langevin diffusion, and demonstrating exponential convergence to the invariant measure in a number of settings. We introduce a new noisy variant on ensemble Kalman inversion (EKI) algorithms found from the original SDE by replacing exact gradients with ensemble differences; this defines the ensemble Kalman sampler (EKS). Numerical results are presented which demonstrate its efficacy as a derivative-free approximate sampler for the Bayesian posterior arising from inverse problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Garbuno-Inigo2019 - Interacting Langevin Diffusions.pdf},
}
@article{Rubin1981,
  title = {Estimation in {{Parallel Randomized Experiments}}},
  author = {Rubin, Donald B.},
  year = {1981},
  journal = {Journal of Educational Statistics},
  volume = {6},
  number = {4},
  pages = {377--401},
  publisher = {{[Sage Publications, Inc., American Educational Research Association, American Statistical Association]}},
  issn = {0362-9791},
  doi = {10.2307/1164617},
  abstract = {Many studies comparing new treatments to standard treatments consist of parallel randomized experiments. In the example considered here, randomized experiments were conducted in eight schools to determine the effectiveness of special coaching programs for the SAT. The purpose here is to illustrate Bayesian and empirical Bayesian techniques that can be used to help summarize the evidence in such data about differences among treatments, thereby obtaining improved estimates of the treatment effect in each experiment, including the one having the largest observed effect. Three main tools are illustrated: 1) graphical techniques for displaying sensitivity within an empirical Bayes framework, 2) simple simulation techniques for generating Bayesian posterior distributions of individual effects and the largest effect, and 3) methods for monitoring the adequacy of the Bayesian model specification by simulating the posterior predictive distribution in hypothetical replications of the same treatments in the same eight schools.},
}
@book{Gelman2014a,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year = {2014},
  volume = {2},
  publisher = {{CRC press Boca Raton, FL}}
}
@book{Tarantola2005,
  title = {Inverse {{Problem Theory}} and {{Methods}} for {{Model Parameter Estimation}}},
  author = {Tarantola, Albert},
  year = {2005},
  month = {jan},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898717921},
  isbn = {978-0-89871-572-9 978-0-89871-792-1},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Tarantola2005 - Inverse Problem Theory and Methods for Model Parameter Estimation.pdf},
}
@book{Kaipio2005,
  title = {Statistical and Computational Inverse Problems},
  author = {Kaipio, Jari and Somersalo, Erkki},
  year = {2005},
  series = {Applied Mathematical Sciences},
  number = {v. 160},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-22073-4},
  langid = {english},
  lccn = {QA1 QA377 .A647 vol. 160},
  keywords = {Inverse problems (Differential equations),Numerical solutions},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Kaipio2005 - Statistical and computational inverse problems.pdf},
}
@book{Hastie2009c,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Hastie2009 - The Elements of Statistical Learning.pdf},
}
@article{Piironen2017a,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  month = {jan},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  eprint = {1707.01694},
  eprinttype = {arxiv},
  issn = {1935-7524},
  doi = {10.1214/17-EJS1337SI},
  abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Piironen2017 - Sparsity information and regularization in the horseshoe and other shrinkage.pdf},
}
@article{Fourier1878,
  title={The analytical theory of heat},
  author={Baron Fourier, Jean Baptiste Joseph},
  year={1878},
  publisher={The University Press}
}
@article{Vehtari2021a,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R\textasciicircum}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = {jun},
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  issn = {1936-0975},
  doi = {10.1214/20-BA1221},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Vehtari2021 - Rank-Normalization, Folding, and Localization.pdf},
}
@article{Geyer2002,
  title = {Introduction to {{Markov Chain Monte Carlo}}},
  author = {Geyer, Charles J},
  year = {2002},
  journal = {Handbook of Markov Chain Monte Carlo},
  number = {1990},
  pages = {3--48},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory. The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology. The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
}
@book{Cressie2015,
  title = {Statistics for Spatio-Temporal Data},
  author = {Cressie, Noel and Wikle, Christopher K},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
}
@book{Martin2021,
  title = {Bayesian {{Modeling}} and {{Computation}} in {{Python}}},
  author = {Martin, Osvaldo A. and Kumar, Ravin and Lao, Junpeng},
  year = {2021},
  edition = {{First}},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Martin2021 - Bayesian Modeling and Computation in Python.pdf},
}
@article{Gelman2002a,
  title = {A {{Probability Model}} for {{Golf Putting}}},
  author = {Gelman, Andrew and Nolan, Deborah},
  year = {2002},
  journal = {Teaching Statistics},
  volume = {24},
  number = {3},
  pages = {93--95},
  issn = {1467-9639},
  doi = {10.1111/1467-9639.00097},
  abstract = {We derive a model, using trigonometry and the Normal distribution, for the probability that a golf putt is successful. We describe a class activity in which we lead the students through the steps of examining the data, considering possible models, constructing a probability model and checking the fit. The model is,of necessity, oversimplified, a point which the class discusses at the end of the demonstration.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9639.00097},
  file = {/Users/agarbuno/Zotero/storage/APZXX7WS/Gelman and Nolan - 2002 - A Probability Model for Golf Putting.pdf;/Users/agarbuno/Zotero/storage/S8VUMYZY/1467-9639.html},
}
@misc{Gelman2019,
  title = {Model Building and Expansion for Golf Putting},
  author = {Gelman, Andrew},
  year = {2019},
  howpublished = {https://mc-stan.org/users/documentation/case-studies/golf.html},
  file = {/Users/agarbuno/Zotero/storage/PMMLHWB3/golf.html},
}
@article{Penner2002,
  title = {The Physics of Putting},
  author = {Penner, A R},
  year = {2002},
  month = {feb},
  journal = {Canadian Journal of Physics},
  volume = {80},
  number = {2},
  pages = {83--96},
  issn = {0008-4204, 1208-6045},
  doi = {10.1139/p01-137},
  abstract = {The motion of a rolling golf ball on a sloped golf green is modeled. The resulting calculated path of a golf ball is then used, along with a model of the capture of the golf ball by the hole, to determine the resulting launch conditions required for a successful putt. Estimates of the probability of making certain putts are also presented.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Penner2002 - The physics of putting.pdf},
}
@article{Mikkola2021,
  title = {Prior Knowledge Elicitation: {{The}} Past, Present, and Future},
  shorttitle = {Prior Knowledge Elicitation},
  author = {Mikkola, Petrus and Martin, Osvaldo A. and Chandramouli, Suyog and Hartmann, Marcelo and Pla, Oriol Abril and Thomas, Owen and Pesonen, Henri and Corander, Jukka and Vehtari, Aki and Kaski, Samuel and B{\"u}rkner, Paul-Christian and Klami, Arto},
  year = {2021},
  month = {dec},
  journal = {arXiv:2112.01380 [stat]},
  eprint = {2112.01380},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem, in principle. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyze the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Mikkola2021 - Prior knowledge elicitation.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Mikkola2021 - Prior knowledge elicitation2.pdf;/Users/agarbuno/Zotero/storage/9DFNSFKT/2112.html;/Users/agarbuno/Zotero/storage/BVJCYV5Y/2112.html},
}
@book{Lunn2012,
  title = {The {{BUGS Book}}: {{A Practical Introduction}} to {{Bayesian Analysis}}},
  shorttitle = {The {{BUGS Book}}},
  author = {Lunn, David and Jackson, Chris and Best, Nicky and Thomas, Andrew and Spiegelhalter, David},
  year = {2012},
  month = {oct},
  publisher = {{CRC Press}},
  abstract = {Bayesian statistical methods have become widely used for data analysis and modelling in recent years, and the BUGS software has become the most popular software for Bayesian analysis worldwide. Authored by the team that originally developed this software, The BUGS Book provides a practical introduction to this program and its use. The text presents},
  googlebooks = {zG7SBQAAQBAJ},
  isbn = {978-1-4665-8666-6},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General},
}
@book{Box2011,
  title = {Bayesian {{Inference}} in {{Statistical Analysis}}},
  author = {Box, George E. P. and Tiao, George C.},
  year = {2011},
  month = {jan},
  publisher = {{John Wiley \& Sons}},
  abstract = {Its main objective is to examine the application and relevance of Bayes' theorem to problems that arise in scientific investigation in which inferences must be made regarding parameter values about which little is known a priori. Begins with a discussion of some important general aspects of the Bayesian approach such as the choice of prior distribution, particularly noninformative prior distribution, the problem of nuisance parameters and the role of sufficient statistics, followed by many standard problems concerned with the comparison of location and scale parameters. The main thrust is an investigation of questions with appropriate analysis of mathematical results which are illustrated with numerical examples, providing evidence of the value of the Bayesian approach.},
  googlebooks = {T8Askeyk1k4C},
  isbn = {978-1-118-03144-5},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
}
@article{Gabry2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  month = {feb},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {182},
  number = {2},
  pages = {389--402},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/rssa.12378},
  abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gabry2019 - Visualization in Bayesian workflow.pdf},
}
@book{Gelman2006,
  author = {Gelman, Andrew and Hill, Jennifer},
  title = {Data analysis using regression and multilevel/hierarchical models},
  year = {2006},
  publisher = {Cambridge university press},
}
@article{Bayarri2000,
  title = {P {{Values}} for {{Composite Null Models}}},
  author = {Bayarri, M. J. and Berger, James O.},
  year = {2000},
  journal = {Journal of the American Statistical Association},
  volume = {95},
  number = {452},
  pages = {1127--1142},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2669749},
  abstract = {The problem of investigating compatibility of an assumed model with the data is investigated in the situation when the assumed model has unknown parameters. The most frequently used measures of compatibility are p values, based on statistics T for which large values are deemed to indicate incompatibility of the data and the model. When the null model has unknown parameters, p values are not uniquely defined. The proposals for computing a p value in such a situation include the plug-in and similar p values on the frequentist side, and the predictive and posterior predictive p values on the Bayesian side. We propose two alternatives, the conditional predictive p value and the partial posterior predictive p value, and indicate their advantages from both Bayesian and frequentist perspectives.},
}
@article{Newton1994,
  title = {Approximate {{Bayesian Inference}} with the {{Weighted Likelihood Bootstrap}}},
  author = {Newton, Michael A. and Raftery, Adrian E.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {56},
  number = {1},
  pages = {3--48},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling-importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR-adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second-order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models.},
}
@article{Talts2020,
  title = {Validating {{Bayesian Inference Algorithms}} with {{Simulation-Based Calibration}}},
  author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
  year = {2020},
  month = {oct},
  journal = {arXiv:1804.06788 [stat]},
  eprint = {1804.06788},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce simulation-based calibration (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Talts2020 - Validating Bayesian Inference Algorithms with Simulation-Based Calibration.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Talts2020 - Validating Bayesian Inference Algorithms with Simulation-Based Calibration2.pdf},
}
@article{Cook2006,
  title = {Validation of {{Software}} for {{Bayesian Models Using Posterior Quantiles}}},
  author = {Cook, Samantha R and Gelman, Andrew and Rubin, Donald B},
  year = {2006},
  month = {sep},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {3},
  pages = {675--692},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/106186006X136976},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Cook2006 - Validation of Software for Bayesian Models Using Posterior Quantiles.pdf},
}
@article{Halmos1970,
  title = {How to Write Mathematics},
  author = {Halmos, PR},
  year = {1970},
  journal = {L'Enseignement Math\'ematique},
  volume = {16},
}
@article{Gneiting2007a,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = {mar},
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/SAMXR36H/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf},
}
@book{MacKay2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  year = {2003},
  publisher = {{Cambridge University Press}},
  file = {/Users/agarbuno/Zotero/storage/Y563KRWF/MacKay - Information Theory, Inference, and Learning Algori.pdf},
}
@article{Gelman2014c,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  year = {2014},
  month = {nov},
  journal = {Statistics and Computing},
  volume = {24},
  number = {6},
  pages = {997--1016},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gelman2014 - Understanding predictive information criteria for Bayesian models.pdf},
}
@inproceedings{Akaike1973,
  title = {Information Theory and an Extension of the Maximum Likelihood Principle},
  booktitle = {Second {{International Symposium}} on {{Information Theory}}},
  author = {Akaike, H},
  editor = {Petrov, B N and Csaki, F},
  year = {1973},
  pages = {267--281},
  publisher = {{Akademiai Kiado}},
  address = {{Budapest, Hungary}},
}
@article{Wilks1938,
  title = {The {{Large-Sample Distribution}} of the {{Likelihood Ratio}} for {{Testing Composite Hypotheses}}},
  author = {Wilks, S. S.},
  year = {1938},
  month = {mar},
  journal = {The Annals of Mathematical Statistics},
  volume = {9},
  number = {1},
  pages = {60--62},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732360},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Wilks1938 - The Large-Sample Distribution of the Likelihood Ratio for Testing Composite.pdf;/Users/agarbuno/Zotero/storage/84NI423I/1177732360.html},
}
@article{Vehtari2021,
  title = {Pareto {{Smoothed Importance Sampling}}},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  year = {2021},
  month = {feb},
  journal = {arXiv:1507.02646 [stat]},
  eprint = {1507.02646},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Vehtari2021 - Pareto Smoothed Importance Sampling.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Vehtari2021 - Pareto Smoothed Importance Sampling2.pdf},
}
@article{Vehtari2012a,
  title = {A Survey of {{Bayesian}} Predictive Methods for Model Assessment, Selection and Comparison},
  author = {Vehtari, Aki and Ojanen, Janne},
  year = {2012},
  month = {jan},
  journal = {Statistics Surveys},
  volume = {6},
  number = {none},
  pages = {142--228},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/12-SS102},
  abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
  keywords = {62-02,62C10,Bayesian,cross-validation,decision theory,Expected utility,information criteria,model assessment,Model selection,predictive},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Vehtari2012 - A survey of Bayesian predictive methods for model assessment, selection and3.pdf;/Users/agarbuno/Zotero/storage/5946UBXE/12-SS102.html},
}
@article{Gilboaa,
  title = {Theory of {{Decision}} under {{Uncertainty}}},
  author = {Gilboa, Itzhak},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gilboa - Theory of Decision under Uncertainty.pdf},
}
@book{Smith2010,
  title = {Bayesian {{Decision Analysis}}: {{Principles}} and {{Practice}}},
  shorttitle = {Bayesian {{Decision Analysis}}},
  author = {Smith, Jim Q.},
  year = {2010},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511779237},
  abstract = {Bayesian decision analysis supports principled decision making in complex domains. This textbook takes the reader from a formal analysis of simple decision problems to a careful analysis of the sometimes very complex and data rich structures confronted by practitioners. The book contains basic material on subjective probability theory and multi-attribute utility theory, event and decision trees, Bayesian networks, influence diagrams and causal Bayesian networks. The author demonstrates when and how the theory can be successfully applied to a given decision problem, how data can be sampled and expert judgements elicited to support this analysis, and when and how an effective Bayesian decision analysis can be implemented. Evolving from a third-year undergraduate course taught by the author over many years, all of the material in this book will be accessible to a student who has completed introductory courses in probability and mathematical statistics.},
  isbn = {978-0-521-76454-4},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Smith2010 - Bayesian Decision Analysis.pdf;/Users/agarbuno/Zotero/storage/3S5XHSIE/89BF5447AF494046CB6CD6E7247DF152.html},
}
@article{Thompson1933,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  author = {Thompson, William R.},
  year = {1933},
  journal = {Biometrika},
  volume = {25},
  number = {3/4},
  pages = {285--294},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332286},
}
@inproceedings{Li2010b,
  title = {A Contextual-Bandit Approach to Personalized News Article Recommendation},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web - {{WWW}} '10},
  author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
  year = {2010},
  pages = {661},
  publisher = {{ACM Press}},
  address = {{Raleigh, North Carolina, USA}},
  doi = {10.1145/1772690.1772758},
  abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.},
  isbn = {978-1-60558-799-8},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/EGV8X74C/Li et al. - 2010 - A contextual-bandit approach to personalized news .pdf},
}
@article{Law2015,
  title = {Data {{Assimilation}}: {{A Mathematical Introduction}}},
  shorttitle = {Data {{Assimilation}}},
  author = {Law, K. J. H. and Stuart, A. M. and Zygalakis, K. C.},
  year = {2015},
  month = {jun},
  journal = {arXiv:1506.07825 [math, stat]},
  eprint = {1506.07825},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {These notes provide a systematic mathematical treatment of the subject of data assimilation.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Law2015 - Data Assimilation.pdf},
}
@article{DelMoral2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = {jun},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2006.00553.x},
  abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Del Moral2006 - Sequential Monte Carlo samplers.pdf},
}
@article{Scott2010,
  title = {A Modern {{Bayesian}} Look at the Multi-Armed Bandit},
  author = {Scott, Steven L.},
  year = {2010},
  month = {nov},
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {26},
  number = {6},
  pages = {639--658},
  issn = {15241904},
  doi = {10.1002/asmb.874},
  abstract = {A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are `optimal' in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright q 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Scott2010 - A modern Bayesian look at the multi-armed bandit.pdf},
}
@article{Gronau2020,
  title = {Informed {{Bayesian Inference}} for the {{A}}/{{B Test}}},
  author = {Gronau, Quentin F. and Raj, K. N. Akash and Wagenmakers, Eric-Jan},
  year = {2020},
  month = {nov},
  journal = {arXiv:1905.02068 [stat]},
  eprint = {1905.02068},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Booming in business and a staple analysis in medical trials, the A/B test assesses the effect of an intervention or treatment by comparing its success rate with that of a control condition. Across many practical applications, it is desirable that (1) evidence can be obtained in favor of the null hypothesis that the treatment is ineffective; (2) evidence can be monitored as the data accumulate; (3) expert prior knowledge can be taken into account. Most existing approaches do not fulfill these desiderata. Here we describe a Bayesian A/B procedure based on Kass and Vaidyanathan (1992) that allows one to monitor the evidence for the hypotheses that the treatment has either a positive effect, a negative effect, or, crucially, no effect. Furthermore, this approach enables one to incorporate expert knowledge about the relative prior plausibility of the rival hypotheses and about the expected size of the effect, given that it is non-zero. To facilitate the wider adoption of this Bayesian procedure we developed the abtest package in R. We illustrate the package options and the associated statistical results with a fictitious business example and a real data medical example.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Gronau2020 - Informed Bayesian Inference for the A-B Test.pdf;/Users/agarbuno/Zotero/storage/CMGL8Y46/1905.html},
}
@book{Lattimore2020,
  title = {Bandit {{Algorithms}}},
  author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year = {2020},
  month = {jul},
  publisher = {{Cambridge University Press}},
  abstract = {Decision-making in the face of uncertainty is a significant challenge in machine learning, and the multi-armed bandit model is a commonly used framework to address it. This comprehensive and rigorous introduction to the multi-armed bandit problem examines all the major settings, including stochastic, adversarial, and Bayesian frameworks. A focus on both mathematical intuition and carefully worked proofs makes this an excellent reference for established researchers and a helpful resource for graduate students in computer science, engineering, statistics, applied mathematics and economics. Linear bandits receive special attention as one of the most useful models in applications, while other chapters are dedicated to combinatorial bandits, ranking, non-stationary problems, Thompson sampling and pure exploration. The book ends with a peek into the world beyond bandits with an introduction to partial monitoring and learning in Markov decision processes.},
  googlebooks = {bbjpDwAAQBAJ},
  isbn = {978-1-108-48682-8},
  langid = {english},
  keywords = {Business \& Economics / Economics / Microeconomics,Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Programming / Algorithms,Mathematics / Discrete Mathematics,Mathematics / Game Theory,Mathematics / Optimization},
}
@book{Murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01802-9},
  langid = {english},
  lccn = {Q325.5 .M87 2012},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Murphy2012 - Machine learning.pdf},
}
@book{Bishop2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Bishop2006 - Pattern recognition and machine learning.pdf},
}
@inproceedings{Kucukelbir2015,
  title = {Automatic {{Variational Inference}} in {{Stan}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI); we implement it in Stan (code available), a probabilistic programming system. In ADVI the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Conference Paper/Kucukelbir2015 - Automatic Variational Inference in Stan.pdf},
}
<<<<<<< HEAD
@book{Koller2009,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  shorttitle = {Probabilistic Graphical Models},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01319-2},
  langid = {english},
  lccn = {QA279.5 .K65 2009},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Koller2009 - Probabilistic graphical models.pdf},
=======
@article{Blei2012,
  title = {Probabilistic Topic Models},
  author = {Blei, David M.},
  year = {2012},
  month = {apr},
  journal = {Communications of the ACM},
  volume = {55},
  number = {4},
  pages = {77--84},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2133806.2133826},
  abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/BYUAKF96/Blei - 2012 - Probabilistic topic models.pdf},
}
@article{Blei2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Jan},
  pages = {993--1022},
  issn = {ISSN 1533-7928},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Blei2003 - Latent Dirichlet Allocation.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Blei2003 - Latent Dirichlet Allocation2.pdf},
}
@article{Blei2007,
  title = {A Correlated Topic Model of {{Science}}},
  author = {Blei, David M. and Lafferty, John D.},
  year = {2007},
  month = {jun},
  journal = {The Annals of Applied Statistics},
  volume = {1},
  number = {1},
  pages = {17--35},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/07-AOAS114},
  abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139\textendash 177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990\textendash 1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.},
  keywords = {approximate posterior inference,hierarchical models,text analysis,variational methods},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Blei2007 - A correlated topic model of Science.pdf;/Users/agarbuno/Zotero/storage/R7FB3BPU/07-AOAS114.html},
}
@inproceedings{Hoffman2010,
  title = {Online {{Learning}} for {{Latent Dirichlet Allocation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hoffman, Matthew and Bach, Francis and Blei, David},
  year = {2010},
  volume = {23},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Conference Paper/Hoffman2010 - Online Learning for Latent Dirichlet Allocation.pdf},
}
@inproceedings{Zhai2012,
  title = {Mr. {{LDA}}: A Flexible Large Scale Topic Modeling Package Using Variational Inference in {{MapReduce}}},
  shorttitle = {Mr. {{LDA}}},
  booktitle = {Proceedings of the 21st International Conference on {{World Wide Web}}},
  author = {Zhai, Ke and {Boyd-Graber}, Jordan and Asadi, Nima and Alkhouja, Mohamad L.},
  year = {2012},
  month = {apr},
  pages = {879--888},
  publisher = {{ACM}},
  address = {{Lyon France}},
  doi = {10.1145/2187836.2187955},
  abstract = {Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring document collections. Because of the increasing prevalence of large datasets, there is a need to improve the scalability of inference for LDA. In this paper, we introduce a novel and flexible large scale topic modeling package in MapReduce (Mr. LDA). As opposed to other techniques which use Gibbs sampling, our proposed framework uses variational inference, which easily fits into a distributed environment. More importantly, this variational implementation, unlike highly tuned and specialized implementations based on Gibbs sampling, is easily extensible. We demonstrate two extensions of the models possible with this scalable framework: informed priors to guide topic discovery and extracting topics from a multilingual corpus. We compare the scalability of Mr. LDA against Mahout, an existing large scale topic modeling package. Mr. LDA out-performs Mahout both in execution speed and held-out likelihood.},
  isbn = {978-1-4503-1229-5},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/D9EJP7Z8/Zhai et al. - 2012 - Mr. LDA a flexible large scale topic modeling pac.pdf},
}
@inproceedings{Hu2014,
  title = {Polylingual {{Tree-Based Topic Models}} for {{Translation Domain Adaptation}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hu, Yuening and Zhai, Ke and Eidelman, Vladimir and {Boyd-Graber}, Jordan},
  year = {2014},
  month = {jun},
  pages = {1166--1176},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-1110},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Conference Paper/Hu2014 - Polylingual Tree-Based Topic Models for Translation Domain Adaptation.pdf},
}
@book{Jaynes2003,
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  author = {Jaynes, E.T. and Bretthorst, G.L.},
  year = {2003},
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-59271-0},
  lccn = {2002071486},
  file = {/Users/agarbuno/Library/CloudStorage/GoogleDrive-alfredogarbuno@gmail.com/My Drive/bibliography/Book/Jaynes2003 - Probability Theory.pdf},
}
