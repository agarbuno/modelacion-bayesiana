#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Integración Monte Carlo~
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+STARTUP: showall
#+PROPERTY: header-args:R :session intro :exports both :results output org :tangle ../rscripts/01-montecarlo.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022.\\
*Objetivo*. Estudiar integración numérica en el contexto probabilistico. Estudiar,
 en particular, el método Monte Carlo y entender sus bondades y limitaciones en
 el contexto de inferencia Bayesiana. \\
*Lectura recomendada*: Sección 6.1 de citet:Dogucu2021. Una lectura mas técnica
 sobre reglas de cuadratura se puede encontrar en la sección 3.1 de
 citet:Reich2015. Y una buena referencia (técnica) sobre el método Monte Carlo
 lo encuentran en citet:Sanz-Alonso2019.
#+END_NOTES


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#integración-numérica][Integración numérica]]
  - [[#ejemplo-proporción][Ejemplo: Proporción]]
  - [[#más-de-un-parámetro][Más de un parámetro]]
  - [[#reglas-de-cuadratura][Reglas de cuadratura]]
- [[#integración-monte-carlo][Integración Monte Carlo]]
  - [[#ejemplo-dardos][Ejemplo: Dardos]]
  - [[#propiedades][Propiedades]]
  - [[#ejemplo-proporciones][Ejemplo: Proporciones]]
  - [[#ejemplo-sabores-de-helados][Ejemplo: Sabores de helados]]
  - [[#tarea-sabores-de-helados][Tarea: Sabores de helados]]
- [[#extensiones-muestreo-por-importancia][Extensiones: Muestreo por importancia]]
  - [[#propiedades-muestreo-por-importancia][Propiedades: muestreo por importancia]]
- [[#referencias][Referencias]]
:END:



* Introducción

En inferencia bayesiana lo que queremos es poder resolver

\begin{align}
\mathbb{E}[f] = \int_{\Theta}^{} f(\theta) \, \pi(\theta | y ) \,  \text{d}\theta\,. 
\end{align}

#+BEGIN_NOTES

Lo que necesitamos es resolver integrales con respecto a la distribución de interés.

#+END_NOTES

#+REVEAL: split
#+ATTR_REVEAL: :frag (appear)
- La pregunta clave (I) es: ¿qué distribución?
- La pregunta clave (II) es: ¿con qué método numérico resuelvo la integral?
- La pregunta clave (III) es: ¿y si no hay método numérico? 

* Integración numérica

Recordemos la definición de integrales Riemann:

$$\int f(x) \text{d} x\,.$$

#+BEGIN_NOTES
La aproximación utilizando una malla de $N$ puntos sería: 
$$\sum_{n=1}^N f(u_n) \Delta u_n.$$

El método útil cuando las integrales se realizan cuando tenemos pocos parámetros. Es decir, $\theta \in \mathbb{R}^p$ con $p$ pequeña. 
#+END_NOTES


#+begin_src R :exports none :results none
  ## Setup --------------------------------------------------
#+end_src

#+begin_src R :exports none

  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), 
        axis.text = element_blank())

  ## Ejemplo de integracion numerica -----------------------

  grid.n          <- 11                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )


  norm.density <- tibble(x = seq(-5, 5, by = .01), 
         y = dnorm(x) ) 

#+end_src

#+RESULTS:
#+begin_src org
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature.jpeg :exports results :results output graphics file
  norm.cuadrature |>
    ggplot(aes(x=x + grid.size/2, y=y)) + 
    geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
    geom_bar(stat="identity", alpha = .3) + 
    geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
    sin_lineas + xlab('x') + ylab("density") + 
    annotate('text', label = expression(Delta~u[n]),
             x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
    annotate('text', label = expression(f(u[n]) ),
             x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
    annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
             x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
             angle = -90, alpha = .7, size = 12)
#+end_src
#+caption: Integral por medio de discretización.
#+RESULTS:
[[file:../images/quadrature.jpeg]]

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature-hi.jpeg :exports results :results output graphics file
  grid.n          <- 101                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )

  norm.cuadrature |>
      ggplot(aes(x=x + grid.size/2, y=y)) + 
      geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
      geom_bar(stat="identity", alpha = .3) + 
      geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
      sin_lineas + xlab('x') + ylab("density") + 
      annotate('text', label = expression(Delta~u[n]),
               x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
      annotate('text', label = expression(f(u[n]) ),
               x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
      annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
               x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
               angle = -90, alpha = .7, size = 12)
#+end_src
#+caption: Integral por medio de una malla fina. 
#+RESULTS:
[[file:../images/quadrature-hi.jpeg]]

** Ejemplo: Proporción

Supongamos que $p(S_n = k|\theta) \propto \theta^k(1-\theta)^{n-k}$ cuando
observamos $k$ éxitos en $n$ pruebas independientes. Supongamos que nuestra
inicial es $p(\theta) = 2\theta$ (~checa que es una densidad~).

#+REVEAL: split
#+begin_src R :exports code :results none
  crear_log_post <- function(n, k){
    function(theta){
      verosim <- k * log(theta) + (n - k) * log(1 - theta)
      inicial <- log(theta)
      verosim + inicial
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R
  # observamos 3 éxitos en 4 pruebas:
  log_post <- crear_log_post(4, 3)
  prob_post <- function(x) { exp(log_post(x))}
  # integramos numéricamente
  p_x <- integrate(prob_post, lower = 0, upper = 1, subdivisions = 100L)
  p_x
#+end_src

#+RESULTS:
#+begin_src org
0.033 with absolute error < 3.7e-16
#+end_src

#+REVEAL: split
Y ahora podemos calcular la media posterior:
\begin{align}
\mathbb{E}[\theta | S_n] = \int \theta \, \pi(\theta | S_n)\, \text{d}\theta\,.
\end{align}

#+begin_src R
      media_funcion <- function(theta){
        theta * prob_post(theta) / p_x$value
      }
      integral_media <- integrate(media_funcion,
                                  lower = 0, upper = 1,
                                  subdivisions = 100L)
      media_post <- integral_media$value 
      c(Numerico = media_post, Analitico = 5/(2+5))
#+end_src

#+RESULTS:
#+begin_src org
 Numerico Analitico 
     0.71      0.71
#+end_src

** Más de un parámetro

#+BEGIN_NOTES
Consideramos ahora un espacio con $\theta \in \mathbb{R}^p$. Si conservamos $N$
puntos por cada dimensión, ¿cuántos puntos en la malla necesitaríamos?  Lo que
tenemos son recursos computacionales limitados y hay que buscar hacer el mejor
uso de ellos. En el ejemplo, hay zonas donde no habrá contribución en la
integral.
#+END_NOTES


#+HEADER: :width 1500 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/eruption-quadrature.jpeg :exports results :results output graphics file
      canvas <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
       xlim(0.5, 6) +
       ylim(40, 110)

      grid.size <- 10 - 1

      mesh <- expand.grid(x = seq(0.5, 6, by = (6-.5)/grid.size),
                          y = seq(40, 110, by = (110-40)/grid.size))

    g1 <- canvas +
        geom_density_2d_filled(aes(alpha = ..level..), bins = 8) +
        scale_fill_manual(values = rev(color.itam)) + 
        sin_lineas + theme(legend.position = "none") +
        geom_point(data = mesh, aes(x = x, y = y)) + 
        annotate("rect", xmin = .5 + 5 * (6-.5)/grid.size, 
                  xmax = .5 + 6 * (6-.5)/grid.size, 
                  ymin = 40 + 3 * (110-40)/grid.size, 
                  ymax = 40 + 4 * (110-40)/grid.size,
                  linestyle = 'dashed', 
                 fill = 'salmon', alpha = .4) + ylab("") + xlab("") + 
        annotate('text', x = .5 + 5.5 * (6-.5)/grid.size, 
                         y = 40 + 3.5 * (110-40)/grid.size, 
                 label = expression(u[n]), color = 'red', size = 15) +
          theme(axis.ticks = element_blank(), 
              axis.text = element_blank())


    g2 <- canvas + 
        stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/grid.size, (110-40)/grid.size)) +
        sin_lineas + theme(legend.position = "none") +
        theme(axis.ticks = element_blank(), 
                axis.text = element_blank()) +
        scale_fill_distiller(palette = "Greens", direction = 1) + 
        sin_lineas + theme(legend.position = "none") +
        ylab("") + xlab("")

    g3 <- canvas + 
        stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/25, (110-40)/25)) +
        sin_lineas + theme(legend.position = "none") +
        theme(axis.ticks = element_blank(), 
                axis.text = element_blank()) +
        scale_fill_distiller(palette = "Greens", direction = 1) + 
        sin_lineas + theme(legend.position = "none") +
        ylab("") + xlab("")

  g1 + g2 + g3
#+end_src
#+caption: Integral por método de malla. 
#+RESULTS:
[[file:../images/eruption-quadrature.jpeg]]

** Reglas de cuadratura

Por el momento hemos escogido aproximar las integrales por medio de una aproximación con una ~malla uniforme~.
Sin embargo, se pueden utilizar aproximaciones 

$$\int f(x) \text{d} x \approx \sum_{n=1}^N f(\xi_n)\, \omega_n\,.$$

Estas aproximaciones usualmente se realizan para integrales en intervalos cerrados $[a,b]$. La regla de cuadratura determina los pesos $\omega_n$ y los centros $\xi_n$ pues se escogen de acuerdo a ~ciertos criterios de convergencia~.

#+BEGIN_NOTES
Por ejemplo, se consideran polinomios que aproximen con cierto grado de precisión el integrando. Los pesos y los centros se escogen de acuerdo a la familia de polinomios. Pues para cada familia se tienen identificadas las mallas que optimizan la aproximación. Ver sección 3.1 de citet:Reich2015. 
#+END_NOTES

* Integración Monte Carlo

\begin{gather*}
\pi(f) = \mathbb{E}_\pi[f] = \int f(x) \pi(x) \text{d}x\,,\\
\pi_N^{\textsf{MC}}(f) = \frac1N \sum_{n = 1}^N f( x^{(n)}), \qquad \text{ donde }  x^{(n)} \overset{\mathsf{iid}}{\sim} \pi, \qquad \text{ con } n = 1, \ldots, N \,, \\
 \pi(f) \approx \pi_N^{\textsf{MC}}(f)\,.
\end{gather*} 


** Ejemplo: Dardos

Consideremos el experimento de lanzar dardos uniformemente en un cuadrado de
tamaño 2, el cual contiene un circulo de radio 1.

#+HEADER: :width 1100 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-montecarlo.jpeg :exports results :results output graphics file
  ## Integración Monte Carlo ----------------------------------- 
  genera_dardos <- function(n = 100){
      tibble(x1 = runif(n, min = -1, max = 1), 
             x2 = runif(n, min = -1, max = 1)) %>% 
        mutate(resultado = ifelse(x1**2 + x2**2 <= 1., 1., 0.))
    }

    dardos <- tibble(n = seq(2,5)) %>% 
      mutate(datos = map(10**n, genera_dardos)) %>% 
      unnest() 

    dardos %>% 
      ggplot(aes(x = x1, y = x2)) + 
        geom_point(aes(color = factor(resultado))) + 
        facet_wrap(~n, nrow = 1) +  
      sin_lineas + sin_ejes + sin_leyenda + coord_equal()
#+end_src
#+caption: Integración Monte Carlo para aproximar $\pi$. 
#+RESULTS:
[[file:../images/dardos-montecarlo.jpeg]]

#+begin_src R :exports none :results none
  dardos |>
    group_by(n) |>
    summarise(aprox = 4 * mean(resultado)) |>
    as.data.frame()
#+end_src

#+RESULTS:
#+begin_src org
  n aprox
1 2   3.1
2 3   3.2
3 4   3.1
4 5   3.1
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-consistencia.jpeg :exports results :results output graphics file

  set.seed(1087)

  genera_dardos(n = 2**16) %>% 
    mutate(n = seq(1, 2**16), 
           approx = cummean(resultado) * 4) %>% 
    ggplot(aes(x = n, y = approx)) + 
      geom_line() + 
      geom_hline(yintercept = pi, linetype = 'dashed') + 
      scale_x_continuous(trans='log10', 
                         labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Muestras") + sin_lineas

#+end_src
#+caption: Estimación $\pi_N^{\textsf{MC}}(f)$ con $N \rightarrow \infty$. 
#+RESULTS:
[[file:../images/dardos-consistencia.jpeg]]

** Propiedades

*Teorema (~Error Monte Carlo~).* Sea $f : \mathbb{R}^p \rightarrow \mathbb{R}$
cualquier función bien comportada$^\dagger$.  Entonces, el estimador Monte Carlo es
*insesgado*. Es decir, se satisface 

\begin{align}
\mathbb{E}\left[ \pi_N^{\textsf{MC}}(f) - \pi(f)\right] = 0,
\end{align}
para cualquier $N$. Usualmente estudiamos el error en un escenario pesimista
donde medimos el *error cuadrático medio* en el peor escenario

\begin{align*}
\sup_{f \in \mathcal{F}} \, \,  \mathbb{E}\left[ \left(\pi_N^{\textsf{MC}}(f) - \pi(f) \right)^2 \right] \leq \frac1N.
\end{align*}

#+REVEAL: split
En particular, la varianza del estimador (*error estándar*) satisface la igualdad

$$ \textsf{ee}^2\left(\pi_N^{\textsf{MC}}(f)\right) = \frac{\mathbb{V}_\pi( f )}{N}.$$

#+REVEAL: split
*Teorema (~TLC para estimadores Monte Carlo~).* Sea $f$ una función *bien comportada*
$^{\dagger\dagger}$, entonces bajo una $N$ suficientemente grande tenemos
\begin{align}
\sqrt{N} \left(\pi_N^{\textsf{MC}} (f) - \pi(f) \right) \sim \mathsf{N}\left(0, \mathbb{V}_\pi(f)\right)\,.
\end{align}

** Ejemplo: Proporciones

Consideramos la estimación de una proporción $\theta$, tenemos como inicial
$p(\theta) \propto \theta$, que es una $\mathsf{Beta}(2,1)$. Si observamos 3
éxitos en 4 pruebas, entonces sabemos que la posterior es $p(\theta|x)\propto
\theta^4(1-\theta)$, que es una $\mathsf{Beta}(5, 2)$. Si queremos calcular la
media y el segundo momento posterior para $\theta$, en teoría necesitamos
calcular

\begin{align}
\mu_1 = \int_0^1 \theta \,\, p(\theta|X = 3)\, \text{d}\theta,\qquad  \mu_2=\int_0^1 \theta^2 \,\, p(\theta|X = 3)\, \text{d}\theta.
\end{align}

#+REVEAL: split
#+begin_src R :exports none :results none
  ### Ejemplo proporciones ------------------ 
#+end_src

Utilizando el ~método Monte Carlo~: 
#+begin_src R
theta <- rbeta(10000, 5, 2)
media_post <- mean(theta)
momento_2_post <- mean(theta^2)
c(mu_1 = media_post, mu_2 = momento_2_post)
#+end_src

#+RESULTS:
#+begin_src org
mu_1 mu_2 
0.71 0.54
#+end_src

#+REVEAL: split
Incluso, podemos calcular cosas mas /exóticas/ como
\begin{align}
P(e^{\theta}> 2|x)\,.
\end{align}

#+begin_src R
mean(exp(theta) > 2)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.61
#+end_src

** Ejemplo: Sabores de helados

Supongamos que probamos el nivel de gusto para 4 sabores distintos de una
paleta. Usamos 4 muestras de aproximadamente 50 personas diferentes para cada
sabor, y cada uno evalúa si le gustó mucho o no. Obtenemos los siguientes
resultados:
#+begin_src R :exports none :results none
  ### Ejemplo helados ------------------------- 
#+end_src

#+begin_src R :exports results
  datos <- tibble(
    sabor = c("fresa", "limon", "mango", "guanabana"),
    n = c(50, 45, 51, 50), gusto = c(36, 35, 42, 29)) %>% 
    mutate(prop_gust = gusto / n)

  datos |>
  as.data.frame()
#+end_src

#+caption: Resultados de las encuestas.
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust
1     fresa 50    36      0.72
2     limón 45    35      0.78
3     mango 51    42      0.82
4 guanábana 50    29      0.58
#+end_src

#+REVEAL: split
Usaremos como inicial $\mathsf{Beta}(2, 1)$ (pues hemos obervado cierto sesgo de
cortesía en la calificación de sabores, y no es tan probable tener valores muy
bajos) para todos los sabores, es decir $p(\theta_i)$ es la funcion de densidad
de una $\mathsf{Beta}(2, 1)$. La inicial conjunta la definimos entonces, usando
~independencia inicial~, como

$$p(\theta_1,\theta_2, \theta_3,\theta_4) = p(\theta_1)p(\theta_2)p(\theta_3)p(\theta_4)\,.$$

#+REVEAL: split
Pues inicialmente establecemos que ningún parámetro da información sobre otro:
saber que mango es muy gustado no nos dice nada acerca del gusto por fresa. Bajo
este supuesto, y el supuesto adicional de que las muestras de cada sabor son
independientes, podemos mostrar que las ~posteriores son independientes~:

$$p(\theta_1,\theta_2,\theta_3, \theta_4|k_1,k_2,k_3,k_4) = p(\theta_4|k_1)p(\theta_4|k_2)p(\theta_4|k_3)p(\theta_4|k_4)$$

#+REVEAL: split
#+begin_src R :exports results
  datos <- datos |>
    mutate(a_post = gusto + 2,
           b_post = n - gusto + 1,
           media_post = a_post/(a_post + b_post))
  datos |>
    as.data.frame()
#+end_src

#+caption: Resultado de inferencia Bayesiana. 
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust a_post b_post media_post
1     fresa 50    36      0.72     38     15       0.72
2     limón 45    35      0.78     37     11       0.77
3     mango 51    42      0.82     44     10       0.81
4 guanábana 50    29      0.58     31     22       0.58
#+end_src

#+REVEAL: split
Podemos hacer preguntas interesantes como: ¿cuál es la probabilidad de que mango
sea el sabor preferido?  Para contestar esta pregunta podemos utilizar
simulación y responder por medio de un procedimiento Monte Carlo.

#+begin_src R :exports none :results none
  modelo_beta <- function(params, n = 5000){
    rbeta(n, params$alpha, params$beta)
  }
#+end_src

#+begin_src R :exports code :results none
  ## Generamos muestras de la posterior
  paletas <- datos |>
    mutate(alpha = a_post, beta = b_post) |>
    nest(params.posterior = c(alpha, beta)) |>
    mutate(muestras.posterior = map(params.posterior, modelo_beta)) |>
    select(sabor, muestras.posterior)
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/paletas-hist.jpeg :exports results :results output graphics file
  paletas |>
    unnest(muestras.posterior) |>
    ggplot(aes(muestras.posterior)) +
    geom_histogram(aes(fill = sabor), position = "identity" ) +
    sin_lineas
#+end_src
#+caption: Histogramas de la distribución predictiva marginal para cada $\theta_j$. 
#+RESULTS:
[[file:../images/paletas-hist.jpeg]]

#+REVEAL: split
#+begin_src R
  ## Utilizamos el metodo Monte Carlo para aproximar la integral. 
  paletas |>
    unnest(muestras.posterior) |>
    mutate(id = rep(seq(1, 5000), 4)) |> group_by(id) |>
    summarise(favorito = sabor[which.max(muestras.posterior)]) |>
    group_by(favorito) |> tally() |>
    mutate(prop = n/sum(n)) |>
    as.data.frame()
#+end_src
#+caption: Aproximación Monte Carlo.
#+RESULTS:
#+begin_src org
   favorito    n   prop
1     fresa  308 0.0616
2 guanábana    1 0.0002
3     limón 1319 0.2638
4     mango 3372 0.6744
#+end_src

#+BEGIN_NOTES
Escencialmente estamos preguntándonos sobre calcular la integral:
\begin{align}
\mathbb{P}(\text{mango sea preferido}) = \int_\Theta f(\theta_1, \ldots, \theta_4) \, p(\theta_1, \ldots, \theta_4 | X_1, \ldots, X_n) d\theta\,,
\end{align}
donde $f(\theta_1, \ldots, \theta_4) = \mathbb{I}_{[\theta_4 \geq \theta_j, j \neq 4]}(\theta_1, \ldots, \theta_4)$. 
#+END_NOTES

** Tarea: Sabores de helados

- ¿Cuál es la probabilidad a priori de que cada sabor sea el preferido?
- Con los datos de arriba, calcula la probabilidad de que la gente prefiera el sabor de mango sobre limón.



* Extensiones: Muestreo por importancia

Incluso cuando tenemos una integral *complicada* podemos ~relajar~ el problema de integración. De tal forma que podemos ~sustituir~
$$\int f(x) \pi(x) \text{d} x = \int f(x) \frac{\pi(x)}{\rho(x)}\,\rho(x) \text{d} x = \int f(x) \, w(x) \, \rho(x) \, \text{d}x\,,$$
donde $\rho$ es una densidad de una variable aleatoria ~adecuada~.

#+REVEAL: split
Esto nos permite utilizar lo que sabemos de las propiedades del método Monte Carlo para resolver la integral
\begin{align*}
\pi(f) =  \int f(x) \pi(x) \text{d} x = \int f(x) w(x) \, \rho(x) \, \text{d}x =: \rho(fw)\,,
\end{align*}
por medio de una aproximación
\begin{align}
\pi(f) \approx \sum_{n = 1}^{N} \bar w^{(n)} f(x^{(n)}), \qquad x^{(n)} \overset{\mathsf{iid}}{\sim} \rho\,.
\end{align}
#+REVEAL: split
Al estimador le llamamos el estimador por importancia y lo denotamos por
\begin{align}
\pi_N^{\mathsf{IS}}(f) = \sum_{n = 1}^{N} \bar w^{(n)} f(x^{(n)}), \qquad \bar w^{(n)} = \frac{w(x^{(n)})}{\sum_{m= 1}^{N}w(x^{(m)})}\,.
\end{align}

** Propiedades: muestreo por importancia

Lamentablemente, utilizar muestreo por importancia ~impacta la calidad de la
estimación~ (medida, por ejemplo, en términos del *peor error cuadrático medio
cometido*). El impacto es un factor que incorpora la /diferencia/ entre la distribución
~objetivo~ --para integrales de la forma $\int f(x) \text{d}x$, implica la
distribución uniforme-- y la distribución ~sustituto~. Puedes leer más de esto
(aunque a un nivel mas técnico) en la sección 5 de las notas de
citet:Sanz-Alonso2019.


* Referencias                                                         :latex: 

bibliographystyle:abbrvnat
bibliography:./references.bib
