#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Integración Monte Carlo~
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+STARTUP: showall
#+PROPERTY: header-args:R :session intro :exports both :results output org :tangle ../rscripts/01-montecarlo.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022.\\
*Objetivo*. Estudiar integración numérica en el contexto probabilistico. Estudiar,
 en particular, el método Monte Carlo y entender sus bondades y limitaciones en
 el contexto de inferencia Bayesiana. \\
*Lectura recomendada*: Sección 6.1 de citet:Dogucu2021. Una lectura mas técnica
 sobre reglas de cuadratura se puede encontrar en la sección 3.1 de
 citet:Reich2015. Y una buena referencia (técnica) sobre el método Monte Carlo
 lo encuentran en citet:Sanz-Alonso2019.
#+END_NOTES


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#integración-numérica][Integración numérica]]
  - [[#ejemplo-proporción][Ejemplo: Proporción]]
  - [[#más-de-un-parámetro][Más de un parámetro]]
  - [[#reglas-de-cuadratura][Reglas de cuadratura]]
- [[#integración-monte-carlo][Integración Monte Carlo]]
  - [[#ejemplo-dardos][Ejemplo: Dardos]]
  - [[#propiedades][Propiedades]]
  - [[#ejemplo-proporciones][Ejemplo: Proporciones]]
  - [[#ejemplo-sabores-de-helados][Ejemplo: Sabores de helados]]
  - [[#tarea-sabores-de-helados][Tarea: Sabores de helados]]
- [[#extensiones-muestreo-por-importancia][Extensiones: Muestreo por importancia]]
  - [[#propiedades-muestreo-por-importancia][Propiedades: muestreo por importancia]]
  - [[#ejemplo][Ejemplo]]
- [[#referencias][Referencias]]
:END:



* Introducción

En inferencia bayesiana lo que queremos es poder resolver

\begin{align}
\mathbb{E}[f] = \int_{\Theta}^{} f(\theta) \, \pi(\theta | y ) \,  \text{d}\theta\,. 
\end{align}

#+BEGIN_NOTES

Lo que necesitamos es resolver integrales con respecto a la distribución de interés.

#+END_NOTES

#+REVEAL: split
#+ATTR_REVEAL: :frag (appear)
- La pregunta clave (I) es: ¿qué distribución?
- La pregunta clave (II) es: ¿con qué método numérico resuelvo la integral?
- La pregunta clave (III) es: ¿y si no hay método numérico? 

* Integración numérica

Recordemos la definición de integrales Riemann:

$$\int f(x) \text{d} x\,.$$

#+BEGIN_NOTES
La aproximación utilizando una malla de $N$ puntos sería: 
$$\sum_{n=1}^N f(u_n) \Delta u_n.$$

El método útil cuando las integrales se realizan cuando tenemos pocos parámetros. Es decir, $\theta \in \mathbb{R}^p$ con $p$ pequeña. 
#+END_NOTES


#+begin_src R :exports none :results none
  ## Setup --------------------------------------------------
#+end_src

#+begin_src R :exports none

  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), 
        axis.text = element_blank())

  ## Ejemplo de integracion numerica -----------------------

  grid.n          <- 11                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )


  norm.density <- tibble(x = seq(-5, 5, by = .01), 
         y = dnorm(x) ) 

#+end_src

#+RESULTS:
#+begin_src org
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature.jpeg :exports results :results output graphics file
  norm.cuadrature |>
    ggplot(aes(x=x + grid.size/2, y=y)) + 
    geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
    geom_bar(stat="identity", alpha = .3) + 
    geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
    sin_lineas + xlab('x') + ylab("density") + 
    annotate('text', label = expression(Delta~u[n]),
             x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
    annotate('text', label = expression(f(u[n]) ),
             x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
    annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
             x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
             angle = -90, alpha = .7, size = 12)
#+end_src
#+caption: Integral por medio de discretización.
#+RESULTS:
[[file:../images/quadrature.jpeg]]

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/quadrature-hi.jpeg :exports results :results output graphics file
  grid.n          <- 101                 # Número de celdas 
  grid.size       <- 6/(grid.n+1)       # Tamaño de celdas en el intervalo [-3, 3]
  norm.cuadrature <- tibble(x = seq(-3, 3, by = grid.size), y = dnorm(x) )

  norm.cuadrature |>
      ggplot(aes(x=x + grid.size/2, y=y)) + 
      geom_area(data = norm.density, aes(x = x, y = y), fill = 'lightblue') + 
      geom_bar(stat="identity", alpha = .3) + 
      geom_bar(aes(x = x + grid.size/2, y = -0.01), fill = 'black', stat="identity") + 
      sin_lineas + xlab('x') + ylab("density") + 
      annotate('text', label = expression(Delta~u[n]),
               x = .01 + 5 * grid.size/2, y = -.02, size = 12) + 
      annotate('text', label = expression(f(u[n]) ),
               x = .01 + 9 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2), size = 12) + 
      annotate('text', label = expression(f(u[n]) * Delta~u[n]), 
               x = .01 + 5 * grid.size/2, y = dnorm(.01 + 4 * grid.size/2)/2, 
               angle = -90, alpha = .7, size = 12)
#+end_src
#+caption: Integral por medio de una malla fina. 
#+RESULTS:
[[file:../images/quadrature-hi.jpeg]]

** Ejemplo: Proporción

Supongamos que $p(S_n = k|\theta) \propto \theta^k(1-\theta)^{n-k}$ cuando
observamos $k$ éxitos en $n$ pruebas independientes. Supongamos que nuestra
inicial es $p(\theta) = 2\theta$ (~checa que es una densidad~).

#+REVEAL: split
#+begin_src R :exports code :results none
  crear_log_post <- function(n, k){
    function(theta){
      verosim <- k * log(theta) + (n - k) * log(1 - theta)
      inicial <- log(2) + log(theta)
      verosim + inicial
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R
  # observamos 3 éxitos en 4 pruebas:
  log_post <- crear_log_post(4, 3)
  prob_post <- function(x) { exp(log_post(x))}
  # integramos numéricamente
  p_x <- integrate(prob_post, lower = 0, upper = 1, subdivisions = 100L)
  p_x
#+end_src

#+RESULTS:
#+begin_src org
0.033 with absolute error < 3.7e-16
#+end_src

#+REVEAL: split
Y ahora podemos calcular la media posterior:
\begin{align}
\mathbb{E}[\theta | S_n] = \int \theta \, \pi(\theta | S_n)\, \text{d}\theta\,.
\end{align}

#+begin_src R
      media_funcion <- function(theta){
        theta * prob_post(theta) / p_x$value
      }
      integral_media <- integrate(media_funcion,
                                  lower = 0, upper = 1,
                                  subdivisions = 100L)
      media_post <- integral_media$value 
      c(Numerico = media_post, Analitico = 5/(2+5))
#+end_src

#+RESULTS:
#+begin_src org
 Numerico Analitico 
     0.71      0.71
#+end_src

** Más de un parámetro

#+BEGIN_NOTES
Consideramos ahora un espacio con $\theta \in \mathbb{R}^p$. Si conservamos $N$
puntos por cada dimensión, ¿cuántos puntos en la malla necesitaríamos?  Lo que
tenemos son recursos computacionales limitados y hay que buscar hacer el mejor
uso de ellos. En el ejemplo, hay zonas donde no habrá contribución en la
integral.
#+END_NOTES


#+HEADER: :width 1500 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/eruption-quadrature.jpeg :exports results :results output graphics file
      canvas <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
       xlim(0.5, 6) +
       ylim(40, 110)

      grid.size <- 10 - 1

      mesh <- expand.grid(x = seq(0.5, 6, by = (6-.5)/grid.size),
                          y = seq(40, 110, by = (110-40)/grid.size))

    g1 <- canvas +
        geom_density_2d_filled(aes(alpha = ..level..), bins = 8) +
        scale_fill_manual(values = rev(color.itam)) + 
        sin_lineas + theme(legend.position = "none") +
        geom_point(data = mesh, aes(x = x, y = y)) + 
        annotate("rect", xmin = .5 + 5 * (6-.5)/grid.size, 
                  xmax = .5 + 6 * (6-.5)/grid.size, 
                  ymin = 40 + 3 * (110-40)/grid.size, 
                  ymax = 40 + 4 * (110-40)/grid.size,
                  linestyle = 'dashed', 
                 fill = 'salmon', alpha = .4) + ylab("") + xlab("") + 
        annotate('text', x = .5 + 5.5 * (6-.5)/grid.size, 
                         y = 40 + 3.5 * (110-40)/grid.size, 
                 label = expression(u[n]), color = 'red', size = 15) +
          theme(axis.ticks = element_blank(), 
              axis.text = element_blank())


    g2 <- canvas + 
        stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/grid.size, (110-40)/grid.size)) +
        sin_lineas + theme(legend.position = "none") +
        theme(axis.ticks = element_blank(), 
                axis.text = element_blank()) +
        scale_fill_distiller(palette = "Greens", direction = 1) + 
        sin_lineas + theme(legend.position = "none") +
        ylab("") + xlab("")

    g3 <- canvas + 
        stat_bin2d(aes(fill = after_stat(density)), binwidth = c((6-.5)/25, (110-40)/25)) +
        sin_lineas + theme(legend.position = "none") +
        theme(axis.ticks = element_blank(), 
                axis.text = element_blank()) +
        scale_fill_distiller(palette = "Greens", direction = 1) + 
        sin_lineas + theme(legend.position = "none") +
        ylab("") + xlab("")

  g1 + g2 + g3
#+end_src
#+caption: Integral por método de malla. 
#+RESULTS:
[[file:../images/eruption-quadrature.jpeg]]

** Reglas de cuadratura

Por el momento hemos escogido aproximar las integrales por medio de una aproximación con una ~malla uniforme~.
Sin embargo, se pueden utilizar aproximaciones 

$$\int f(x) \text{d} x \approx \sum_{n=1}^N f(\xi_n)\, \omega_n\,.$$

Estas aproximaciones usualmente se realizan para integrales en intervalos cerrados $[a,b]$. La regla de cuadratura determina los pesos $\omega_n$ y los centros $\xi_n$ pues se escogen de acuerdo a ~ciertos criterios de convergencia~.

#+BEGIN_NOTES
Por ejemplo, se consideran polinomios que aproximen con cierto grado de precisión el integrando. Los pesos y los centros se escogen de acuerdo a la familia de polinomios. Pues para cada familia se tienen identificadas las mallas que optimizan la aproximación. Ver sección 3.1 de citet:Reich2015. 
#+END_NOTES

* Integración Monte Carlo

\begin{gather*}
\pi(f) = \mathbb{E}_\pi[f] = \int f(x) \pi(x) \text{d}x\,,\\
\pi_N^{\textsf{MC}}(f) = \frac1N \sum_{n = 1}^N f( x^{(n)}), \qquad \text{ donde }  x^{(n)} \overset{\mathsf{iid}}{\sim} \pi, \qquad \text{ con } n = 1, \ldots, N \,, \\
 \pi(f) \approx \pi_N^{\textsf{MC}}(f)\,.
\end{gather*} 


** Ejemplo: Dardos

Consideremos el experimento de lanzar dardos uniformemente en un cuadrado de
tamaño 2, el cual contiene un circulo de radio 1.

#+HEADER: :width 1100 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-montecarlo.jpeg :exports results :results output graphics file
  ## Integración Monte Carlo ----------------------------------- 
  genera_dardos <- function(n = 100){
      tibble(x1 = runif(n, min = -1, max = 1), 
             x2 = runif(n, min = -1, max = 1)) %>% 
        mutate(resultado = ifelse(x1**2 + x2**2 <= 1., 1., 0.))
    }

    dardos <- tibble(n = seq(2,5)) %>% 
      mutate(datos = map(10**n, genera_dardos)) %>% 
      unnest() 

    dardos %>% 
      ggplot(aes(x = x1, y = x2)) + 
        geom_point(aes(color = factor(resultado))) + 
        facet_wrap(~n, nrow = 1) +  
      sin_lineas + sin_ejes + sin_leyenda + coord_equal()
#+end_src
#+caption: Integración Monte Carlo para aproximar $\pi$. 
#+RESULTS:
[[file:../images/dardos-montecarlo.jpeg]]

#+begin_src R :exports none :results none
  dardos |>
    group_by(n) |>
    summarise(aprox = 4 * mean(resultado)) |>
    as.data.frame()
#+end_src

#+RESULTS:
#+begin_src org
  n aprox
1 2   3.1
2 3   3.2
3 4   3.1
4 5   3.1
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/dardos-consistencia.jpeg :exports results :results output graphics file

  set.seed(1087)

  genera_dardos(n = 2**16) %>% 
    mutate(n = seq(1, 2**16), 
           approx = cummean(resultado) * 4) %>% 
    ggplot(aes(x = n, y = approx)) + 
      geom_line() + 
      geom_hline(yintercept = pi, linetype = 'dashed') + 
      scale_x_continuous(trans='log10', 
                         labels = trans_format("log10", math_format(10^.x))) + 
    ylab('Aproximación') + xlab("Muestras") + sin_lineas

#+end_src
#+caption: Estimación $\pi_N^{\textsf{MC}}(f)$ con $N \rightarrow \infty$. 
#+RESULTS:
[[file:../images/dardos-consistencia.jpeg]]

** Propiedades

*Teorema (~Error Monte Carlo~).* Sea $f : \mathbb{R}^p \rightarrow \mathbb{R}$
cualquier función bien comportada$^\dagger$.  Entonces, el estimador Monte Carlo es
*insesgado*. Es decir, se satisface 

\begin{align}
\mathbb{E}\left[ \pi_N^{\textsf{MC}}(f) - \pi(f)\right] = 0,
\end{align}
para cualquier $N$. Usualmente estudiamos el error en un escenario pesimista
donde medimos el *error cuadrático medio* en el peor escenario

\begin{align*}
\sup_{f \in \mathcal{F}} \, \,  \mathbb{E}\left[ \left(\pi_N^{\textsf{MC}}(f) - \pi(f) \right)^2 \right] \leq \frac1N.
\end{align*}

#+REVEAL: split
En particular, la varianza del estimador (*error estándar*) satisface la igualdad

$$ \textsf{ee}^2\left(\pi_N^{\textsf{MC}}(f)\right) = \frac{\mathbb{V}_\pi( f )}{N}.$$

#+REVEAL: split
*Teorema (~TLC para estimadores Monte Carlo~).* Sea $f$ una función *bien comportada*
$^{\dagger\dagger}$, entonces bajo una $N$ suficientemente grande tenemos
\begin{align}
\sqrt{N} \left(\pi_N^{\textsf{MC}} (f) - \pi(f) \right) \sim \mathsf{N}\left(0, \mathbb{V}_\pi(f)\right)\,.
\end{align}

** Ejemplo: Proporciones

Consideramos la estimación de una proporción $\theta$, tenemos como inicial
$p(\theta) \propto \theta$, que es una $\mathsf{Beta}(2,1)$. Si observamos 3
éxitos en 4 pruebas, entonces sabemos que la posterior es $p(\theta|x)\propto
\theta^4(1-\theta)$, que es una $\mathsf{Beta}(5, 2)$. Si queremos calcular la
media y el segundo momento posterior para $\theta$, en teoría necesitamos
calcular

\begin{align}
\mu_1 = \int_0^1 \theta \,\, p(\theta|X = 3)\, \text{d}\theta,\qquad  \mu_2=\int_0^1 \theta^2 \,\, p(\theta|X = 3)\, \text{d}\theta.
\end{align}

#+REVEAL: split
#+begin_src R :exports none :results none
  ### Ejemplo proporciones ------------------ 
#+end_src

Utilizando el ~método Monte Carlo~: 
#+begin_src R
theta <- rbeta(10000, 5, 2)
media_post <- mean(theta)
momento_2_post <- mean(theta^2)
c(mu_1 = media_post, mu_2 = momento_2_post)
#+end_src

#+RESULTS:
#+begin_src org
mu_1 mu_2 
0.71 0.54
#+end_src

#+REVEAL: split
Incluso, podemos calcular cosas mas /exóticas/ como
\begin{align}
P(e^{\theta}> 2|x)\,.
\end{align}

#+begin_src R
mean(exp(theta) > 2)
#+end_src

#+RESULTS:
#+begin_src org
[1] 0.61
#+end_src

** Ejemplo: Sabores de helados

Supongamos que probamos el nivel de gusto para 4 sabores distintos de una
paleta. Usamos 4 muestras de aproximadamente 50 personas diferentes para cada
sabor, y cada uno evalúa si le gustó mucho o no. Obtenemos los siguientes
resultados:
#+begin_src R :exports none :results none
  ### Ejemplo helados ------------------------- 
#+end_src

#+begin_src R :exports results
  datos <- tibble(
    sabor = c("fresa", "limon", "mango", "guanabana"),
    n = c(50, 45, 51, 50), gusto = c(36, 35, 42, 29)) %>% 
    mutate(prop_gust = gusto / n)

  datos |>
  as.data.frame()
#+end_src

#+caption: Resultados de las encuestas.
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust
1     fresa 50    36      0.72
2     limón 45    35      0.78
3     mango 51    42      0.82
4 guanábana 50    29      0.58
#+end_src

#+REVEAL: split
Usaremos como inicial $\mathsf{Beta}(2, 1)$ (pues hemos obervado cierto sesgo de
cortesía en la calificación de sabores, y no es tan probable tener valores muy
bajos) para todos los sabores, es decir $p(\theta_i)$ es la funcion de densidad
de una $\mathsf{Beta}(2, 1)$. La inicial conjunta la definimos entonces, usando
~independencia inicial~, como

$$p(\theta_1,\theta_2, \theta_3,\theta_4) = p(\theta_1)p(\theta_2)p(\theta_3)p(\theta_4)\,.$$

#+REVEAL: split
Pues inicialmente establecemos que ningún parámetro da información sobre otro:
saber que mango es muy gustado no nos dice nada acerca del gusto por fresa. Bajo
este supuesto, y el supuesto adicional de que las muestras de cada sabor son
independientes, podemos mostrar que las ~posteriores son independientes~:

$$p(\theta_1,\theta_2,\theta_3, \theta_4|k_1,k_2,k_3,k_4) = p(\theta_4|k_1)p(\theta_4|k_2)p(\theta_4|k_3)p(\theta_4|k_4)$$

#+REVEAL: split
#+begin_src R :exports results
  datos <- datos |>
    mutate(a_post = gusto + 2,
           b_post = n - gusto + 1,
           media_post = a_post/(a_post + b_post))
  datos |>
    as.data.frame()
#+end_src

#+caption: Resultado de inferencia Bayesiana. 
#+RESULTS:
#+begin_src org
      sabor  n gusto prop_gust a_post b_post media_post
1     fresa 50    36      0.72     38     15       0.72
2     limón 45    35      0.78     37     11       0.77
3     mango 51    42      0.82     44     10       0.81
4 guanábana 50    29      0.58     31     22       0.58
#+end_src

#+REVEAL: split
Podemos hacer preguntas interesantes como: ¿cuál es la probabilidad de que mango
sea el sabor preferido?  Para contestar esta pregunta podemos utilizar
simulación y responder por medio de un procedimiento Monte Carlo.

#+begin_src R :exports none :results none
  modelo_beta <- function(params, n = 5000){
    rbeta(n, params$alpha, params$beta)
  }
#+end_src

#+begin_src R :exports code :results none
  ## Generamos muestras de la posterior
  paletas <- datos |>
    mutate(alpha = a_post, beta = b_post) |>
    nest(params.posterior = c(alpha, beta)) |>
    mutate(muestras.posterior = map(params.posterior, modelo_beta)) |>
    select(sabor, muestras.posterior)
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/paletas-hist.jpeg :exports results :results output graphics file
  paletas |>
    unnest(muestras.posterior) |>
    ggplot(aes(muestras.posterior)) +
    geom_histogram(aes(fill = sabor), position = "identity" ) +
    sin_lineas
#+end_src
#+caption: Histogramas de la distribución predictiva marginal para cada $\theta_j$. 
#+RESULTS:
[[file:../images/paletas-hist.jpeg]]

#+REVEAL: split
#+begin_src R
  ## Utilizamos el metodo Monte Carlo para aproximar la integral. 
  paletas |>
    unnest(muestras.posterior) |>
    mutate(id = rep(seq(1, 5000), 4)) |> group_by(id) |>
    summarise(favorito = sabor[which.max(muestras.posterior)]) |>
    group_by(favorito) |> tally() |>
    mutate(prop = n/sum(n)) |>
    as.data.frame()
#+end_src
#+caption: Aproximación Monte Carlo.
#+RESULTS:
#+begin_src org
   favorito    n   prop
1     fresa  308 0.0616
2 guanábana    1 0.0002
3     limón 1319 0.2638
4     mango 3372 0.6744
#+end_src

#+BEGIN_NOTES
Escencialmente estamos preguntándonos sobre calcular la integral:
\begin{align}
\mathbb{P}(\text{mango sea preferido}) = \int_\Theta f(\theta_1, \ldots, \theta_4) \, p(\theta_1, \ldots, \theta_4 | X_1, \ldots, X_n) d\theta\,,
\end{align}
donde $f(\theta_1, \ldots, \theta_4) = \mathbb{I}_{[\theta_4 \geq \theta_j, j \neq 4]}(\theta_1, \ldots, \theta_4)$. 
#+END_NOTES

** Tarea: Sabores de helados

- ¿Cuál es la probabilidad a priori de que cada sabor sea el preferido?
- Con los datos de arriba, calcula la probabilidad de que la gente prefiera el sabor de mango sobre limón.



* Extensiones: Muestreo por importancia

Incluso cuando tenemos una integral *complicada* podemos ~relajar~ el problema de integración. De tal forma que podemos ~sustituir~
$$\int f(x) \pi(x) \text{d} x = \int f(x) \frac{\pi(x)}{\rho(x)}\,\rho(x) \text{d} x = \int f(x) \, w(x) \, \rho(x) \, \text{d}x\,,$$
donde $\rho$ es una densidad de una variable aleatoria ~adecuada~.

#+REVEAL: split
Esto nos permite utilizar lo que sabemos de las propiedades del método Monte Carlo para resolver la integral
\begin{align*}
\pi(f) =  \int f(x) \pi(x) \text{d} x = \int f(x) w(x) \, \rho(x) \, \text{d}x =: \rho(fw)\,,
\end{align*}
por medio de una aproximación
\begin{align}
\pi(f) \approx \sum_{n = 1}^{N} \bar w^{(n)} f(x^{(n)}), \qquad x^{(n)} \overset{\mathsf{iid}}{\sim} \rho\,.
\end{align}
#+REVEAL: split
Al estimador le llamamos el estimador por importancia y lo denotamos por
\begin{align}
\pi_N^{\mathsf{IS}}(f) = \sum_{n = 1}^{N} \bar w^{(n)} f(x^{(n)}), \qquad \bar w^{(n)} = \frac{w(x^{(n)})}{\sum_{m= 1}^{N}w(x^{(m)})}\,.
\end{align}

** Propiedades: muestreo por importancia

Lamentablemente, utilizar muestreo por importancia ~impacta la calidad de la
estimación~ (medida, por ejemplo, en términos del *peor error cuadrático medio
cometido*). El impacto es un factor que incorpora la /diferencia/ entre la distribución
~objetivo~ --para integrales de la forma $\int f(x) \text{d}x$, implica la
distribución uniforme-- y la distribución ~sustituto~. Puedes leer más de esto
(aunque a un nivel mas técnico) en la sección 5 de las notas de
citet:Sanz-Alonso2019.

** Ejemplo

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/muestreo-importancia-mezcla.jpeg :exports results :results output graphics file
  crea_mezcla <- function(weights){
      function(x){
        weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
          weights$w2 * dnorm(x, mean = 1.5, sd = .7)
      }
    }
  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))

  muestras_mezcla <- function(id){
    n <- 100
    tibble(u = runif(n)) |>
      mutate(muestras = ifelse(u <= .6,
                               rnorm(1, -1.5, sd = .5),
                               rnorm(1,  1.5, sd = .7))) |>
      pull(muestras)
  }

  muestras.mezcla <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_mezcla)) |>
    unnest(muestras) |>
    group_by(id) |>
    summarise(estimate = mean(muestras))

  g0 <- muestras.mezcla |>
    ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4, lty = 2, color = 'red') +
    geom_vline(xintercept = mean(muestras.mezcla$estimate), lty = 2, color = 'steelblue') +
    xlim(-1, 1) +
    ggtitle("Objetivo")

  muestras_uniforme <- function(id){
    n <- 100
    runif(n, -5, 5)
  }

  muestras.uniforme <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_uniforme)) |>
    unnest(muestras) |>
    mutate(pix = objetivo(muestras),
           gx  = dunif(muestras, -5, 5),
           wx  = pix/gx) |>
    group_by(id) |>
    summarise(estimate = sum(muestras * wx)/sum(wx))

  g1 <- muestras.uniforme |>
    ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4, lty = 2, color = 'red') +
    geom_vline(xintercept = mean(muestras.uniforme$estimate), lty = 2, color = 'steelblue') +
    xlim(-1, 1) +
    ggtitle("Uniforme(-5,5)")

  muestras_importancia <- function(id){
    n <- 100
    rnorm(n, 0, sd = 1)
  }  

  muestras.normal  <- tibble(id = 1:1000) |>
    mutate(muestras  = map(id, muestras_importancia)) |>
    unnest(muestras) |>
    mutate(pix = objetivo(muestras),
           gx  = dnorm(muestras, 0, sd = 1),
           wx  = pix/gx) |>
    group_by(id) |>
    summarise(estimate = sum(muestras * wx)/sum(wx))

  g2  <- muestras.normal |> ggplot(aes(estimate)) +
    geom_histogram() +
    geom_vline(xintercept = -1.5 * .6 + 1.5 * .4, lty = 2, color = 'red') +
    geom_vline(xintercept = mean(muestras.normal$estimate), lty = 2, color = 'steelblue') +
    xlim(-1, 1) +
    ggtitle("Normal(0, 2)")

  g0 + g1 + g2

#+end_src
#+caption: Muestreo por importancia utilizando distintas distribuciones instrumentales. Distribución /bootstrap/ de muestreo con $B = 10,000$ y $n = 100$. 
#+RESULTS:
[[file:../images/muestreo-importancia-mezcla.jpeg]]

#+BEGIN_NOTES
El análisis  del error en la sección anterior habla en del error cuadrático medio en el peor escenario posible bajo una familia de funciones de prueba (resumen). El ejemplo anterior muestra el error Monte Carlo cometido con respecto a una función resumen $f(\theta) = \theta$ con la cual, vemos, se reduce la varianza. Esto no contradice lo anterior pues para esta función resumen nuestra distribución instrumental satisface el criterio de reducción de varianza. En general, lo complicado es encontrar dicha distribución que podamos usar en la estimación Monte Carlo. 
#+END_NOTES


* Referencias                                                         :latex: 

bibliographystyle:abbrvnat
bibliography:./references.bib
