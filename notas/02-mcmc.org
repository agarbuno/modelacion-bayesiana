#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~MCMC~  
#+STARTUP: showall
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/02-mcmc.pdf
:END:
#+PROPERTY: header-args:R :session mcmc :exports both :results output org :tangle ../rscripts/02-mcmc.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2023 | MCMC.\\
*Objetivo*. Estudiar el método general de integración Monte Carlo vía cadenas de Markov (MCMC). La estrategia será construir poco a poco utilizando los principios básicos que lo componen. \\
*Lectura recomendada*: Capítulo 7 de citep:Dogucu2021. Sección 6 de citep:Sanz-Alonso2019 y Capítulo 3 de citep:Reich2015 (avanzado). Si te interesa saber mas sobre programación orientada a objetos dentro del contexto de ~R~ puedes consultar citep:Wickham2019. 
#+END_NOTES


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#muestreo-por-aceptación-rechazo][Muestreo por aceptación rechazo]]
  - [[#implementación][Implementación]]
    - [[#disclaimer][Disclaimer:]]
    - [[#implementación-de-una-distribución-de-muestreo][Implementación de una distribución de muestreo.]]
    - [[#ejercicio][Ejercicio:]]
  - [[#propiedades][Propiedades]]
- [[#qué-hemos-visto][¿Qué hemos visto?]]
- [[#muestreo-por-cadenas-de-markov][Muestreo por cadenas de Markov]]
  - [[#definición-cadena-de-markov][Definición [Cadena de Markov]:]]
- [[#generalizando][Generalizando...]]
  - [[#pseudo-código][Pseudo-código]]
  - [[#desentrañando][Desentrañando]]
  - [[#implementación][Implementación]]
    - [[#ejercicio][Ejercicio:]]
    - [[#ejercicio][Ejercicio:]]
    - [[#tarea][Tarea:]]
- [[#el-método-metropolis-hastings][El método Metropolis-Hastings]]
  - [[#ejercicio-3][Ejercicio (3)]]
  - [[#distribución-propuesta][Distribución propuesta]]
- [[#en-más-dimensiones][En más dimensiones]]
- [[#por-qué-funciona][¿Por qué funciona?]]
  - [[#definición-invarianza][Definición [Invarianza]:]]
  - [[#lema-comportamiento-asintótico-de-metropolis-hastings][Lema [Comportamiento asintótico de Metropolis-Hastings]:]]
:END:

* Introducción

#+begin_src R :exports none :results none
  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  ## Para el tema de ggplot
  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
#+end_src

El interés es poder resolver
\begin{align}
\mathbb{E}[f] = \int_{\Theta}^{} f(\theta) \, \pi(\theta | y ) \,  \text{d}\theta\,. 
\end{align}

Sin embargo, ~no podemos generar~ $\theta^{(i)} \overset{\mathsf{iid}}{\sim} \pi(\theta|y)$.

* Muestreo por aceptación rechazo

Podemos utilizar una versión estocástica de muestreo por importancia.

#+BEGIN_NOTES
Para muestrear de $\pi$ necesitamos utilizar una distribución sustituto (lo
mismo hicimos con muestreo por importancia). Sólo que ahora permitimos rechazar
muestras que no correspondan con las regiones de alta densidad de nuestra
distribución objetivo. El rechazo se realiza lanzando una moneda. La tasa de
éxito depende del qué tanto cubre nuestra distribución sustituto.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Muestreo por aceptacion rechazo ---------------
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rejection-sampling.jpeg :exports results :results output graphics file
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }

  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))

  tibble(x = seq(-5, 5, length.out = 100)) |>
    mutate(y = objetivo(x),
           aprox = 3.3 * dnorm(x, 0, sd = 2)) |>
    ggplot(aes(x,y)) +
    geom_area(fill = "lightblue") +
    geom_line(aes(x, aprox), lty = 2) +
    geom_ribbon(aes(ymin = y, ymax = aprox), fill = "salmon") + sin_lineas +
    sin_ejes

#+end_src
#+caption: Esquema de muestreo. 
#+RESULTS:
[[file:../images/rejection-sampling.jpeg]]

** Implementación
Necesitamos algunas cosas. Ser capaces de ~evaluar~ nuestra distribución
objetivo. Ser capaces de ~evaluar~ *y* ~muestrear~ de nuestra distribución de
muestreo.

#+REVEAL: split
#+caption: Distribución objetivo. 
#+begin_src R :exports code :results none
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }
  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))
  M        <- 3.3
#+end_src

*** /Disclaimer/:
:PROPERTIES:
:reveal_background: #00468b
:END:

El objetivo del curso *no* es enseñar programación orientada a objetos. Sin
embargo, permitirá abstraer los puntos importantes y concentrarnos en las ideas
generales y no preocuparnos por lo detalles.

*** Implementación de una distribución de muestreo.

Recordemos que lo que queremos son dos cosas: generar números aleatorios y evaluar la función de densidad. 

#+caption: Distribución de muestreo. 
#+begin_src R :exports code :results none
  library(R6)
  ModeloNormal <-
    R6Class("ProbabilityModel",
            list(
              mean = NA,
              sd = NA,
              ## Inicializador
              initialize = function(mean = 0, sd = 1){
                self$mean = mean
                self$sd   = sd
              },
              ## Muestreador
              sample = function(n = 1){
                rnorm(n, self$mean, sd = self$sd)              
              },
              ## Evaluacion de densidad
              density = function(x, log = TRUE){
                dnorm(x, self$mean, sd = self$sd, log = log)
              }           
            ))
#+end_src

#+BEGIN_NOTES
En muestreo con rechazo necesitamos definir una distribución de la
cual *si podamos* generar números aleatorios. El inconveniente es, además, *conocer*
qué tanto podemos inflar la densidad de nuestra propuesta para /cubrir/ la
distribución objetivo.
#+END_NOTES

#+REVEAL: split
#+caption: Algoritmo de muestreo con rechazo. 
#+begin_src R :exports code :results none
  crea_rejection_sampling <- function(objetivo, aprox, M){
    function(niter){
      muestras <- matrix(nrow = niter, ncol = 3)
      for (ii in seq(1, niter)){
        propuesta <- aprox$sample()
        p <- objetivo(propuesta)
        g <- aprox$density(propuesta, log = FALSE)
        u <- runif(1)
        if (u < p/(M * g)) {  ## Aceptamos 
          muestras[ii, 1] <- 1
        } else {              ## Rechazamos 
          muestras[ii, 1] <- 0
        }
        muestras[ii, 2] <- propuesta
        muestras[ii, 3] <- u 
      }
      colnames(muestras) <- c("accept", "value", "auxiliar")
      muestras
    }
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/muestreo-aceptacion.jpeg  :exports results :results output graphics file
  modelo.muestreo  <- ModeloNormal$new(mean = 0, sd = 2)
  muestreo_rechazo <- crea_rejection_sampling(objetivo, modelo.muestreo, M)

  muestras <- muestreo_rechazo(5000) |>
    as.tibble() |>
    mutate(density = modelo.muestreo$density(value, log = FALSE))

  g1 <- muestras |>
    ggplot(aes(value, auxiliar * modelo.muestreo$density(value, log = FALSE))) +
    geom_point(aes(color = factor(accept))) + sin_lineas + sin_ejes + sin_leyenda +
    xlab("") + ylab("") +
    ggtitle(paste("Muestras en el espacio (x,u), aceptación: ", mean(muestras$accept)))

  g2 <- muestras |>
    filter(accept == 1) |>
    ggplot(aes(value)) +
    geom_histogram() + 
    sin_lineas + sin_ejes + sin_leyenda +
    xlab("") + ylab("") +
    ggtitle("Histograma de las muestras generadas")

  g1 + g2 
#+end_src

#+RESULTS:
[[file:../images/muestreo-aceptacion.jpeg]]

*** Ejercicio:
:PROPERTIES:
:reveal_background: #00468b
:END:

- ¿Qué pasa si $M$ es demasiado grande? Juega con el código e interpreta los resultados. 
- ¿Qué pasa si $M$ no es suficiente para cubrir la distribución objetivo? Juega con el código e interpreta los resultados.

** Propiedades

*Lema (~Consistencia de muestreo por rechazo~)*. El método de muestreo por aceptación-rechazo genera muestras $x^{(i)}$ con $i = 1, \ldots, N$ que son independientes y distribuidas acorde a la distribución objetivo $\pi$.

#+REVEAL: split
/Prueba/. Usemos probabilidad condicional para medir
\begin{align}
\pi(x | \textsf{aceptar}) = \frac{\pi(\textsf{aceptar} | x) \times \pi(x)}{\pi(\textsf{aceptar})}\,.
\end{align}

* ¿Qué hemos visto?

- El método Monte Carlo se puede utilizar para aproximar integrales.
- Se puede utilizar una distribución sustituto para generar números aleatorios que nos interesan.
- Podemos lanzar monedas para /filtrar/ sólo los aleatorios que tengan altas probabilidades.
- Hemos utilizado el supuesto de independencia.

* Muestreo por cadenas de Markov

Vamos a ~relajar~ el supuesto de ~independencia~. Es decir, vamos a generar una
secuencia de números aleatorios con cierta correlación.

*** ~Definición~ [Cadena de Markov]:
Un *proceso estócastico* en tiempo discreto --una colección de variables
 aleatorias $X_1, X_2, \ldots$ -- que satisface la propiedad de dependencia
 condicional
\begin{align}
\mathbb{P}\left( X_{n+1}  = x | X_1 = x_1, \ldots, X_n = x_n \right) = \mathbb{P}\left( X_{n+1}  = x | X_n = x_n \right)\,,
\end{align}
se llama una *cadena de Markov* en tiempo discreto.

** Ejemplo:

#+DOWNLOADED: screenshot @ 2022-02-03 12:21:07
#+caption: Problema del café. 
#+attr_html: :width 700 :align center
[[file:images/20221031-163123_screenshot.png]]

#+REVEAL: split
El vendedor de galletas quiere satisfacer la demanda para acompañar un café. El vendedor:
- Viaja entre las islas.
- Decide si se queda o no se queda en la isla donde está. 
- Se puede mover entre islas contiguas (a través de puentes). 
- Tiene mala memoria y  pregunta el número de casas en las islas aledañas (todos los días).
- Quiere visitar todas las islas y vender galletas.
- Viaja en bicicleta. 


#+REVEAL: split
También es astuto. Sabe que en /donde haya /mucha gente venderá mas/, pero también
sabe que una isla siempre lo /podría llevar a una mas grande/. Asi que a veces le
convendrá viajar a una isla pequeña. Asi que utilizará el ~principio de
aceptación rechazo~ para decidir si se moverá a la siguiente isla.

#+REVEAL: split
1. Lanza una moneda para decidir si se mueve a la izquierda o derecha.
2. Decide si se mueve de acuerdo al cociente de poblaciones.

** Pregunta

En el contexto de nuestro problema ¿qué cambiaría si tuviera conocimiento censal
del archipiélago y pudiera viajar en avión?

** Modelación del /tour/ de ventas

El vendedor se encuentra en el $t$ -ésimo día. Supongamos que va a evaluar si se
cambia a la isla de la derecha. Sea $\pi_\star$ la población de la isla propuesta y
$\pi_{t}$ la población de la isla actual. Entonces el vendedor acepta cambiar de isla
con probabilidad

$$\alpha_{\textsf{mover}}= \frac{\pi_\star}{\pi_{t}}\,.$$

#+BEGIN_NOTES
Nota que nunca dudará moverse a una isla mas grande. Por otro lado, entre mas
parecidas sean las poblaciones de las islas mas *indeciso* será de moverse. Por
definición $\alpha_{\textsf{mover}} \in (0,1)$. De hecho, podemos definir la
probabilidad de aceptar un viaje a otra isla por medio de

$$\alpha(t, \star) = \min \Bigg\{ 1, \frac{\pi_\star}{\pi_{t}}\Bigg\},$$

pues incluye los dos casos. 
#+END_NOTES

#+REVEAL: split
#+begin_src R :exports none :results none
  ## Caminata entre islas --------------------------
  set.seed(1087)
#+end_src

#+caption: Mecanismo de cambio o permanencia desde la isla $i$. 
#+begin_src R :exports code :results none
  islas <- tibble(islas = 1:7, pob = c(1,2,3,4,5,4,3))
  camina_isla <- function(i){ # i: isla actual
    u_izq <- runif(1) # Lanzamos volado para ver si nos vamos izq o der. 
    v <- ifelse(u_izq < 0.5, i - 1, i + 1)  # Pedimos índice isla vecina. 
    if (v < 1 | v > 7) { # si estas en los extremos y el volado indica salir
      return(i)
    }
    u_cambio <- runif(1) # Moneda de aceptacion de cambio
    p_cambio = min(islas$pob[v]/islas$pob[i], 1)
    if (u_cambio < p_cambio) {
      return(v) # isla destino
    }
    else {
      return(i) # me quedo en la misma isla
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports none :results none
  pasos <- 100000; iteraciones <- numeric(pasos)
  iteraciones[1] <- sample(1:7, 1) # isla inicial
  for (j in 2:pasos) {
      iteraciones[j] <- camina_isla(iteraciones[j - 1])
  }
  caminata <- tibble(paso = 1:pasos, isla = iteraciones)
#+end_src

#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/caminata-cafe.jpeg :exports results :results output graphics file
  plot_caminata <- ggplot(caminata[1:500, ], aes(x = paso, y = isla)) +
    geom_point(size = 0.8) +
    geom_path(alpha = 0.5) +
    labs(title = "Caminata aleatoria") +
    scale_x_continuous(trans = "log10", "Tiempo", breaks = c(1, 2, 5, 20, 100, 500)) +
    scale_y_continuous( expression(theta)) + sin_lineas
  plot_dist <- ggplot(caminata, aes(x = isla)) +
    geom_bar(fill = "darkgray", aes(y = (..count..)/sum(..count..))) +
    geom_bar(data = islas |>  mutate(prop = pob/sum(pob)),
             aes(x = islas, y = prop), fill = "steelblue", alpha = .3, stat = "identity") + 
    scale_x_continuous(expression(theta), breaks = 1:10) +
    ylim(0,.5) + 
    labs(title = "Distribución objetivo (Histograma)", 
         y = expression(hat(pi)(theta))) + sin_lineas + coord_flip()
  plot_caminata + plot_dist
#+end_src
#+caption: Caminata aleatoria en un archipiélago de 7 islas. 
#+RESULTS:
[[file:../images/caminata-cafe.jpeg]]

#+begin_src R :exports none :results none :eval never :tangle no
  ## Animación histograma -----------------------------------
  library(gganimate)
  res <- caminata |>
    mutate(tiempo = cut(paso, breaks = seq(0, n(), by = 10))) |>
    group_by(isla, tiempo) |>
    count() |>
    ungroup() |>
    complete(tiempo, nesting(isla), fill = list(n = 0)) |>
    group_by(isla) |>
    mutate(count = cumsum(n)) |>
    group_by(tiempo) |>
    mutate(prop = count/sum(count)) |>
    arrange(tiempo, isla) |>
    ungroup()

  anim <- res |>
    mutate(tiempo = as.numeric(tiempo)) |>
    filter(tiempo <= 1500) |>
    ggplot(aes(x = isla, y = prop)) +
    geom_bar(fill = "darkgray", stat = "identity") +
    coord_flip() + sin_lineas +
    geom_bar(data = islas |>  mutate(prop = pob/sum(pob)),
             aes(x = islas, y = prop), fill = "steelblue", alpha = .3, stat = "identity") + 
    scale_x_continuous(expression(theta), breaks = 1:10) +
    transition_states(tiempo, transition_length = 2, state_length = 1) +
    ease_aes("exponential-out")

  animate(anim, renderer = ffmpeg_renderer(), height = 300, width = 900)

  anim_save("./images/islas-histograma.mp4")

#+end_src

** Conclusiones

- La estrategia del vendedor le permitirá, en el ~largo plazo~,  visitar todas las islas.
- El tiempo que pasa en cada isla$^\dagger$ corresponde a la población relativa.
- Al principio, aún no representa dicha proporción.

* Generalizando... 

Supongamos que tenemos un modelo
\begin{gather}
Y| \mu \sim \mathsf{N}(\mu, 0.75^2)\,,\\
\mu \sim \mathsf{N}(0,1^2)\,.
\end{gather}

~Verifica~ que bajo la observación $y = 6.25$ la distribución posterior que nos interesa es
\begin{gather}
\mu | y \sim \mathsf{N}(4, 0.6^2)\,.
\end{gather}

#+REVEAL: split
~Vamos a suponer~ que *no* sabemos muestrear de una Normal. Asi que usaremos una
estrategia parecida que con el vendedor de galletas. La estrategia será:
1. Generar una propuesta $\mu_\star$ para cambiarnos de nuestro valor actual $\mu_t$.
2. Decidir si nos movemos utilizando un cociente que tome en cuenta los pesos relativos.

** Pseudo-código 
- Vamos a proponer una ''moneda'' para lanzar la *dirección* de movimiento. Esto lo haremos con
  
  \begin{align}
  \mu_\star | \mu_t \sim \mathsf{Uniforme}( \mu_t - \omega, \mu_t + \omega)\,.
  \end{align}

#+REVEAL: split
- Vamos a decidir si nos movemos de acuerdo a los pesos relativos
  \begin{align}
  \alpha(\mu_t, \mu_\star)  = \min \left\lbrace1 , \frac{\pi(\mu_\star|y)}{\pi(\mu_t|y)} \right\rbrace\,.
  \end{align}
  
** Desentrañando

Escribamos el cociente en términos de la densidad de la distribución posterior y simplifiquemos. ¿Qué observas? 

** Implementación

Veamos cómo implementarlo. Vamos a suponer una distribución de muestreo con un intervalo de longitud 2. Es decir,  $\omega = 1$. 

#+begin_src R :exports none :results none
  ## Caminata en espacio continuo ------------------------
#+end_src

#+REVEAL: split
#+caption: Modelo de muestreo uniforme. 
#+begin_src R :exports code :results none
  ModeloUniforme <-
    R6Class("ProbabilityModel",
            list(
              a = NA,
              b = NA, 
              initialize = function(a = 0, b = 1){
                self$a = a
                self$b = b
              }, 
              sample = function(n = 1){
                runif(n, self$a, self$b)              
              },
              density = function(x, log = TRUE){
                dunif(x, self$a, self$b, log = log)
              }           
            ))
#+end_src

#+REVEAL: split
#+caption: Nuestra segunda cadena de Markov. 
#+begin_src R :exports code :results none
  crea_cadena_markov <- function(objetivo, muestreo){
    function(niter){
      muestras <- matrix(nrow = niter, ncol = 2)
      ## Empezamos en algun lugar
      estado   <- muestreo$sample()
      muestras[1,1] <- 1
      muestras[1,2] <- estado
      for (ii in 2:niter){
        ## Generamos un candidato (caminata aleatoria)
        propuesta   <- estado + muestreo$sample()
        p_propuesta <- objetivo$density(propuesta, log = FALSE)
        p_estado    <- objetivo$density(estado, log = FALSE)
        ## Evaluamos probabilidad de aceptar
        if (runif(1) < p_propuesta/p_estado) {
          muestras[ii, 1] <- 1 ## Aceptamos
          muestras[ii, 2] <- propuesta
        } else {
          muestras[ii, 1] <- 0 ## Rechazamos
          muestras[ii, 2] <- estado
        }
        estado <- muestras[ii, 2]
      }
      colnames(muestras) <- c("accept", "value")
      muestras
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  objetivo <- ModeloNormal$new(mean = 4, sd = .6)
  muestreo <- ModeloUniforme$new(a = -1, b = 1)

  mcmc <- crea_cadena_markov(objetivo, muestreo)
  muestras <- mcmc(5000)
#+end_src

#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/primer-mcmc.jpeg :exports results :results output graphics file
  g1 <- muestras |>
    as.tibble() |>
    mutate(iter = 1:n()) |>
    ggplot(aes(iter, value)) +
    geom_line() + sin_lineas + 
    ggtitle(paste("Trayectoria, eficiencia: ", mean(muestras[,1])))

  g2 <- muestras |>
    as.tibble() |>
    ggplot(aes(value)) +
    geom_histogram(aes(y = ..density..)) +
    stat_function(fun = objetivo$density,
                  args = list(log = FALSE),
                  color = "salmon",
                  size = 2) + sin_lineas + 
    ggtitle("Histograma")

  g1 + g2
#+end_src
#+caption: Nuestra segunda cadena de Markov. 
#+RESULTS:
[[file:../images/primer-mcmc.jpeg]]

*** Ejercicio:
:PROPERTIES:
:reveal_background: #00468b
:END:

Sin modificar el número de iteraciones, considera cambiar la dispersión de la distribución de muestreo.
- ¿Qué observas si $\omega = 0.01$?
- ¿Qué observas si $\omega = 100$?

*** Ejercicio:
:PROPERTIES:
:reveal_background: #00468b
:END:

Regresa a nuestro ejemplo conjugado Beta-Binomial. Considera una previa $\theta \sim \mathsf{Beta}(2,3)$ y una verosimilitud $Y|\theta \sim \mathsf{Binomial}(2, \theta)$. Escribe la distribución posterior asumiendo $Y = k$. 

#+REVEAL: split
Para este caso tenemos un ligero inconveniente. El soporte para $\theta$ es el intervalo cerrado $[0,1]$ y utilizar una propuesta como en el caso anterior nos podría colocar (casi seguramente) fuera del intervalo. Así que lo que haremos será un pequeña modificación a cómo generamos nuestra propuesta y cómo evaluamos la probabilidad de aceptar dicha propuesta.

#+REVEAL: split
- Vamos a generar propuestas de la siguiente manera
  \begin{align}
  \theta_\star | \theta_t \sim \mathsf{Beta}(\alpha, \beta)\,.
  \end{align}
- Vamos a calcular la probabilidad de aceptar dicho movimiento a través de
  \begin{align}
  \alpha(\theta_t, \theta_\star) = \min \left\lbrace 1,  \frac{\pi(\theta_\star|y)}{\pi(\theta_t|y)} \cdot \frac{g(\theta_t)}{g(\theta_\star)}\right\rbrace\,,
  \end{align}
  donde $g$ denota la densidad de la distribución de muestreo definida arriba.

*** Tarea:
:PROPERTIES:
:reveal_background: #00468b
:END:
Modifica el código de clase para implementar este muestreador. Utiliza distintas configuraciones de $a,b$ para la distribución de propuesta. Compara con muestras exactas del modelo posterior bajo la observación $Y = 1$.

* El método Metropolis-Hastings 

La forma más general que tenemos para generar una cadena de muestras es el método de Metropolis-Hastings.
#+REVEAL: split

- Generamos propuestas en cada iteración por medio de 
  \begin{align}
  \theta_\star | \theta_t \sim q( \theta_\star | \theta_t )\,.
  \end{align}
- Calculamos la probabilidad de aceptar la propuesta como 
  \begin{align}
  \alpha(\theta_t, \theta_\star) = \min \left\lbrace 1,  \frac{\pi(\theta_\star)}{\pi(\theta_t)} \cdot \frac{q(\theta_t|\theta_\star)}{q(\theta_\star|\theta_t)}\right\rbrace\,,
  \end{align}
  donde la notación hace énfasis en que este mecanismo puede generar muestras de
  la distribución $\pi$ utilizando un generador $q$.

** Ejercicio (3)
:PROPERTIES:
:reveal_background: #00468b
:END:

- Repasemos ~los métodos anteriores~.
- ¿Qué pasa si desconocemos la constante de normalización de la distribución objetivo?

** Distribución propuesta

El /arte/ está en proponer una distribución de muestreo eficiente. Como ya hemos
discutido, si no está bien calibrada podríamos tener un comportamiento no
deseado. Supongamos que queremos muestrear de una $\mathsf{Gamma}(20,
100)$. Para esto veamos tres configuraciones de la distribución de muestreo que será
$\mathsf{N}(\theta_t, \sigma^2)$. 

#+begin_src R :exports none :results none
  ## Implementacion Metropolis Hastings -----------------------
  ModeloGamma <-
    R6Class("ProbabilityModel",
            list(
              shape = NA,
              rate  = NA, 
              initialize = function(a = 0, b = 1){
                self$shape = a
                self$rate  = b
              }, 
              sample = function(n = 1){
                rgamma(n, shape = self$shape, rate = self$rate)              
              },
              density = function(x, log = TRUE){
                dgamma(x, shape = self$shape, rate = self$rate, log = log)
              }           
            ))
#+end_src

#+begin_src R :exports none :results none
  ### Muestreador Metropolis-Hastings -------------------------
  crea_metropolis_hastings <- function(objetivo, muestreo){
    ## Este muestreador aprovecha la simetría de la propuesta 
    function(niter){
      ## Empezamos en algun lugar
      estado <- muestreo$sample()
      ndim <- length(estado) 
      muestras <- matrix(nrow = niter, ncol = ndim + 1)      
      muestras[1,2:(ndim+1)] <- estado
      muestras[1,1] <- 1
      for (ii in 2:niter){
        propuesta   <- estado + muestreo$sample()
        log_pi_propuesta <- objetivo$density(propuesta)
        log_pi_estado    <- objetivo$density(estado)
        log_alpha <- log_pi_propuesta - log_pi_estado

        if (log(runif(1)) < log_alpha) {
          muestras[ii, 1] <- 1 ## Aceptamos
          muestras[ii, 2:(ndim+1)] <- propuesta
        } else {
          muestras[ii, 1] <- 0 ## Rechazamos
          muestras[ii, 2:(ndim+1)] <- estado
        }
        estado <- muestras[ii, 2:(ndim+1)]
      }
      if (ndim == 1) {colnames(muestras) <- c("accept", "value")}
      muestras
    }
  }
#+end_src


#+REVEAL: split
#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-pasochico.jpeg :exports results :results output graphics file
  set.seed(108727)
  objetivo <- ModeloGamma$new(a = 20, b = 100)
  muestreo <- ModeloNormal$new(sd = 0.001)
  mcmc_chico <- crea_metropolis_hastings(objetivo, muestreo)

  g1 <- mcmc_chico(3000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso muy pequeño. 
#+RESULTS:
[[file:../images/mh-pasochico.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-pasogrande.jpeg :exports results :results output graphics file
  set.seed(108727)
  muestreo <- ModeloNormal$new(sd = 20)
  mcmc_grande <- crea_metropolis_hastings(objetivo, muestreo)

  g1 <- mcmc_grande(3000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso muy grande. 
#+RESULTS:
[[file:../images/mh-pasogrande.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-pasojusto.jpeg :exports results :results output graphics file
  set.seed(108727)
  muestreo <- ModeloNormal$new(sd = 0.1)
  mcmc_justo <- crea_metropolis_hastings(objetivo, muestreo)

  g1 <- mcmc_justo(3000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso /justo/. 
#+RESULTS:
[[file:../images/mh-pasojusto.jpeg]]

#+REVEAL: split
#+begin_src R :exports results :results org
  tibble(configuracion = c("Paso chico", "Paso grande", "Paso justo"), 
         cadena   = c(mcmc_chico, mcmc_grande, mcmc_justo)) |>
    mutate(muestras = map(cadena, function(x) {
      set.seed(108727)
      x(3000) |>
        as.tibble()
    })) |>
    unnest(muestras) |>
    group_by(configuracion) |>
    summarise(media = mean(value),
              tasa.aceptacion = mean(accept)) |>
    rbind(tibble(configuracion = "Teorica",
                 media = objetivo$shape/objetivo$rate,
                 tasa.aceptacion = NA))
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 3 × 3
  configuracion  media tasa.aceptacion
  <chr>          <dbl>           <dbl>
1 Paso chico    0.0863         0.944  
2 Paso grande   0.309          0.00667
3 Paso justo    0.197          0.463
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-largoplazo.jpeg :exports results :results output graphics file :eval never
  set.seed(108727)

  g1 <- mcmc_chico(1000000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso /pequeño/ y un periodo suficientemente amplio. 
#+RESULTS:
[[file:../images/mh-largoplazo.jpeg]]

* En más dimensiones

Consideremos la siguiente distribución objetivo
\begin{align}
\theta \sim \mathsf{N}(\textsf{m}, \textsf{S}), \qquad \textsf{m} = (1,2)^\top, \qquad \mathsf{S} = \begin{pmatrix}1 & .75\\.75 &1 \end{pmatrix}\,,
\end{align}
y utilicemos el modelo de muestreo 
\begin{align}
\theta \sim \mathsf{N}(\mathsf{0}, \mathsf{\Sigma}), \qquad \mathsf{0} \in \mathbb{R}^2, \qquad \mathsf{\Sigma} =\sigma^2 \cdot \begin{pmatrix}1 & 0\\0 &1 \end{pmatrix}\,.
\end{align}

#+begin_src R :exports none :results none
  ## En mas dimensiones -------------------------------
#+end_src
#+REVEAL: split
#+caption: Modelo de muestreo multivariado.
#+begin_src R :exports code :results none
  library(mvtnorm)
  ModeloNormalMultivariado <-
    R6Class("ProbabilityModel",
            list(
              mean = NA,
              cov  = NA, 
              initialize = function(mu = 0, sigma = 1){
                self$mean = mu
                self$cov  = sigma |> as.matrix()
              }, 
              sample = function(n = 1){
                rmvnorm(n, mean = self$mean, sigma = self$cov)              
              },
              density = function(x, log = TRUE){
                dmvnorm(x, self$mean, self$cov, log = log)              
              }           
            ))
#+end_src

#+begin_src R :exports none :results none
  mu <- c(1, 2)
  Sigma <- matrix(c(1, .75, .75, 1), nrow = 2)
  objetivo <- ModeloNormalMultivariado$new(mu, Sigma)


  genera_experimento <- function(sigma){
    muestreo <- ModeloNormalMultivariado$new(c(0,0),
                                             sigma * diag(c(1,1)))
    set.seed(10)
    mcmc_multi <- crea_metropolis_hastings(objetivo, muestreo)
    mcmc_multi(50) |>
      as.tibble()
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal-propuestas-mh.jpeg :exports results :results output graphics file
  set.seed(108727)
  ## Para dibujar las curvas de nivel - distribucion objetivo 
  plot.grid <- expand_grid(x = seq(-2,5, by = 7/99), y = seq(-1,5, by = 6/99))
  plot.grid <- plot.grid %>% 
    mutate(density.target = dmvnorm(plot.grid, mean = mu, sigma = Sigma))
  plot.breaks.target <- plot.grid %>% 
    summarise(breaks = quantile(density.target, probs = c(.67, .90, .99, 1))) %>% 
    pull(breaks)

  ## Caminatas aleatorias 
  muestras.normal <- tibble(sigma = c(.1, .75, 2.33/sqrt(2), 5)) |>
     mutate(muestras = map(sigma, genera_experimento)) |>
     unnest(muestras)

  ## Para dibujar las curvas de nivel - distribucion propuesta
  contours.proposal <- muestras.normal |>
    filter(sigma == 2.33/sqrt(2)) |>
    slice(1,3,7) |> mutate(id = 1:3) |>
    nest(location = c(V2, V3)) |>
    mutate(density.proposal = map(location,
           function(x){
             dmvnorm(plot.grid |> select(x,y),
                     mean = as.matrix(x),
                     sigma = 1.65 * diag(c(1,1)))
           }),
           coords = list(plot.grid |> select(x,y))) |>
    mutate(breaks.proposal = map(density.proposal, quantile, probs = c(.67,.90,.99)))

  contours.proposal |>
    unnest(location, density.proposal, coords) |> 
    ggplot(aes(x, y, z = density.proposal)) +
    geom_contour_filled(bins = 4) + scale_fill_brewer(palette = "Purples") + 
    geom_point(data = contours.proposal |> unnest(location),
               aes(V2, V3), shape = 19, size = 10) +
    geom_contour(data = plot.grid, aes(x,y,z = density.target),
                 breaks = plot.breaks.target, color = "black") +
    xlab(expression(x[1])) + ylab(expression(x[2])) + 
    facet_wrap(~id) + sin_lineas + coord_equal() + sin_leyenda
#+end_src
#+caption: Propuestas Gaussianas (morado) contra densidad objetivo (línea sólida). Tres primeras iteraciones.
#+RESULTS:
[[file:../images/multinormal-propuestas-mh.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal-aceptacion-mh.jpeg :exports results :results output graphics file
  contours.proposal |>
    mutate(denominator = map(location, objetivo$density),
           numerator   = map(coords  , objetivo$density)) |>
    unnest(numerator, denominator) |>
    mutate(metropolis.hastings = ifelse(exp(numerator-denominator) < 1,
                                        exp(numerator-denominator), 1.00),
           contours.proposal |> unnest(coords) |> select(x,y),
           contours.proposal |> unnest(density.proposal),
           alpha = metropolis.hastings * density.proposal) |>
    ggplot(aes(x, y, z = log(metropolis.hastings + 1))) +
    geom_contour_filled(bins = 7) +
    scale_fill_brewer(palette = "Purples", direction = 1) +
    facet_wrap(~id) + sin_lineas + coord_equal() + sin_leyenda + 
    geom_point(data = contours.proposal |> unnest(location),
               aes(V2, V3), inherit.aes = FALSE, shape = 19, size = 10) +
    xlab(expression(x[1])) + ylab(expression(x[2])) 
#+end_src
#+caption: Probabilidad de aceptación de la propuesta de transición.
#+RESULTS:
[[file:../images/multinormal-aceptacion-mh.jpeg]]


#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal-transicion-mh.jpeg :exports results :results output graphics file
  contours.proposal |>
    mutate(denominator = map(location, objetivo$density),
           numerator   = map(coords  , objetivo$density)) |>
    unnest(numerator, denominator) |>
    mutate(metropolis.hastings = ifelse(exp(numerator-denominator) < 1,
                                        exp(numerator-denominator), 1.00),
           contours.proposal |> unnest(coords) |> select(x,y),
           contours.proposal |> unnest(density.proposal),
           alpha = metropolis.hastings * density.proposal) |>
    ggplot(aes(x, y, z = alpha)) +
    geom_contour_filled(bins = 5) +
    scale_fill_brewer(palette = "Purples") +
    facet_wrap(~id) + sin_lineas + coord_equal() + sin_leyenda + 
    geom_point(data = contours.proposal |> unnest(location),
               aes(V2, V3), inherit.aes = FALSE, shape = 19, size = 10) +
    xlab(expression(x[1])) + ylab(expression(x[2])) 
#+end_src
#+caption: Probabilidad de transición (morado) = probabilidad de proponer un nuevo estado multiplicada por la probabilidad de aceptar dicha transición. 
#+RESULTS:
[[file:../images/multinormal-transicion-mh.jpeg]]

#+begin_src R :exports none :results none :eval never :tangle no
  ##
  muestreo <- ModeloNormalMultivariado$new(c(0,0), 1.65 * diag(c(1,1)))
  set.seed(10)
  mcmc_multi <- crea_metropolis_hastings(objetivo, muestreo)

  anim <- mcmc_multi(5000) |>
    as.tibble() |>
    mutate(tiempo = seq(1,5000)) |>
    ggplot(aes(x = V2, y = V3)) +
    geom_contour_filled(data = plot.grid, aes(x,y,z = density.target),
                        breaks = plot.breaks.target, inherit.aes = FALSE) +
    scale_fill_brewer(palette = "Reds") + 
    ## geom_path(alpha = .3) +
    geom_point() + 
    xlab(expression(x[1])) + ylab(expression(x[2])) + 
    sin_lineas + coord_equal() + sin_leyenda +
    transition_reveal(tiempo) +
    shadow_trail(alpha = .3, distance = 0.01) +
    ## shadow_mark(past=TRUE, future = TRUE) + 
    ease_aes("exponential-out")

  animate(anim, renderer = ffmpeg_renderer(), height = 300, width = 300)

  anim_save("./images/caminata-aleatoria.mp4")
#+end_src



#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal.jpeg :exports results :results output graphics file
  ## Caminatas aleatorias 
  muestras.normal |>
    ggplot(aes(x = V2, y = V3)) +
    geom_contour_filled(data = plot.grid, aes(x,y,z = density.target),
                 breaks = plot.breaks.target) +
    scale_fill_brewer(palette = "Reds") + 
    geom_path() + geom_point() + 
    facet_wrap(~round(sigma,2), nrow = 1) + 
    xlab(expression(x[1])) + ylab(expression(x[2])) + 
    sin_lineas + coord_equal() + sin_leyenda
#+end_src
#+caption: Caminata aleatoria utilizando Metropolis-Hastings para $\theta\in \mathbb{R}^2$. 
#+RESULTS:
[[file:../images/multinormal.jpeg]]



* ¿Por qué funciona?

Ya vimos cómo funciona y describimos una versión suficientemente
robusta. Ahora estudiaremos el por qué esa manera de operar las transiciones nos
lleva a tener un mecanismo que genera muestras de la distribución (en el largo
plazo).

#+REVEAL: split
Para esto tenemos que preguntarnos sobre las probabilidades de transición entre
dos estados. Es decir, la probabilidad de movernos al estado $\theta_\star$
condicional en estar en $\theta$. Lo denotamos por
\begin{align}
\mathbb{P}\left(  \theta_{t + 1} = \theta_\star | \theta_t = \theta\right)\,.
\end{align}

#+REVEAL: split
Si el algoritmo es capaz de mantener un balance entre las probabilidades
condicionales entre dos estados de acuerdo a su frecuencia relativa, entonces el
algoritmo será capaz de preservar las frecuencias.

#+REVEAL: split
En palabras (bueno...), buscamos que
\begin{align}
\frac{\mathbb{P}\left(  \theta_{t + 1} = \theta_\star | \theta_t = \theta\right)}{\mathbb{P}\left(  \theta_{t + 1} = \theta | \theta_t = \theta_\star\right)} = \frac{\pi(\theta_\star)}{\pi(\theta)}\,,
\end{align}
donde $\pi(\cdot)$ denota la probabilidad objetivo.

#+REVEAL: split
Sólo nos falta calcular la probabilidad de transición. Esto lo logramos con dos
pasos: 1) generar la propuesta y 2) aceptar o rechazar la propuesta. Por lo tanto
\begin{align}
\mathbb{P}\left(  \theta_{t + 1} = \theta_\star | \theta_t = \theta\right) = q(\theta_\star | \theta ) \cdot   \alpha(\theta, \theta_\star) =  q(\theta_\star | \theta ) \cdot \min \left\lbrace 1,  \frac{\pi(\theta_\star)}{\pi(\theta)} \cdot \frac{q(\theta|\theta_\star)}{q(\theta_\star|\theta)}\right\rbrace\,. 
\end{align}

*** ~Definición~ [Invarianza]:
Decimos que la distribución $\pi$ es ~invariante~ ante un
mecanismo de transición Markoviana ($p(u, v)$) si satisface que
\begin{align}
\int \pi(u)\, p(u, v) \text{d}u = \pi(v) \,.
\end{align}

#+BEGIN_NOTES
Lo que aprendemos de esto es que si tenemos un mecanismo de transición
Markoviana que satisface las ecuaciones de balance entonces se mantendrá el
comportamiento aleatorio de la distribución objetivo. Lo importante es que la
transición preserva la distribución objetivo.
#+END_NOTES

*** ~Lema~ [Comportamiento asintótico de Metropolis-Hastings]:

El mecanismo de MH descrito anteriormente tiene como distribución límite $\pi(\cdot)$.

#+BEGIN_NOTES
Lo que aprendemos de esto es que en particular MH preserva las ecuaciones de
balance. Por lo tanto, si la cadena empieza en la distribución que nos interesa,
entonces se mantendrá en ese comportamiento. Estudiar formalmente las
condiciones y la tasa de convergencia para llegar a esa distribución escapa a
los intereses del curso y se puede encontrar un tratamiento mas cuidadoso de
esto en citep:Meyn1993. Sin embargo, podemos entenderlo bajo el argumento que MH
busca las zonas de alta densidad. Tal como el vendedor ambulante prefería de
manera consistente las islas mas grandes.
#+END_NOTES

bibliographystyle:abbrvnat
bibliography:references.bib


